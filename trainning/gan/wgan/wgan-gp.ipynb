{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get receipt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "df_col = [\"sentence\",\"brand_name\",\"info\",\"index\",\"content\",\"total\",\"thank_you\"]\n",
    "y_col = [\"brand_name\",\"info\",\"index\",\"content\",\"total\",\"thank_you\"]\n",
    "train_df = pd.read_csv('../../text_classification/31-07-vigroupped.csv',   encoding='utf-8')\n",
    "\n",
    "seed = 120\n",
    "np.random.seed(seed)\n",
    "train_df = shuffle(train_df)\n",
    "train_df.head()\n",
    "\n",
    "X_train = train_df[\"sentence\"].fillna(\"fillna\").values\n",
    "Y_train = train_df[['brand_name', 'info', 'index', 'content', 'total', 'thank_you']].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "texts = X_train\n",
    "\n",
    "tokenizer.fit_on_texts(texts) \n",
    "Tokenizer_vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train_encoded_words = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "maxWordCount= 16\n",
    "maxDictionary_size=Tokenizer_vocab_size\n",
    "X_train_encoded_padded_words = sequence.pad_sequences(X_train_encoded_words, maxlen=maxWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded_padded_words.shape\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['ngá»«']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644\n"
     ]
    }
   ],
   "source": [
    "# Convert Y_train\n",
    "# range(0,len(y_col))[5]\n",
    "y_train = []\n",
    "for row in Y_train:\n",
    "    for index,col in enumerate(range(0,len(y_col))):\n",
    "        if row[col] == 1:\n",
    "            y_train.append(index)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "def padded_sequence_to_text(int_arr):\n",
    "    \n",
    "    padded_sequence = int_arr.reshape((maxWordCount))\n",
    "    padded_sequence = padded_sequence.tolist()\n",
    "#     print(padded_sequence)\n",
    "    started = False\n",
    "    word_seq = []\n",
    "    for word in padded_sequence:\n",
    "        if started:\n",
    "            word_seq.append(word)\n",
    "        else:\n",
    "            if word != 0:\n",
    "                started = True\n",
    "                word_seq.append(word)\n",
    "    \n",
    "    sentences = list(map(sequence_to_text, [word_seq]))\n",
    "    if len(sentences)>0:\n",
    "        my_texts = []\n",
    "        for word in sentences[0]:\n",
    "            if word:\n",
    "                my_texts.append(word)\n",
    "            \n",
    "        return ' '.join(my_texts)\n",
    "    return None\n",
    "\n",
    "def convert_y(y):\n",
    "    result = []\n",
    "    for index, col in enumerate(y_col):\n",
    "        if index == y:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "#     print(result)\n",
    "    return result\n",
    "\n",
    "def reshape_x_train(X_train_encoded_padded_words, r, c):\n",
    "    x_train = []\n",
    "    for row in X_train_encoded_padded_words:\n",
    "        aa = np.array(row)\n",
    "\n",
    "        aa = np.reshape(aa,(r ,c))\n",
    "#         print(aa)\n",
    "        x_train.append(aa)\n",
    "    return np.array(x_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wgan gp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "from functools import partial\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "class WGANGP():\n",
    "    def __init__(self, r,c ,vocal_size):\n",
    "        self.img_rows = r\n",
    "        self.img_cols = c\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build the generator and critic\n",
    "        self.generator = self.build_generator(4,4)\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "\n",
    "        # Freeze generator's layers while training critic\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        # Image input (real sample)\n",
    "        real_img = Input(shape=self.img_shape)\n",
    "\n",
    "        # Noise input\n",
    "        z_disc = Input(shape=(self.latent_dim,))\n",
    "        # Generate image based of noise (fake sample)\n",
    "        fake_img = self.generator(z_disc)\n",
    "\n",
    "        # Discriminator determines validity of the real and fake images\n",
    "        fake = self.critic(fake_img)\n",
    "        valid = self.critic(real_img)\n",
    "\n",
    "        # Construct weighted average between real and fake images\n",
    "        interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated = self.critic(interpolated_img)\n",
    "\n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=interpolated_img)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "        self.critic_model = Model(inputs=[real_img, z_disc],\n",
    "                            outputs=[valid, fake, validity_interpolated])\n",
    "        self.critic_model.compile(loss=[self.wasserstein_loss,\n",
    "                                              self.wasserstein_loss,\n",
    "                                              partial_gp_loss],\n",
    "                                        optimizer=optimizer,\n",
    "                                        loss_weights=[1, 1, 10])\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Sampled noise for input to generator\n",
    "        z_gen = Input(shape=(100,))\n",
    "        # Generate images based of noise\n",
    "        img = self.generator(z_gen)\n",
    "        # Discriminator determines validity\n",
    "        valid = self.critic(img)\n",
    "        # Defines generator model\n",
    "        self.generator_model = Model(z_gen, valid)\n",
    "        self.generator_model.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self, r, c):\n",
    "        i = r/4\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 1 * 1, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((1, 1, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, X_train, epochs, batch_size, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "#         (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "#         X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake =  np.ones((batch_size, 1))\n",
    "        dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for _ in range(self.n_critic):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                # Sample generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                # Train the critic\n",
    "                d_loss = self.critic_model.train_on_batch([imgs, noise],\n",
    "                                                                [valid, fake, dummy])\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.generator_model.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        csvfile = 'wgan.csv'\n",
    "        c = 6\n",
    "        noise = np.random.normal(0, 1, (c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        # Rescale images 0 - 1\n",
    "\n",
    "        gen_imgs = Tokenizer_vocab_size*gen_imgs\n",
    "        \n",
    "        \n",
    "        int_arr = np.array(gen_imgs, dtype='int')\n",
    "#         print(int_arr[0])\n",
    "        \n",
    "        \n",
    "#         print(len(int_arr[0,:,:,0]))\n",
    "#         fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "\n",
    "        for j in range(c):\n",
    "            sentence = padded_sequence_to_text(int_arr[cnt])\n",
    "#             result = convert_y(sampled_labels[cnt])\n",
    "            if len(sentence) <= 0:\n",
    "                continue\n",
    "            print(sentence)\n",
    "            cnt += 1\n",
    "            df = pd.read_csv(csvfile)# Loading a csv file with headers \n",
    "            data = {\n",
    "                'sentence':sentence,\n",
    "            }\n",
    "#             for index, col in enumerate(y_col):\n",
    "#                 data[col] = result[index]\n",
    "            df = df.append(data, ignore_index=True)\n",
    "            df.to_csv(csvfile, index = False,  encoding='utf-8')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 128)               12928     \n",
      "_________________________________________________________________\n",
      "reshape_9 (Reshape)          (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_17 (UpSampling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_18 (UpSampling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 4, 4, 64)          131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 4, 4, 1)           1025      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 4, 4, 1)           0         \n",
      "=================================================================\n",
      "Total params: 408,129\n",
      "Trainable params: 407,745\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_60 (Conv2D)           (None, 2, 2, 16)          160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 1, 1, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 2, 2, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 2, 2, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 1, 1, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 1, 1, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 1, 1, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,177\n",
      "Trainable params: 97,729\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 7.316593] [G loss: 0.008038]\n",
      "chua bh ttiÃ©n ÄÄ©a nháº¥n 0u 108 viet 012018 lÃ¹n 2711 dongid 39 883a dÆ°á»ng nÆ¡\n",
      "dáº§m dam 126 khÄn âtran skycoffee lÃ² gá»mi 5128 2017 mantis chá»n chanh b4n 43pm nlá»\n",
      "hha tháº£o phÃ¬ 742 25000 viá»n taxrm tÃ¡m 120 ct 6a phs thu lÃ  phÃºt phá»\n",
      "opera 380000 cocacola 181 1111 cam chanh quÃ¡n bao 2x khoaimÃ´n Äáº­u steak zzuuu rh gia\n",
      "50000 bahru f13 mc handmade cÆ¡m 0s7 mÆ¡ sá»­u chá» má»m ad 7000 ÄÃºc 280g7 hy\n",
      "0008 85 47 thá»«a ms 35 uni Äá»i ed ngÃ n am 3003 0856393 jÃ¿1l3ÄÆ°á»ng trá»©ng âgold\n",
      "1 [D loss: 7.806074] [G loss: -0.009572]\n",
      "2 [D loss: 2.667544] [G loss: 0.001580]\n",
      "3 [D loss: 4.733757] [G loss: 0.108767]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [D loss: 3.416958] [G loss: -0.125371]\n",
      "5 [D loss: 10.510408] [G loss: -0.133659]\n",
      "6 [D loss: 3.041807] [G loss: 0.109695]\n",
      "7 [D loss: 2.678748] [G loss: 0.226354]\n",
      "8 [D loss: 9.304796] [G loss: 0.059720]\n",
      "9 [D loss: 8.554906] [G loss: 0.027675]\n",
      "10 [D loss: 3.662563] [G loss: 0.188428]\n",
      "11 [D loss: 2.348656] [G loss: 0.074648]\n",
      "12 [D loss: 2.978536] [G loss: 0.091530]\n",
      "13 [D loss: 2.932428] [G loss: -0.037330]\n",
      "14 [D loss: 3.066978] [G loss: -0.007465]\n",
      "15 [D loss: 2.829661] [G loss: 0.029104]\n",
      "16 [D loss: 4.824082] [G loss: 0.227641]\n",
      "17 [D loss: 3.507866] [G loss: -0.079684]\n",
      "18 [D loss: 3.220538] [G loss: -0.015109]\n",
      "19 [D loss: 2.383972] [G loss: 0.091953]\n",
      "20 [D loss: 3.856756] [G loss: -0.140660]\n",
      "21 [D loss: 2.160548] [G loss: -0.001427]\n",
      "22 [D loss: 3.077034] [G loss: 0.056698]\n",
      "23 [D loss: 2.428942] [G loss: 0.140732]\n",
      "24 [D loss: 1.842381] [G loss: 0.063399]\n",
      "25 [D loss: 1.444975] [G loss: -0.036269]\n",
      "26 [D loss: 4.033134] [G loss: -0.041706]\n",
      "27 [D loss: 3.703445] [G loss: 0.066588]\n",
      "28 [D loss: 2.662534] [G loss: -0.049836]\n",
      "29 [D loss: 2.840472] [G loss: 0.100757]\n",
      "30 [D loss: 2.430286] [G loss: 0.263674]\n",
      "31 [D loss: 3.530397] [G loss: 0.052396]\n",
      "32 [D loss: 5.067154] [G loss: 0.048055]\n",
      "33 [D loss: 2.009055] [G loss: 0.140196]\n",
      "34 [D loss: 1.832022] [G loss: 0.246592]\n",
      "35 [D loss: 2.060055] [G loss: -0.046084]\n",
      "36 [D loss: 1.852514] [G loss: 0.133793]\n",
      "37 [D loss: 2.026524] [G loss: 0.126463]\n",
      "38 [D loss: 2.445626] [G loss: -0.058376]\n",
      "39 [D loss: 2.162294] [G loss: 0.137084]\n",
      "40 [D loss: 2.811793] [G loss: 0.128987]\n",
      "41 [D loss: 2.115321] [G loss: 0.152768]\n",
      "42 [D loss: 2.970561] [G loss: 0.026037]\n",
      "43 [D loss: 2.644445] [G loss: 0.014556]\n",
      "44 [D loss: 4.746533] [G loss: -0.054433]\n",
      "45 [D loss: 2.203431] [G loss: -0.008741]\n",
      "46 [D loss: 0.735718] [G loss: 0.069053]\n",
      "47 [D loss: 1.723741] [G loss: 0.088430]\n",
      "48 [D loss: 1.020308] [G loss: 0.104923]\n",
      "49 [D loss: 1.226245] [G loss: 0.176414]\n",
      "50 [D loss: 1.812661] [G loss: 0.017440]\n",
      "51 [D loss: 1.564187] [G loss: 0.038590]\n",
      "52 [D loss: 2.228954] [G loss: -0.039749]\n",
      "53 [D loss: 1.856684] [G loss: 0.043608]\n",
      "54 [D loss: 0.731916] [G loss: -0.251418]\n",
      "55 [D loss: 2.545986] [G loss: 0.039690]\n",
      "56 [D loss: 1.156348] [G loss: -0.155973]\n",
      "57 [D loss: 2.088982] [G loss: 0.054607]\n",
      "58 [D loss: 1.380957] [G loss: 0.142455]\n",
      "59 [D loss: 2.445289] [G loss: -0.056632]\n",
      "60 [D loss: 1.950072] [G loss: 0.007906]\n",
      "61 [D loss: 4.197763] [G loss: 0.075676]\n",
      "62 [D loss: 1.009213] [G loss: 0.122946]\n",
      "63 [D loss: 1.609396] [G loss: 0.022263]\n",
      "64 [D loss: 2.311577] [G loss: -0.054849]\n",
      "65 [D loss: 1.344512] [G loss: -0.024785]\n",
      "66 [D loss: 1.277951] [G loss: 0.084939]\n",
      "67 [D loss: 1.764315] [G loss: -0.071280]\n",
      "68 [D loss: 0.573798] [G loss: 0.047865]\n",
      "69 [D loss: 1.629835] [G loss: 0.000158]\n",
      "70 [D loss: 0.932060] [G loss: -0.089152]\n",
      "71 [D loss: 1.129659] [G loss: 0.148688]\n",
      "72 [D loss: 1.258674] [G loss: -0.142985]\n",
      "73 [D loss: 0.713625] [G loss: -0.137946]\n",
      "74 [D loss: 2.443882] [G loss: -0.080189]\n",
      "75 [D loss: 0.861718] [G loss: 0.131105]\n",
      "76 [D loss: 2.334819] [G loss: 0.019106]\n",
      "77 [D loss: 1.277391] [G loss: 0.254097]\n",
      "78 [D loss: 0.890882] [G loss: 0.075289]\n",
      "79 [D loss: 3.290651] [G loss: -0.049850]\n",
      "80 [D loss: 0.697416] [G loss: 0.158154]\n",
      "81 [D loss: 0.938227] [G loss: 0.030219]\n",
      "82 [D loss: 0.857943] [G loss: -0.066953]\n",
      "83 [D loss: 1.544193] [G loss: 0.034651]\n",
      "84 [D loss: 1.112793] [G loss: 0.166585]\n",
      "85 [D loss: 1.491508] [G loss: 0.166594]\n",
      "86 [D loss: 0.955990] [G loss: -0.044066]\n",
      "87 [D loss: 1.283753] [G loss: 0.011697]\n",
      "88 [D loss: 1.393479] [G loss: -0.069280]\n",
      "89 [D loss: 0.863686] [G loss: 0.007380]\n",
      "90 [D loss: 1.063722] [G loss: 0.028620]\n",
      "91 [D loss: 2.875685] [G loss: 0.119796]\n",
      "92 [D loss: 1.304140] [G loss: 0.035256]\n",
      "93 [D loss: 1.116591] [G loss: 0.038675]\n",
      "94 [D loss: 1.430422] [G loss: -0.012681]\n",
      "95 [D loss: 1.542352] [G loss: -0.068953]\n",
      "96 [D loss: 1.482993] [G loss: 0.052672]\n",
      "97 [D loss: 0.955830] [G loss: 0.066872]\n",
      "98 [D loss: 1.510845] [G loss: -0.047844]\n",
      "99 [D loss: 0.747075] [G loss: 0.023084]\n",
      "100 [D loss: 1.354584] [G loss: -0.117328]\n",
      "101 [D loss: 1.929267] [G loss: -0.041227]\n",
      "102 [D loss: 1.246423] [G loss: -0.013318]\n",
      "103 [D loss: 1.268174] [G loss: 0.008332]\n",
      "104 [D loss: 1.244719] [G loss: -0.073886]\n",
      "105 [D loss: 1.598742] [G loss: -0.034869]\n",
      "106 [D loss: 1.373015] [G loss: -0.200495]\n",
      "107 [D loss: 1.039459] [G loss: 0.078331]\n",
      "108 [D loss: 0.693125] [G loss: -0.161022]\n",
      "109 [D loss: 1.323801] [G loss: 0.089753]\n",
      "110 [D loss: 1.273565] [G loss: 0.099625]\n",
      "111 [D loss: 1.040840] [G loss: -0.068521]\n",
      "112 [D loss: 1.100963] [G loss: 0.008549]\n",
      "113 [D loss: 1.377600] [G loss: 0.136732]\n",
      "114 [D loss: 1.626032] [G loss: 0.031820]\n",
      "115 [D loss: 1.220994] [G loss: -0.054635]\n",
      "116 [D loss: 0.804186] [G loss: -0.060103]\n",
      "117 [D loss: 0.838957] [G loss: 0.063232]\n",
      "118 [D loss: 1.220415] [G loss: -0.090019]\n",
      "119 [D loss: 1.101728] [G loss: 0.151876]\n",
      "120 [D loss: 0.823599] [G loss: -0.016190]\n",
      "121 [D loss: 1.925052] [G loss: -0.109127]\n",
      "122 [D loss: 1.434177] [G loss: 0.128825]\n",
      "123 [D loss: 1.980173] [G loss: 0.087232]\n",
      "124 [D loss: 0.644734] [G loss: -0.050413]\n",
      "125 [D loss: 1.252686] [G loss: 0.042256]\n",
      "126 [D loss: 1.116021] [G loss: 0.105237]\n",
      "127 [D loss: 1.471507] [G loss: 0.069436]\n",
      "128 [D loss: 1.355469] [G loss: -0.059034]\n",
      "129 [D loss: 1.114318] [G loss: 0.084175]\n",
      "130 [D loss: 1.169367] [G loss: 0.141207]\n",
      "131 [D loss: 0.816456] [G loss: 0.114925]\n",
      "132 [D loss: 1.193558] [G loss: -0.026661]\n",
      "133 [D loss: 1.378781] [G loss: 0.069260]\n",
      "134 [D loss: 0.961609] [G loss: 0.054401]\n",
      "135 [D loss: 0.814005] [G loss: 0.078590]\n",
      "136 [D loss: 0.927202] [G loss: 0.150395]\n",
      "137 [D loss: 1.101104] [G loss: 0.036150]\n",
      "138 [D loss: 1.045262] [G loss: 0.125345]\n",
      "139 [D loss: 1.155214] [G loss: -0.022459]\n",
      "140 [D loss: 1.166978] [G loss: -0.102926]\n",
      "141 [D loss: 0.844556] [G loss: -0.006591]\n",
      "142 [D loss: 1.106081] [G loss: 0.036208]\n",
      "143 [D loss: 1.534521] [G loss: 0.079635]\n",
      "144 [D loss: 0.831924] [G loss: -0.061778]\n",
      "145 [D loss: 0.883098] [G loss: -0.118035]\n",
      "146 [D loss: 1.466393] [G loss: -0.031536]\n",
      "147 [D loss: 0.861182] [G loss: -0.046990]\n",
      "148 [D loss: 1.263108] [G loss: -0.029246]\n",
      "149 [D loss: 1.184937] [G loss: -0.174533]\n",
      "150 [D loss: 1.211650] [G loss: -0.022132]\n",
      "151 [D loss: 0.999123] [G loss: 0.114219]\n",
      "152 [D loss: 0.730266] [G loss: -0.050385]\n",
      "153 [D loss: 1.664331] [G loss: 0.022305]\n",
      "154 [D loss: 1.048920] [G loss: -0.095759]\n",
      "155 [D loss: 1.119988] [G loss: -0.125212]\n",
      "156 [D loss: 1.182388] [G loss: -0.079088]\n",
      "157 [D loss: 1.127504] [G loss: -0.204837]\n",
      "158 [D loss: 1.659495] [G loss: 0.131377]\n",
      "159 [D loss: 1.725556] [G loss: -0.071479]\n",
      "160 [D loss: 0.617487] [G loss: -0.032636]\n",
      "161 [D loss: 2.221308] [G loss: -0.051613]\n",
      "162 [D loss: 1.538942] [G loss: -0.137616]\n",
      "163 [D loss: 0.894277] [G loss: 0.043702]\n",
      "164 [D loss: 0.953433] [G loss: 0.212623]\n",
      "165 [D loss: 1.180591] [G loss: 0.169004]\n",
      "166 [D loss: 1.180924] [G loss: 0.007664]\n",
      "167 [D loss: 0.683511] [G loss: 0.015972]\n",
      "168 [D loss: 0.999610] [G loss: -0.045081]\n",
      "169 [D loss: 1.026713] [G loss: -0.177783]\n",
      "170 [D loss: 0.682447] [G loss: -0.030790]\n",
      "171 [D loss: 1.315731] [G loss: -0.150518]\n",
      "172 [D loss: 1.439747] [G loss: 0.020741]\n",
      "173 [D loss: 1.562062] [G loss: 0.057415]\n",
      "174 [D loss: 1.244205] [G loss: -0.077207]\n",
      "175 [D loss: 1.291539] [G loss: 0.094712]\n",
      "176 [D loss: 1.125995] [G loss: 0.027929]\n",
      "177 [D loss: 1.263728] [G loss: -0.031973]\n",
      "178 [D loss: 0.829783] [G loss: -0.114191]\n",
      "179 [D loss: 0.867913] [G loss: 0.097023]\n",
      "180 [D loss: 0.595436] [G loss: 0.107691]\n",
      "181 [D loss: 1.019494] [G loss: -0.152807]\n",
      "182 [D loss: 1.318573] [G loss: -0.057508]\n",
      "183 [D loss: 1.368066] [G loss: 0.074535]\n",
      "184 [D loss: 0.840991] [G loss: 0.026786]\n",
      "185 [D loss: 0.931253] [G loss: -0.152891]\n",
      "186 [D loss: 0.971300] [G loss: -0.050531]\n",
      "187 [D loss: 1.694549] [G loss: 0.064556]\n",
      "188 [D loss: 1.227566] [G loss: 0.018340]\n",
      "189 [D loss: 1.232440] [G loss: -0.039880]\n",
      "190 [D loss: 0.914291] [G loss: -0.049168]\n",
      "191 [D loss: 0.931238] [G loss: -0.203078]\n",
      "192 [D loss: 0.517819] [G loss: -0.079049]\n",
      "193 [D loss: 0.769489] [G loss: -0.046059]\n",
      "194 [D loss: 0.940117] [G loss: 0.026176]\n",
      "195 [D loss: 1.152022] [G loss: -0.317601]\n",
      "196 [D loss: 1.109631] [G loss: -0.112369]\n",
      "197 [D loss: 0.509681] [G loss: -0.074074]\n",
      "198 [D loss: 1.057576] [G loss: -0.008872]\n",
      "199 [D loss: 0.884067] [G loss: -0.048522]\n",
      "200 [D loss: 0.647506] [G loss: 0.153725]\n",
      "rang 4x toÃ n magarla dgia 1 quy á»i 6493 trÃ  green há»c cÃ ng snn nÆ¡ gháº¹\n",
      "eitnidl tÃ¢m 6684 há»ng vien dáº» 59h Â¡rong tram 09 Äc thÄng kÃ½ mo gias eitnidl\n",
      "âââ 34 Ã­ change vinntaba hÃ  85 khuyáº¿n sÃ¡ 17 the sá» 151104 2Â¬ lavie gias\n",
      "an cofee vinntaba km nhÃ£n 1851 00098 phs tÄ©nh on 771 3385 hotline 006 free 450\n",
      "xÃ o sso hÃ© baÃ¸ dáº§m 010 41 hÃ© suÃ½ sÆ¡n ' 03109a refund telâ mtv ne\n",
      "it eo ÄÆ°á»ng 0008 azteen am Äua 88 3x 14 ta Â¥ ty sáº§u 02020001 124727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 [D loss: 1.284649] [G loss: -0.213296]\n",
      "202 [D loss: 0.996106] [G loss: -0.112008]\n",
      "203 [D loss: 0.677682] [G loss: -0.279235]\n",
      "204 [D loss: 0.800828] [G loss: 0.018916]\n",
      "205 [D loss: 1.217490] [G loss: -0.051006]\n",
      "206 [D loss: 0.551360] [G loss: 0.041296]\n",
      "207 [D loss: 1.521615] [G loss: -0.085839]\n",
      "208 [D loss: 0.928192] [G loss: -0.202003]\n",
      "209 [D loss: 1.137791] [G loss: 0.209681]\n",
      "210 [D loss: 1.108524] [G loss: -0.180972]\n",
      "211 [D loss: 0.681286] [G loss: -0.117295]\n",
      "212 [D loss: 1.410299] [G loss: 0.262780]\n",
      "213 [D loss: 1.222479] [G loss: -0.045377]\n",
      "214 [D loss: 0.930633] [G loss: -0.025599]\n",
      "215 [D loss: 0.809691] [G loss: 0.023589]\n",
      "216 [D loss: 0.623365] [G loss: -0.130721]\n",
      "217 [D loss: 1.050912] [G loss: 0.041378]\n",
      "218 [D loss: 1.441209] [G loss: 0.123915]\n",
      "219 [D loss: 0.989068] [G loss: -0.041253]\n",
      "220 [D loss: 1.396318] [G loss: -0.153507]\n",
      "221 [D loss: 0.813985] [G loss: 0.043270]\n",
      "222 [D loss: 0.686523] [G loss: -0.057894]\n",
      "223 [D loss: 1.316004] [G loss: -0.000074]\n",
      "224 [D loss: 0.791192] [G loss: 0.101276]\n",
      "225 [D loss: 0.786412] [G loss: 0.049234]\n",
      "226 [D loss: 0.540980] [G loss: -0.025598]\n",
      "227 [D loss: 0.790734] [G loss: -0.081877]\n",
      "228 [D loss: 1.070454] [G loss: 0.092089]\n",
      "229 [D loss: 0.629248] [G loss: 0.039707]\n",
      "230 [D loss: 0.556598] [G loss: -0.027827]\n",
      "231 [D loss: 0.822459] [G loss: 0.038534]\n",
      "232 [D loss: 0.577799] [G loss: -0.097821]\n",
      "233 [D loss: 0.985723] [G loss: -0.208692]\n",
      "234 [D loss: 0.809428] [G loss: -0.128907]\n",
      "235 [D loss: 0.860733] [G loss: 0.093318]\n",
      "236 [D loss: 1.095005] [G loss: -0.057035]\n",
      "237 [D loss: 0.934399] [G loss: 0.272943]\n",
      "238 [D loss: 1.499762] [G loss: 0.084145]\n",
      "239 [D loss: 1.093133] [G loss: 0.025136]\n",
      "240 [D loss: 0.914853] [G loss: 0.086836]\n",
      "241 [D loss: 0.613618] [G loss: -0.093276]\n",
      "242 [D loss: 1.133003] [G loss: -0.044811]\n",
      "243 [D loss: 0.977954] [G loss: 0.005519]\n",
      "244 [D loss: 1.020524] [G loss: -0.067760]\n",
      "245 [D loss: 1.085501] [G loss: -0.208500]\n",
      "246 [D loss: 0.686786] [G loss: 0.007723]\n",
      "247 [D loss: 0.712732] [G loss: -0.015809]\n",
      "248 [D loss: 1.422362] [G loss: -0.148048]\n",
      "249 [D loss: 1.179588] [G loss: -0.160876]\n",
      "250 [D loss: 0.994727] [G loss: -0.140368]\n",
      "251 [D loss: 1.350247] [G loss: -0.011293]\n",
      "252 [D loss: 1.111113] [G loss: 0.086076]\n",
      "253 [D loss: 0.949387] [G loss: -0.005229]\n",
      "254 [D loss: 0.694973] [G loss: -0.142475]\n",
      "255 [D loss: 0.806962] [G loss: -0.107898]\n",
      "256 [D loss: 0.808340] [G loss: -0.054699]\n",
      "257 [D loss: 1.134815] [G loss: -0.071800]\n",
      "258 [D loss: 1.223816] [G loss: -0.011375]\n",
      "259 [D loss: 1.156774] [G loss: -0.001600]\n",
      "260 [D loss: 1.525039] [G loss: 0.084189]\n",
      "261 [D loss: 1.081977] [G loss: -0.024535]\n",
      "262 [D loss: 1.022351] [G loss: -0.046102]\n",
      "263 [D loss: 0.978162] [G loss: -0.220552]\n",
      "264 [D loss: 1.000000] [G loss: 0.012165]\n",
      "265 [D loss: 0.944536] [G loss: 0.079436]\n",
      "266 [D loss: 0.659757] [G loss: 0.099753]\n",
      "267 [D loss: 1.247969] [G loss: 0.012725]\n",
      "268 [D loss: 0.748633] [G loss: -0.050340]\n",
      "269 [D loss: 1.412557] [G loss: 0.011431]\n",
      "270 [D loss: 0.694449] [G loss: -0.061102]\n",
      "271 [D loss: 1.447081] [G loss: 0.001496]\n",
      "272 [D loss: 1.126445] [G loss: -0.123138]\n",
      "273 [D loss: 0.858168] [G loss: 0.010547]\n",
      "274 [D loss: 0.991359] [G loss: -0.111157]\n",
      "275 [D loss: 1.056324] [G loss: -0.103679]\n",
      "276 [D loss: 1.012859] [G loss: -0.029458]\n",
      "277 [D loss: 1.144208] [G loss: 0.027818]\n",
      "278 [D loss: 1.221469] [G loss: -0.051953]\n",
      "279 [D loss: 0.897358] [G loss: -0.164378]\n",
      "280 [D loss: 0.994019] [G loss: -0.066534]\n",
      "281 [D loss: 0.756211] [G loss: -0.084187]\n",
      "282 [D loss: 1.094579] [G loss: 0.017666]\n",
      "283 [D loss: 0.688678] [G loss: -0.132478]\n",
      "284 [D loss: 0.865882] [G loss: -0.035704]\n",
      "285 [D loss: 1.068497] [G loss: -0.034849]\n",
      "286 [D loss: 0.682016] [G loss: 0.022448]\n",
      "287 [D loss: 0.927027] [G loss: -0.021722]\n",
      "288 [D loss: 1.230263] [G loss: -0.147119]\n",
      "289 [D loss: 0.794813] [G loss: -0.012578]\n",
      "290 [D loss: 1.163695] [G loss: -0.094228]\n",
      "291 [D loss: 1.338235] [G loss: -0.001455]\n",
      "292 [D loss: 1.443982] [G loss: -0.044594]\n",
      "293 [D loss: 0.650796] [G loss: -0.172688]\n",
      "294 [D loss: 0.719147] [G loss: 0.046690]\n",
      "295 [D loss: 0.986017] [G loss: 0.044888]\n",
      "296 [D loss: 0.860464] [G loss: -0.103186]\n",
      "297 [D loss: 0.965883] [G loss: 0.140365]\n",
      "298 [D loss: 0.879996] [G loss: -0.076992]\n",
      "299 [D loss: 1.221428] [G loss: 0.136141]\n",
      "300 [D loss: 1.029964] [G loss: -0.094381]\n",
      "301 [D loss: 0.653968] [G loss: -0.051324]\n",
      "302 [D loss: 1.612345] [G loss: 0.081126]\n",
      "303 [D loss: 1.411217] [G loss: 0.119062]\n",
      "304 [D loss: 0.878625] [G loss: -0.230618]\n",
      "305 [D loss: 1.109912] [G loss: 0.014578]\n",
      "306 [D loss: 0.492848] [G loss: 0.070691]\n",
      "307 [D loss: 0.717192] [G loss: -0.044555]\n",
      "308 [D loss: 0.655996] [G loss: -0.106626]\n",
      "309 [D loss: 0.780239] [G loss: -0.101775]\n",
      "310 [D loss: 1.166230] [G loss: -0.061080]\n",
      "311 [D loss: 0.757085] [G loss: -0.160504]\n",
      "312 [D loss: 0.545442] [G loss: -0.082094]\n",
      "313 [D loss: 0.656833] [G loss: -0.170967]\n",
      "314 [D loss: 0.738945] [G loss: 0.096443]\n",
      "315 [D loss: 1.043793] [G loss: -0.118207]\n",
      "316 [D loss: 0.488249] [G loss: -0.102993]\n",
      "317 [D loss: 1.061421] [G loss: -0.213424]\n",
      "318 [D loss: 0.815099] [G loss: -0.057249]\n",
      "319 [D loss: 1.292489] [G loss: 0.025662]\n",
      "320 [D loss: 0.736725] [G loss: -0.056454]\n",
      "321 [D loss: 0.847756] [G loss: -0.080819]\n",
      "322 [D loss: 0.914782] [G loss: 0.069162]\n",
      "323 [D loss: 1.041542] [G loss: 0.006395]\n",
      "324 [D loss: 0.643728] [G loss: 0.202635]\n",
      "325 [D loss: 0.840758] [G loss: -0.093750]\n",
      "326 [D loss: 0.577430] [G loss: 0.048861]\n",
      "327 [D loss: 0.661241] [G loss: -0.146068]\n",
      "328 [D loss: 1.206396] [G loss: -0.010131]\n",
      "329 [D loss: 1.456511] [G loss: -0.086193]\n",
      "330 [D loss: 0.501880] [G loss: 0.040683]\n",
      "331 [D loss: 1.351497] [G loss: -0.016337]\n",
      "332 [D loss: 1.206989] [G loss: -0.059446]\n",
      "333 [D loss: 1.177871] [G loss: 0.006208]\n",
      "334 [D loss: 1.208980] [G loss: -0.034547]\n",
      "335 [D loss: 0.959457] [G loss: -0.118200]\n",
      "336 [D loss: 1.141567] [G loss: 0.038164]\n",
      "337 [D loss: 0.704299] [G loss: -0.131245]\n",
      "338 [D loss: 0.807231] [G loss: -0.055965]\n",
      "339 [D loss: 1.227563] [G loss: -0.036587]\n",
      "340 [D loss: 0.448515] [G loss: -0.007534]\n",
      "341 [D loss: 1.043078] [G loss: -0.092490]\n",
      "342 [D loss: 0.729658] [G loss: -0.177016]\n",
      "343 [D loss: 1.325424] [G loss: 0.048533]\n",
      "344 [D loss: 0.871334] [G loss: -0.149805]\n",
      "345 [D loss: 0.552119] [G loss: 0.135659]\n",
      "346 [D loss: 0.999866] [G loss: -0.094159]\n",
      "347 [D loss: 0.785031] [G loss: -0.052252]\n",
      "348 [D loss: 0.790508] [G loss: -0.107940]\n",
      "349 [D loss: 0.789912] [G loss: -0.063599]\n",
      "350 [D loss: 0.841373] [G loss: -0.027704]\n",
      "351 [D loss: 0.889569] [G loss: 0.149721]\n",
      "352 [D loss: 0.858661] [G loss: -0.136558]\n",
      "353 [D loss: 0.755317] [G loss: 0.030485]\n",
      "354 [D loss: 0.800859] [G loss: -0.097678]\n",
      "355 [D loss: 1.148560] [G loss: 0.073298]\n",
      "356 [D loss: 1.042044] [G loss: 0.007021]\n",
      "357 [D loss: 0.423944] [G loss: 0.200649]\n",
      "358 [D loss: 1.422761] [G loss: 0.170993]\n",
      "359 [D loss: 1.237618] [G loss: -0.049225]\n",
      "360 [D loss: 1.181768] [G loss: 0.015972]\n",
      "361 [D loss: 0.624139] [G loss: -0.064888]\n",
      "362 [D loss: 0.710429] [G loss: -0.098909]\n",
      "363 [D loss: 1.281141] [G loss: 0.026934]\n",
      "364 [D loss: 0.980250] [G loss: 0.038322]\n",
      "365 [D loss: 0.970321] [G loss: 0.017186]\n",
      "366 [D loss: 0.758459] [G loss: 0.020884]\n",
      "367 [D loss: 0.723393] [G loss: -0.054793]\n",
      "368 [D loss: 1.130112] [G loss: -0.106025]\n",
      "369 [D loss: 1.036812] [G loss: 0.053013]\n",
      "370 [D loss: 1.041224] [G loss: 0.024660]\n",
      "371 [D loss: 0.558213] [G loss: -0.062211]\n",
      "372 [D loss: 1.323259] [G loss: -0.099437]\n",
      "373 [D loss: 0.833028] [G loss: -0.132047]\n",
      "374 [D loss: 1.068156] [G loss: 0.134634]\n",
      "375 [D loss: 0.629973] [G loss: -0.075374]\n",
      "376 [D loss: 1.312996] [G loss: 0.038179]\n",
      "377 [D loss: 0.469121] [G loss: -0.151051]\n",
      "378 [D loss: 1.313710] [G loss: 0.003239]\n",
      "379 [D loss: 1.010162] [G loss: -0.028558]\n",
      "380 [D loss: 0.855977] [G loss: -0.128874]\n",
      "381 [D loss: 0.984990] [G loss: -0.240476]\n",
      "382 [D loss: 0.931186] [G loss: -0.196466]\n",
      "383 [D loss: 0.535175] [G loss: 0.017270]\n",
      "384 [D loss: 1.196410] [G loss: -0.123731]\n",
      "385 [D loss: 1.288491] [G loss: -0.019996]\n",
      "386 [D loss: 1.018084] [G loss: 0.000710]\n",
      "387 [D loss: 1.247879] [G loss: -0.057173]\n",
      "388 [D loss: 1.147297] [G loss: -0.045523]\n",
      "389 [D loss: 0.869084] [G loss: -0.103412]\n",
      "390 [D loss: 0.956041] [G loss: 0.009283]\n",
      "391 [D loss: 0.728696] [G loss: 0.038261]\n",
      "392 [D loss: 1.070941] [G loss: -0.059907]\n",
      "393 [D loss: 1.437325] [G loss: -0.148354]\n",
      "394 [D loss: 1.191927] [G loss: -0.178993]\n",
      "395 [D loss: 0.665136] [G loss: -0.158101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396 [D loss: 1.201508] [G loss: 0.128834]\n",
      "397 [D loss: 1.045231] [G loss: -0.023037]\n",
      "398 [D loss: 0.565591] [G loss: -0.081553]\n",
      "399 [D loss: 1.016376] [G loss: -0.001163]\n",
      "400 [D loss: 1.065852] [G loss: 0.100678]\n",
      "mÆ¡ 57 n 110 45 1 0984 giá» ngÃ y 1 nháº«n 0008 80 ha Ã¬h18 nha\n",
      "260 sá»¥n bank lÃ² 47 50 restaurant tuyáº¿t lÃºc 55 bÃ p go 27 gias frequilasunce tiueen\n",
      "l 6 Ã¢u boom xin 1 thÃ´i Ã°e 70 10 2x 012018 hang vá» suon mÆ°á»i\n",
      "087 5 10 thá»§y bia 50 thue phe 45 capheren tan tá»t to km 091554\n",
      "00098 23112015 48 nháº¥n 360 trÃ  ânÆ°áº¿c bá»¥áº¿a 00 10 digia âtax trÃ  bida xÃ¨o cc\n",
      "fashion gá»mi s 63 i tien rounded 1ran on 5 1424727 50000 35 rib thá»i âgold\n",
      "401 [D loss: 1.004127] [G loss: 0.025363]\n",
      "402 [D loss: 1.068983] [G loss: 0.027953]\n",
      "403 [D loss: 1.057700] [G loss: -0.175920]\n",
      "404 [D loss: 0.866569] [G loss: -0.045832]\n",
      "405 [D loss: 0.675509] [G loss: -0.121575]\n",
      "406 [D loss: 1.062465] [G loss: -0.214258]\n",
      "407 [D loss: 1.583059] [G loss: 0.049536]\n",
      "408 [D loss: 1.475634] [G loss: 0.067080]\n",
      "409 [D loss: 1.367093] [G loss: -0.095960]\n",
      "410 [D loss: 1.025342] [G loss: -0.003771]\n",
      "411 [D loss: 0.583490] [G loss: -0.122829]\n",
      "412 [D loss: 0.908653] [G loss: -0.125277]\n",
      "413 [D loss: 1.182620] [G loss: -0.027394]\n",
      "414 [D loss: 0.723041] [G loss: -0.061299]\n",
      "415 [D loss: 1.385049] [G loss: 0.025291]\n",
      "416 [D loss: 0.648049] [G loss: 0.147866]\n",
      "417 [D loss: 1.133274] [G loss: 0.006405]\n",
      "418 [D loss: 0.559687] [G loss: 0.061777]\n",
      "419 [D loss: 0.851557] [G loss: 0.009215]\n",
      "420 [D loss: 0.773922] [G loss: -0.120238]\n",
      "421 [D loss: 0.516035] [G loss: 0.063412]\n",
      "422 [D loss: 1.330326] [G loss: 0.176255]\n",
      "423 [D loss: 1.102024] [G loss: -0.018178]\n",
      "424 [D loss: 0.990071] [G loss: -0.015983]\n",
      "425 [D loss: 1.541583] [G loss: -0.080326]\n",
      "426 [D loss: 1.123252] [G loss: -0.187006]\n",
      "427 [D loss: 1.052567] [G loss: -0.087776]\n",
      "428 [D loss: 0.834672] [G loss: -0.003759]\n",
      "429 [D loss: 0.763888] [G loss: -0.048041]\n",
      "430 [D loss: 1.007350] [G loss: -0.086643]\n",
      "431 [D loss: 1.131240] [G loss: 0.025116]\n",
      "432 [D loss: 1.217443] [G loss: 0.022196]\n",
      "433 [D loss: 1.402677] [G loss: -0.056921]\n",
      "434 [D loss: 0.847033] [G loss: -0.035601]\n",
      "435 [D loss: 0.640634] [G loss: -0.109478]\n",
      "436 [D loss: 0.467704] [G loss: -0.174852]\n",
      "437 [D loss: 1.074226] [G loss: -0.042555]\n",
      "438 [D loss: 1.234110] [G loss: -0.177602]\n",
      "439 [D loss: 0.695190] [G loss: -0.078040]\n",
      "440 [D loss: 1.490477] [G loss: -0.149853]\n",
      "441 [D loss: 1.624681] [G loss: -0.026166]\n",
      "442 [D loss: 0.697342] [G loss: -0.059356]\n",
      "443 [D loss: 1.253216] [G loss: 0.051950]\n",
      "444 [D loss: 0.579059] [G loss: -0.115532]\n",
      "445 [D loss: 0.749700] [G loss: -0.038363]\n",
      "446 [D loss: 0.731978] [G loss: -0.082905]\n",
      "447 [D loss: 1.085575] [G loss: -0.084702]\n",
      "448 [D loss: 1.406707] [G loss: -0.052526]\n",
      "449 [D loss: 0.879194] [G loss: -0.058391]\n",
      "450 [D loss: 0.786797] [G loss: -0.182707]\n",
      "451 [D loss: 0.890662] [G loss: -0.113010]\n",
      "452 [D loss: 0.656348] [G loss: -0.072411]\n",
      "453 [D loss: 0.850327] [G loss: 0.049632]\n",
      "454 [D loss: 0.683724] [G loss: -0.173195]\n",
      "455 [D loss: 1.270320] [G loss: -0.031462]\n",
      "456 [D loss: 0.602276] [G loss: 0.029482]\n",
      "457 [D loss: 0.943936] [G loss: -0.001375]\n",
      "458 [D loss: 1.030064] [G loss: -0.062395]\n",
      "459 [D loss: 0.684207] [G loss: -0.010340]\n",
      "460 [D loss: 0.503258] [G loss: 0.009013]\n",
      "461 [D loss: 0.732193] [G loss: 0.076976]\n",
      "462 [D loss: 1.134992] [G loss: -0.160020]\n",
      "463 [D loss: 1.130577] [G loss: -0.109115]\n",
      "464 [D loss: 0.968456] [G loss: 0.092503]\n",
      "465 [D loss: 0.691202] [G loss: -0.123752]\n",
      "466 [D loss: 0.753531] [G loss: 0.124075]\n",
      "467 [D loss: 1.058923] [G loss: -0.097176]\n",
      "468 [D loss: 1.075130] [G loss: -0.203995]\n",
      "469 [D loss: 1.021420] [G loss: -0.151907]\n",
      "470 [D loss: 1.094622] [G loss: 0.077970]\n",
      "471 [D loss: 0.411176] [G loss: -0.135643]\n",
      "472 [D loss: 0.855274] [G loss: -0.069889]\n",
      "473 [D loss: 0.848816] [G loss: -0.077704]\n",
      "474 [D loss: 1.199664] [G loss: 0.010482]\n",
      "475 [D loss: 0.908438] [G loss: -0.026463]\n",
      "476 [D loss: 0.778941] [G loss: -0.032186]\n",
      "477 [D loss: 1.293957] [G loss: -0.011842]\n",
      "478 [D loss: 0.768106] [G loss: 0.033179]\n",
      "479 [D loss: 1.176702] [G loss: 0.002205]\n",
      "480 [D loss: 0.991360] [G loss: 0.012580]\n",
      "481 [D loss: 0.914961] [G loss: -0.071958]\n",
      "482 [D loss: 0.592337] [G loss: 0.000307]\n",
      "483 [D loss: 1.228251] [G loss: -0.277448]\n",
      "484 [D loss: 0.766935] [G loss: -0.053930]\n",
      "485 [D loss: 0.635573] [G loss: -0.118592]\n",
      "486 [D loss: 1.080600] [G loss: -0.118731]\n",
      "487 [D loss: 0.832259] [G loss: -0.114889]\n",
      "488 [D loss: 0.809347] [G loss: 0.065418]\n",
      "489 [D loss: 1.340707] [G loss: -0.270859]\n",
      "490 [D loss: 0.793424] [G loss: 0.027033]\n",
      "491 [D loss: 0.877972] [G loss: 0.023332]\n",
      "492 [D loss: 1.293455] [G loss: -0.058753]\n",
      "493 [D loss: 0.887026] [G loss: -0.065633]\n",
      "494 [D loss: 1.216596] [G loss: -0.039254]\n",
      "495 [D loss: 0.386918] [G loss: -0.203826]\n",
      "496 [D loss: 1.024354] [G loss: 0.169737]\n",
      "497 [D loss: 0.739310] [G loss: 0.008064]\n",
      "498 [D loss: 0.515613] [G loss: -0.070568]\n",
      "499 [D loss: 0.426621] [G loss: -0.306694]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-d52d0e927f79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mTokenizer_vocab_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTokenizer_vocab_size\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWGANGP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTokenizer_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-b7aa1977af93>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Train the critic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 d_loss = self.critic_model.train_on_batch([imgs, noise],\n\u001b[0;32m--> 185\u001b[0;31m                                                                 [valid, fake, dummy])\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`inputs` should be a list or tuple.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0mfeed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m         \u001b[0marray_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_keras_initialized'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                     \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r = 4\n",
    "c = 4\n",
    "x_train = reshape_x_train(X_train_encoded_padded_words, r, c)\n",
    "# Rescale -1 to 1\n",
    "x_train = (x_train.astype(np.float32) - Tokenizer_vocab_size/2) / (Tokenizer_vocab_size/2)\n",
    "wgan = WGANGP(r,c,Tokenizer_vocab_size)\n",
    "wgan.train(x_train, epochs=30001, batch_size=32, sample_interval=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
