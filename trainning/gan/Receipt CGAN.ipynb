{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get receipt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "df_col = [\"sentence\",\"brand_name\",\"info\",\"index\",\"content\",\"total\",\"thank_you\"]\n",
    "y_col = [\"brand_name\",\"info\",\"index\",\"content\",\"total\",\"thank_you\"]\n",
    "train_df = pd.read_csv('../text_classification/31-07-vigroupped.csv',   encoding='utf-8')\n",
    "\n",
    "seed = 120\n",
    "np.random.seed(seed)\n",
    "train_df = shuffle(train_df)\n",
    "train_df.head()\n",
    "\n",
    "X_train = train_df[\"sentence\"].fillna(\"fillna\").values\n",
    "Y_train = train_df[['brand_name', 'info', 'index', 'content', 'total', 'thank_you']].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "texts = X_train\n",
    "\n",
    "tokenizer.fit_on_texts(texts) \n",
    "Tokenizer_vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train_encoded_words = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "maxWordCount= 10\n",
    "maxDictionary_size=Tokenizer_vocab_size\n",
    "X_train_encoded_padded_words = sequence.pad_sequences(X_train_encoded_words, maxlen=maxWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded_padded_words.shape\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['ngá»«']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644\n"
     ]
    }
   ],
   "source": [
    "# Convert Y_train\n",
    "# range(0,len(y_col))[5]\n",
    "y_train = []\n",
    "for row in Y_train:\n",
    "    for index,col in enumerate(range(0,len(y_col))):\n",
    "        if row[col] == 1:\n",
    "            y_train.append(index)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_shape, latent_dim,num_classes):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(Tokenizer_vocab_size, input_dim=latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(Tokenizer_vocab_size*2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(Tokenizer_vocab_size*4))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "        model.add(Reshape(img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(num_classes, latent_dim)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape, num_classes):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(Tokenizer_vocab_size*2, input_dim=np.prod(img_shape)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(Tokenizer_vocab_size*2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(Tokenizer_vocab_size*2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "        label_embedding = Flatten()(Embedding(num_classes, np.prod(img_shape))(label))\n",
    "        flat_img = Flatten()(img)\n",
    "\n",
    "        model_input = multiply([flat_img, label_embedding])\n",
    "\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model([img, label], validity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0811 10:38:29.666753 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0811 10:38:29.668124 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0811 10:38:29.670672 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0811 10:38:29.697594 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0811 10:38:29.703322 4367386048 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0811 10:38:29.807307 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0811 10:38:29.812649 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0811 10:38:29.817989 4367386048 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2392)              26312     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2392)              5724056   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2392)              5724056   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 2393      \n",
      "=================================================================\n",
      "Total params: 11,476,817\n",
      "Trainable params: 11,476,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1196)              120796    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1196)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1196)              4784      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2392)              2863224   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2392)              9568      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4784)              11448112  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 4784)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4784)              19136     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                47850     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 10, 1)          0         \n",
      "=================================================================\n",
      "Total params: 14,513,470\n",
      "Trainable params: 14,496,726\n",
      "Non-trainable params: 16,744\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting variable\n",
    "\n",
    "img_rows = 1\n",
    "img_cols = 10\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "num_classes = len(y_col)\n",
    "latent_dim = 100\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(img_shape,num_classes)\n",
    "discriminator.compile(loss=['binary_crossentropy'],\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator(img_shape,latent_dim,num_classes)\n",
    "\n",
    "# The generator takes noise and the target label as input\n",
    "# and generates the corresponding digit of that label\n",
    "noise = Input(shape=(latent_dim,))\n",
    "label = Input(shape=(1,))\n",
    "img = generator([noise, label])\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated image as input and determines validity\n",
    "# and the label of that image\n",
    "valid = discriminator([img, label])\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains generator to fool discriminator\n",
    "combined = Model([noise, label], valid)\n",
    "combined.compile(loss=['binary_crossentropy'],\n",
    "    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model function\n",
    "\n",
    "def save_model(generator,discriminator):\n",
    "    def save(model, model_name):\n",
    "        model_path = \"saved_model/%s.json\" % model_name\n",
    "        weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "        options = {\"file_arch\": model_path,\n",
    "                    \"file_weight\": weights_path}\n",
    "        json_string = model.to_json()\n",
    "        open(options['file_arch'], 'w').write(json_string)\n",
    "        model.save_weights(options['file_weight'])\n",
    "\n",
    "    save(generator, \"generator\")\n",
    "    save(discriminator, \"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 227, 13, 228, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_encoded_words[0])\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_sequence_to_text(int_arr):\n",
    "    \n",
    "    padded_sequence = int_arr.reshape((maxWordCount))\n",
    "    padded_sequence = padded_sequence.tolist()\n",
    "#     print(padded_sequence)\n",
    "    started = False\n",
    "    word_seq = []\n",
    "    for word in padded_sequence:\n",
    "        if started:\n",
    "            word_seq.append(word)\n",
    "        else:\n",
    "            if word != 0:\n",
    "                started = True\n",
    "                word_seq.append(word)\n",
    "    \n",
    "    sentences = list(map(sequence_to_text, [word_seq]))\n",
    "    if len(sentences)>0:\n",
    "        my_texts = []\n",
    "        for word in sentences[0]:\n",
    "            if word:\n",
    "                my_texts.append(word)\n",
    "            \n",
    "        return ' '.join(my_texts)\n",
    "    return None\n",
    "# print(X_train_encoded_padded_words[0])\n",
    "# print(padded_sequence_to_text(X_train_encoded_padded_words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(y):\n",
    "    result = []\n",
    "    for index, col in enumerate(y_col):\n",
    "        if index == y:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "#     print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_\n",
    "\n",
    "def sample_images(epoch, generator):\n",
    "        csvfile = 'cgan2d.csv'\n",
    "        c = len(y_col)\n",
    "        noise = np.random.normal(0, 1, (c, 100))\n",
    "        sampled_labels = np.arange(0, len(y_col)).reshape(-1, 1)\n",
    "\n",
    "        gen_imgs = generator.predict([noise, sampled_labels])\n",
    "        # Rescale images 0 - 1\n",
    "#         print(sampled_labels)\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        gen_imgs = Tokenizer_vocab_size*gen_imgs\n",
    "        \n",
    "        \n",
    "        int_arr = np.array(gen_imgs, dtype='int')\n",
    "#         print(int_arr[0])\n",
    "        \n",
    "        \n",
    "#         print(len(int_arr[0,:,:,0]))\n",
    "#         fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "\n",
    "        for j in range(c):\n",
    "            sentence = padded_sequence_to_text(int_arr[cnt])\n",
    "            result = convert_y(sampled_labels[cnt])\n",
    "            if len(sentence) <= 0:\n",
    "                continue\n",
    "            print(sentence,':',sampled_labels[cnt])\n",
    "            cnt += 1\n",
    "            df = pd.read_csv(csvfile)# Loading a csv file with headers \n",
    "            data = {\n",
    "                'sentence':sentence,\n",
    "            }\n",
    "            for index, col in enumerate(y_col):\n",
    "                data[col] = result[index]\n",
    "            df = df.append(data, ignore_index=True)\n",
    "            df.to_csv(csvfile, index = False,  encoding='utf-8')\n",
    "#                 axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
    "#                 axs[i,j].axis('off')\n",
    "#                 cnt += 1\n",
    "#         fig.savefig(\"images/%d.png\" % epoch)\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   13,  228,    1],\n",
       "       [   0,    0,    0, ...,    0,  415,  416],\n",
       "       [   0,    0,    0, ...,  417,  418,  419],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    0,  377,    1],\n",
       "       [   0,    0,    0, ...,    0,    0,   13],\n",
       "       [ 212, 1193, 1194, ...,    1,   19,    1]], dtype=int32)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded_padded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "for row in X_train_encoded_padded_words:\n",
    "    aa = np.array(row)\n",
    "    \n",
    "    aa = np.reshape(aa,(1,10))\n",
    "#     print(aa)\n",
    "    x_train.append(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 1, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.487375, acc.: 78.12%] [G loss: 2.447657]\n",
      "1 [D loss: 0.549188, acc.: 71.88%] [G loss: 2.512894]\n",
      "2 [D loss: 0.473113, acc.: 75.00%] [G loss: 2.046144]\n",
      "3 [D loss: 0.543856, acc.: 67.19%] [G loss: 2.427663]\n",
      "4 [D loss: 0.556761, acc.: 78.12%] [G loss: 1.999858]\n",
      "5 [D loss: 0.544747, acc.: 71.88%] [G loss: 2.061111]\n",
      "6 [D loss: 0.442042, acc.: 76.56%] [G loss: 2.021250]\n",
      "7 [D loss: 0.472365, acc.: 75.00%] [G loss: 2.475438]\n",
      "8 [D loss: 0.425153, acc.: 81.25%] [G loss: 2.162701]\n",
      "9 [D loss: 0.514163, acc.: 76.56%] [G loss: 2.033675]\n",
      "10 [D loss: 0.460877, acc.: 76.56%] [G loss: 2.034451]\n",
      "11 [D loss: 0.394393, acc.: 84.38%] [G loss: 1.839388]\n",
      "12 [D loss: 0.277071, acc.: 95.31%] [G loss: 2.341034]\n",
      "13 [D loss: 0.571602, acc.: 73.44%] [G loss: 2.111933]\n",
      "14 [D loss: 0.423138, acc.: 73.44%] [G loss: 1.735255]\n",
      "15 [D loss: 0.374353, acc.: 81.25%] [G loss: 2.306901]\n",
      "16 [D loss: 0.495569, acc.: 71.88%] [G loss: 1.727870]\n",
      "17 [D loss: 0.387988, acc.: 81.25%] [G loss: 2.237312]\n",
      "18 [D loss: 0.670584, acc.: 75.00%] [G loss: 1.907917]\n",
      "19 [D loss: 0.339425, acc.: 82.81%] [G loss: 2.124179]\n",
      "20 [D loss: 0.558285, acc.: 76.56%] [G loss: 2.105774]\n",
      "21 [D loss: 0.573918, acc.: 64.06%] [G loss: 2.774875]\n",
      "22 [D loss: 0.622493, acc.: 67.19%] [G loss: 2.639026]\n",
      "23 [D loss: 0.443441, acc.: 76.56%] [G loss: 2.407497]\n",
      "24 [D loss: 0.458654, acc.: 78.12%] [G loss: 1.749438]\n",
      "25 [D loss: 0.512373, acc.: 73.44%] [G loss: 2.112375]\n",
      "26 [D loss: 0.462081, acc.: 81.25%] [G loss: 2.177536]\n",
      "27 [D loss: 0.458120, acc.: 84.38%] [G loss: 1.979747]\n",
      "28 [D loss: 0.520320, acc.: 73.44%] [G loss: 2.071983]\n",
      "29 [D loss: 0.595798, acc.: 70.31%] [G loss: 2.145235]\n",
      "30 [D loss: 0.321241, acc.: 85.94%] [G loss: 1.950872]\n",
      "31 [D loss: 0.338698, acc.: 85.94%] [G loss: 2.181231]\n",
      "32 [D loss: 0.362879, acc.: 87.50%] [G loss: 1.802000]\n",
      "33 [D loss: 0.539273, acc.: 71.88%] [G loss: 1.887533]\n",
      "34 [D loss: 0.494421, acc.: 71.88%] [G loss: 2.196721]\n",
      "35 [D loss: 0.468949, acc.: 76.56%] [G loss: 2.223164]\n",
      "36 [D loss: 0.494990, acc.: 68.75%] [G loss: 2.362737]\n",
      "37 [D loss: 0.494766, acc.: 71.88%] [G loss: 2.070798]\n",
      "38 [D loss: 0.399723, acc.: 76.56%] [G loss: 2.396833]\n",
      "39 [D loss: 0.511354, acc.: 71.88%] [G loss: 1.816103]\n",
      "40 [D loss: 0.565981, acc.: 68.75%] [G loss: 1.524266]\n",
      "41 [D loss: 0.366565, acc.: 81.25%] [G loss: 2.113489]\n",
      "42 [D loss: 0.470831, acc.: 75.00%] [G loss: 1.794288]\n",
      "43 [D loss: 0.393660, acc.: 82.81%] [G loss: 2.596102]\n",
      "44 [D loss: 0.502296, acc.: 68.75%] [G loss: 2.143676]\n",
      "45 [D loss: 0.361873, acc.: 85.94%] [G loss: 2.470573]\n",
      "46 [D loss: 0.495631, acc.: 70.31%] [G loss: 2.491998]\n",
      "47 [D loss: 0.572714, acc.: 67.19%] [G loss: 2.194011]\n",
      "48 [D loss: 0.401655, acc.: 81.25%] [G loss: 2.100490]\n",
      "49 [D loss: 0.403839, acc.: 81.25%] [G loss: 2.309519]\n",
      "50 [D loss: 0.439446, acc.: 79.69%] [G loss: 2.281029]\n",
      "51 [D loss: 0.511440, acc.: 70.31%] [G loss: 1.824382]\n",
      "52 [D loss: 0.456075, acc.: 78.12%] [G loss: 2.485594]\n",
      "53 [D loss: 0.466097, acc.: 78.12%] [G loss: 2.016936]\n",
      "54 [D loss: 0.399756, acc.: 81.25%] [G loss: 1.977031]\n",
      "55 [D loss: 0.450557, acc.: 79.69%] [G loss: 2.538201]\n",
      "56 [D loss: 0.318673, acc.: 84.38%] [G loss: 2.163457]\n",
      "57 [D loss: 0.477823, acc.: 76.56%] [G loss: 2.295329]\n",
      "58 [D loss: 0.601771, acc.: 75.00%] [G loss: 2.602895]\n",
      "59 [D loss: 0.546921, acc.: 60.94%] [G loss: 2.078386]\n",
      "60 [D loss: 0.314546, acc.: 84.38%] [G loss: 2.025114]\n",
      "61 [D loss: 0.348078, acc.: 79.69%] [G loss: 2.094741]\n",
      "62 [D loss: 0.382625, acc.: 84.38%] [G loss: 2.070321]\n",
      "63 [D loss: 0.557983, acc.: 73.44%] [G loss: 2.024133]\n",
      "64 [D loss: 0.507511, acc.: 75.00%] [G loss: 2.274504]\n",
      "65 [D loss: 0.551778, acc.: 73.44%] [G loss: 1.844233]\n",
      "66 [D loss: 0.380203, acc.: 79.69%] [G loss: 1.801077]\n",
      "67 [D loss: 0.415705, acc.: 76.56%] [G loss: 1.812644]\n",
      "68 [D loss: 0.622687, acc.: 75.00%] [G loss: 2.078678]\n",
      "69 [D loss: 0.389765, acc.: 84.38%] [G loss: 2.050086]\n",
      "70 [D loss: 0.483978, acc.: 76.56%] [G loss: 2.226769]\n",
      "71 [D loss: 0.500136, acc.: 78.12%] [G loss: 2.833569]\n",
      "72 [D loss: 0.622114, acc.: 64.06%] [G loss: 3.040048]\n",
      "73 [D loss: 0.483679, acc.: 75.00%] [G loss: 2.039758]\n",
      "74 [D loss: 0.568851, acc.: 62.50%] [G loss: 1.689476]\n",
      "75 [D loss: 0.420125, acc.: 79.69%] [G loss: 1.796739]\n",
      "76 [D loss: 0.667437, acc.: 64.06%] [G loss: 1.559531]\n",
      "77 [D loss: 0.534050, acc.: 68.75%] [G loss: 1.849120]\n",
      "78 [D loss: 0.618688, acc.: 64.06%] [G loss: 2.481625]\n",
      "79 [D loss: 0.553120, acc.: 78.12%] [G loss: 2.087784]\n",
      "80 [D loss: 0.534476, acc.: 73.44%] [G loss: 1.753386]\n",
      "81 [D loss: 0.577456, acc.: 64.06%] [G loss: 1.501699]\n",
      "82 [D loss: 0.484070, acc.: 76.56%] [G loss: 1.794370]\n",
      "83 [D loss: 0.576172, acc.: 76.56%] [G loss: 1.744666]\n",
      "84 [D loss: 0.375096, acc.: 85.94%] [G loss: 2.403613]\n",
      "85 [D loss: 0.452688, acc.: 81.25%] [G loss: 2.075073]\n",
      "86 [D loss: 0.435851, acc.: 82.81%] [G loss: 2.155667]\n",
      "87 [D loss: 0.628279, acc.: 68.75%] [G loss: 1.881021]\n",
      "88 [D loss: 0.429128, acc.: 82.81%] [G loss: 2.099260]\n",
      "89 [D loss: 0.404222, acc.: 81.25%] [G loss: 2.044221]\n",
      "90 [D loss: 0.535236, acc.: 67.19%] [G loss: 2.375519]\n",
      "91 [D loss: 0.530578, acc.: 71.88%] [G loss: 2.156407]\n",
      "92 [D loss: 0.475957, acc.: 76.56%] [G loss: 2.553889]\n",
      "93 [D loss: 0.345191, acc.: 78.12%] [G loss: 2.792250]\n",
      "94 [D loss: 0.597268, acc.: 65.62%] [G loss: 1.835965]\n",
      "95 [D loss: 0.529436, acc.: 70.31%] [G loss: 2.109755]\n",
      "96 [D loss: 0.486978, acc.: 78.12%] [G loss: 1.593865]\n",
      "97 [D loss: 0.356359, acc.: 85.94%] [G loss: 2.128780]\n",
      "98 [D loss: 0.520483, acc.: 68.75%] [G loss: 1.823024]\n",
      "99 [D loss: 0.487482, acc.: 76.56%] [G loss: 1.967169]\n",
      "100 [D loss: 0.443300, acc.: 78.12%] [G loss: 2.115321]\n",
      "101 [D loss: 0.399966, acc.: 82.81%] [G loss: 1.702083]\n",
      "102 [D loss: 0.579470, acc.: 65.62%] [G loss: 1.388524]\n",
      "103 [D loss: 0.430957, acc.: 79.69%] [G loss: 2.277633]\n",
      "104 [D loss: 0.319773, acc.: 85.94%] [G loss: 1.885465]\n",
      "105 [D loss: 0.498575, acc.: 76.56%] [G loss: 1.551374]\n",
      "106 [D loss: 0.449176, acc.: 81.25%] [G loss: 2.014307]\n",
      "107 [D loss: 0.425576, acc.: 82.81%] [G loss: 1.974138]\n",
      "108 [D loss: 0.555963, acc.: 78.12%] [G loss: 1.668406]\n",
      "109 [D loss: 0.758882, acc.: 64.06%] [G loss: 1.621641]\n",
      "110 [D loss: 0.433115, acc.: 76.56%] [G loss: 2.231251]\n",
      "111 [D loss: 0.620559, acc.: 67.19%] [G loss: 2.182698]\n",
      "112 [D loss: 0.578811, acc.: 70.31%] [G loss: 1.720015]\n",
      "113 [D loss: 0.536005, acc.: 75.00%] [G loss: 2.176683]\n",
      "114 [D loss: 0.519023, acc.: 71.88%] [G loss: 2.163748]\n",
      "115 [D loss: 0.506784, acc.: 75.00%] [G loss: 1.786724]\n",
      "116 [D loss: 0.531403, acc.: 68.75%] [G loss: 1.713637]\n",
      "117 [D loss: 0.505105, acc.: 76.56%] [G loss: 1.410375]\n",
      "118 [D loss: 0.502114, acc.: 75.00%] [G loss: 1.345078]\n",
      "119 [D loss: 0.419377, acc.: 85.94%] [G loss: 1.541143]\n",
      "120 [D loss: 0.480715, acc.: 78.12%] [G loss: 1.799067]\n",
      "121 [D loss: 0.647721, acc.: 68.75%] [G loss: 1.860581]\n",
      "122 [D loss: 0.588824, acc.: 75.00%] [G loss: 1.564267]\n",
      "123 [D loss: 0.626477, acc.: 70.31%] [G loss: 1.422225]\n",
      "124 [D loss: 0.449703, acc.: 82.81%] [G loss: 1.539199]\n",
      "125 [D loss: 0.502131, acc.: 68.75%] [G loss: 1.677914]\n",
      "126 [D loss: 0.600025, acc.: 60.94%] [G loss: 1.683901]\n",
      "127 [D loss: 0.388250, acc.: 79.69%] [G loss: 1.950841]\n",
      "128 [D loss: 0.593402, acc.: 68.75%] [G loss: 1.595391]\n",
      "129 [D loss: 0.646920, acc.: 67.19%] [G loss: 1.648989]\n",
      "130 [D loss: 0.552126, acc.: 65.62%] [G loss: 1.591131]\n",
      "131 [D loss: 0.505305, acc.: 67.19%] [G loss: 2.031429]\n",
      "132 [D loss: 0.539891, acc.: 75.00%] [G loss: 1.388447]\n",
      "133 [D loss: 0.546992, acc.: 70.31%] [G loss: 1.695247]\n",
      "134 [D loss: 0.487275, acc.: 78.12%] [G loss: 1.715991]\n",
      "135 [D loss: 0.537008, acc.: 70.31%] [G loss: 2.338629]\n",
      "136 [D loss: 0.559148, acc.: 71.88%] [G loss: 2.049485]\n",
      "137 [D loss: 0.531236, acc.: 76.56%] [G loss: 1.562675]\n",
      "138 [D loss: 0.597918, acc.: 62.50%] [G loss: 1.733884]\n",
      "139 [D loss: 0.451364, acc.: 81.25%] [G loss: 1.549119]\n",
      "140 [D loss: 0.525052, acc.: 71.88%] [G loss: 1.528705]\n",
      "141 [D loss: 0.589456, acc.: 68.75%] [G loss: 1.733014]\n",
      "142 [D loss: 0.463923, acc.: 81.25%] [G loss: 1.582722]\n",
      "143 [D loss: 0.430951, acc.: 78.12%] [G loss: 1.660049]\n",
      "144 [D loss: 0.615323, acc.: 71.88%] [G loss: 1.576051]\n",
      "145 [D loss: 0.642620, acc.: 60.94%] [G loss: 2.029878]\n",
      "146 [D loss: 0.682577, acc.: 64.06%] [G loss: 1.668263]\n",
      "147 [D loss: 0.664321, acc.: 53.12%] [G loss: 2.195389]\n",
      "148 [D loss: 0.820442, acc.: 46.88%] [G loss: 1.820577]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149 [D loss: 0.517659, acc.: 68.75%] [G loss: 2.159917]\n",
      "150 [D loss: 0.533450, acc.: 62.50%] [G loss: 2.026925]\n",
      "151 [D loss: 0.642612, acc.: 59.38%] [G loss: 2.144341]\n",
      "152 [D loss: 0.754118, acc.: 60.94%] [G loss: 1.580049]\n",
      "153 [D loss: 0.573801, acc.: 75.00%] [G loss: 1.366220]\n",
      "154 [D loss: 0.531161, acc.: 67.19%] [G loss: 1.516047]\n",
      "155 [D loss: 0.480310, acc.: 75.00%] [G loss: 1.739447]\n",
      "156 [D loss: 0.619260, acc.: 67.19%] [G loss: 1.464983]\n",
      "157 [D loss: 0.604779, acc.: 56.25%] [G loss: 1.235600]\n",
      "158 [D loss: 0.493459, acc.: 70.31%] [G loss: 1.596788]\n",
      "159 [D loss: 0.573855, acc.: 67.19%] [G loss: 1.380284]\n",
      "160 [D loss: 0.551390, acc.: 71.88%] [G loss: 1.184136]\n",
      "161 [D loss: 0.604857, acc.: 64.06%] [G loss: 1.364444]\n",
      "162 [D loss: 0.507339, acc.: 79.69%] [G loss: 1.491753]\n",
      "163 [D loss: 0.550649, acc.: 75.00%] [G loss: 1.562155]\n",
      "164 [D loss: 0.489862, acc.: 71.88%] [G loss: 1.634855]\n",
      "165 [D loss: 0.464542, acc.: 79.69%] [G loss: 1.397236]\n",
      "166 [D loss: 0.576649, acc.: 70.31%] [G loss: 1.188444]\n",
      "167 [D loss: 0.690538, acc.: 56.25%] [G loss: 1.489622]\n",
      "168 [D loss: 0.554742, acc.: 70.31%] [G loss: 1.380781]\n",
      "169 [D loss: 0.492898, acc.: 68.75%] [G loss: 1.200723]\n",
      "170 [D loss: 0.401995, acc.: 79.69%] [G loss: 1.493524]\n",
      "171 [D loss: 0.752659, acc.: 51.56%] [G loss: 1.479347]\n",
      "172 [D loss: 0.628992, acc.: 71.88%] [G loss: 1.296207]\n",
      "173 [D loss: 0.566804, acc.: 65.62%] [G loss: 1.293754]\n",
      "174 [D loss: 0.649243, acc.: 65.62%] [G loss: 1.410344]\n",
      "175 [D loss: 0.634232, acc.: 65.62%] [G loss: 1.608697]\n",
      "176 [D loss: 0.605451, acc.: 67.19%] [G loss: 1.329958]\n",
      "177 [D loss: 0.627331, acc.: 67.19%] [G loss: 0.885155]\n",
      "178 [D loss: 0.742582, acc.: 57.81%] [G loss: 1.443536]\n",
      "179 [D loss: 0.598045, acc.: 75.00%] [G loss: 1.324894]\n",
      "180 [D loss: 0.575912, acc.: 75.00%] [G loss: 1.419923]\n",
      "181 [D loss: 0.646338, acc.: 65.62%] [G loss: 1.375430]\n",
      "182 [D loss: 0.639648, acc.: 56.25%] [G loss: 1.409241]\n",
      "183 [D loss: 0.673404, acc.: 64.06%] [G loss: 1.521199]\n",
      "184 [D loss: 0.724975, acc.: 53.12%] [G loss: 1.397400]\n",
      "185 [D loss: 0.819015, acc.: 43.75%] [G loss: 1.812121]\n",
      "186 [D loss: 0.638879, acc.: 70.31%] [G loss: 1.285222]\n",
      "187 [D loss: 0.660985, acc.: 64.06%] [G loss: 1.384379]\n",
      "188 [D loss: 0.599442, acc.: 59.38%] [G loss: 1.222899]\n",
      "189 [D loss: 0.720688, acc.: 56.25%] [G loss: 1.183649]\n",
      "190 [D loss: 0.678194, acc.: 60.94%] [G loss: 0.967385]\n",
      "191 [D loss: 0.707029, acc.: 51.56%] [G loss: 0.996758]\n",
      "192 [D loss: 0.615068, acc.: 68.75%] [G loss: 1.097309]\n",
      "193 [D loss: 0.557587, acc.: 65.62%] [G loss: 1.132345]\n",
      "194 [D loss: 0.580957, acc.: 68.75%] [G loss: 0.950865]\n",
      "195 [D loss: 0.610532, acc.: 67.19%] [G loss: 1.315222]\n",
      "196 [D loss: 0.641962, acc.: 67.19%] [G loss: 1.075662]\n",
      "197 [D loss: 0.690376, acc.: 45.31%] [G loss: 1.108101]\n",
      "198 [D loss: 0.602311, acc.: 60.94%] [G loss: 1.186300]\n",
      "199 [D loss: 0.574169, acc.: 73.44%] [G loss: 1.071095]\n",
      "200 [D loss: 0.620638, acc.: 70.31%] [G loss: 1.229653]\n",
      "000 bÃ p ngá»« : [0]\n",
      "000 10 so 12 95 ÄÆ°a ÄÃ¹i tÃªn : [1]\n",
      "máº·t 30 Ä 5 19 32 bÃ¬nh tiá»n : [2]\n",
      "giá» 20 30 tiÃªn thÃ nh vat 86 03 : [3]\n",
      "310 duy tiá»n : [4]\n",
      "000 hÃ ng 60 1 ngÃ¢n 48 hia Â¡ : [5]\n",
      "201 [D loss: 0.566060, acc.: 70.31%] [G loss: 1.068439]\n",
      "202 [D loss: 0.706400, acc.: 57.81%] [G loss: 1.007252]\n",
      "203 [D loss: 0.703634, acc.: 60.94%] [G loss: 1.121350]\n",
      "204 [D loss: 0.794743, acc.: 50.00%] [G loss: 0.973145]\n",
      "205 [D loss: 0.715321, acc.: 51.56%] [G loss: 0.850613]\n",
      "206 [D loss: 0.702364, acc.: 57.81%] [G loss: 1.098606]\n",
      "207 [D loss: 0.721750, acc.: 62.50%] [G loss: 1.185167]\n",
      "208 [D loss: 0.787501, acc.: 50.00%] [G loss: 0.748579]\n",
      "209 [D loss: 0.703511, acc.: 56.25%] [G loss: 0.965557]\n",
      "210 [D loss: 0.635308, acc.: 56.25%] [G loss: 1.347161]\n",
      "211 [D loss: 0.654919, acc.: 65.62%] [G loss: 1.117532]\n",
      "212 [D loss: 0.607984, acc.: 64.06%] [G loss: 1.281114]\n",
      "213 [D loss: 0.712503, acc.: 46.88%] [G loss: 1.221047]\n",
      "214 [D loss: 0.680090, acc.: 50.00%] [G loss: 0.982420]\n",
      "215 [D loss: 0.713629, acc.: 45.31%] [G loss: 0.978062]\n",
      "216 [D loss: 0.640898, acc.: 56.25%] [G loss: 0.946405]\n",
      "217 [D loss: 0.709856, acc.: 51.56%] [G loss: 0.938788]\n",
      "218 [D loss: 0.711272, acc.: 48.44%] [G loss: 0.914513]\n",
      "219 [D loss: 0.657276, acc.: 50.00%] [G loss: 0.992874]\n",
      "220 [D loss: 0.680666, acc.: 56.25%] [G loss: 0.928313]\n",
      "221 [D loss: 0.668544, acc.: 57.81%] [G loss: 0.880002]\n",
      "222 [D loss: 0.645257, acc.: 60.94%] [G loss: 0.990795]\n",
      "223 [D loss: 0.640872, acc.: 60.94%] [G loss: 0.964582]\n",
      "224 [D loss: 0.655626, acc.: 56.25%] [G loss: 1.101710]\n",
      "225 [D loss: 0.658683, acc.: 56.25%] [G loss: 0.925806]\n",
      "226 [D loss: 0.666731, acc.: 54.69%] [G loss: 1.199341]\n",
      "227 [D loss: 0.717543, acc.: 50.00%] [G loss: 1.068804]\n",
      "228 [D loss: 0.739357, acc.: 45.31%] [G loss: 0.955126]\n",
      "229 [D loss: 0.682019, acc.: 54.69%] [G loss: 0.861613]\n",
      "230 [D loss: 0.648741, acc.: 51.56%] [G loss: 0.991383]\n",
      "231 [D loss: 0.616589, acc.: 68.75%] [G loss: 0.927452]\n",
      "232 [D loss: 0.693062, acc.: 53.12%] [G loss: 0.958794]\n",
      "233 [D loss: 0.624843, acc.: 64.06%] [G loss: 0.794581]\n",
      "234 [D loss: 0.684288, acc.: 57.81%] [G loss: 0.908821]\n",
      "235 [D loss: 0.649930, acc.: 54.69%] [G loss: 0.974755]\n",
      "236 [D loss: 0.683889, acc.: 62.50%] [G loss: 1.084576]\n",
      "237 [D loss: 0.704662, acc.: 51.56%] [G loss: 1.015963]\n",
      "238 [D loss: 0.673110, acc.: 56.25%] [G loss: 0.813058]\n",
      "239 [D loss: 0.633259, acc.: 54.69%] [G loss: 1.062836]\n",
      "240 [D loss: 0.621608, acc.: 71.88%] [G loss: 0.966971]\n",
      "241 [D loss: 0.623078, acc.: 64.06%] [G loss: 0.903775]\n",
      "242 [D loss: 0.642156, acc.: 65.62%] [G loss: 0.937392]\n",
      "243 [D loss: 0.686600, acc.: 60.94%] [G loss: 0.859908]\n",
      "244 [D loss: 0.683373, acc.: 56.25%] [G loss: 0.947094]\n",
      "245 [D loss: 0.704462, acc.: 50.00%] [G loss: 0.989464]\n",
      "246 [D loss: 0.642804, acc.: 64.06%] [G loss: 1.152194]\n",
      "247 [D loss: 0.628702, acc.: 68.75%] [G loss: 0.782200]\n",
      "248 [D loss: 0.615886, acc.: 59.38%] [G loss: 1.156732]\n",
      "249 [D loss: 0.636507, acc.: 59.38%] [G loss: 0.993766]\n",
      "250 [D loss: 0.734561, acc.: 46.88%] [G loss: 0.919852]\n",
      "251 [D loss: 0.712288, acc.: 54.69%] [G loss: 0.828671]\n",
      "252 [D loss: 0.601052, acc.: 65.62%] [G loss: 1.058495]\n",
      "253 [D loss: 0.618270, acc.: 65.62%] [G loss: 0.978221]\n",
      "254 [D loss: 0.610148, acc.: 67.19%] [G loss: 0.967390]\n",
      "255 [D loss: 0.686962, acc.: 53.12%] [G loss: 1.183860]\n",
      "256 [D loss: 0.688880, acc.: 59.38%] [G loss: 0.873527]\n",
      "257 [D loss: 0.713178, acc.: 50.00%] [G loss: 0.982953]\n",
      "258 [D loss: 0.680334, acc.: 50.00%] [G loss: 0.866261]\n",
      "259 [D loss: 0.658022, acc.: 62.50%] [G loss: 0.930808]\n",
      "260 [D loss: 0.699688, acc.: 53.12%] [G loss: 0.843585]\n",
      "261 [D loss: 0.720845, acc.: 43.75%] [G loss: 0.730276]\n",
      "262 [D loss: 0.710126, acc.: 50.00%] [G loss: 0.936542]\n",
      "263 [D loss: 0.655252, acc.: 54.69%] [G loss: 0.836559]\n",
      "264 [D loss: 0.672063, acc.: 59.38%] [G loss: 0.953848]\n",
      "265 [D loss: 0.662527, acc.: 54.69%] [G loss: 0.930778]\n",
      "266 [D loss: 0.652503, acc.: 53.12%] [G loss: 0.777797]\n",
      "267 [D loss: 0.676407, acc.: 68.75%] [G loss: 0.876658]\n",
      "268 [D loss: 0.685403, acc.: 54.69%] [G loss: 0.773321]\n",
      "269 [D loss: 0.673369, acc.: 56.25%] [G loss: 0.723975]\n",
      "270 [D loss: 0.669096, acc.: 59.38%] [G loss: 0.839911]\n",
      "271 [D loss: 0.671846, acc.: 53.12%] [G loss: 0.864378]\n",
      "272 [D loss: 0.641914, acc.: 64.06%] [G loss: 0.824662]\n",
      "273 [D loss: 0.655783, acc.: 56.25%] [G loss: 0.843657]\n",
      "274 [D loss: 0.710112, acc.: 50.00%] [G loss: 0.894292]\n",
      "275 [D loss: 0.635617, acc.: 65.62%] [G loss: 0.767200]\n",
      "276 [D loss: 0.661731, acc.: 51.56%] [G loss: 0.835768]\n",
      "277 [D loss: 0.675292, acc.: 48.44%] [G loss: 0.920341]\n",
      "278 [D loss: 0.620849, acc.: 65.62%] [G loss: 0.803592]\n",
      "279 [D loss: 0.659443, acc.: 62.50%] [G loss: 0.884376]\n",
      "280 [D loss: 0.619201, acc.: 65.62%] [G loss: 1.016441]\n",
      "281 [D loss: 0.704067, acc.: 56.25%] [G loss: 1.112286]\n",
      "282 [D loss: 0.618831, acc.: 65.62%] [G loss: 0.977183]\n",
      "283 [D loss: 0.624424, acc.: 57.81%] [G loss: 1.036362]\n",
      "284 [D loss: 0.692699, acc.: 54.69%] [G loss: 0.884090]\n",
      "285 [D loss: 0.661090, acc.: 48.44%] [G loss: 1.056029]\n",
      "286 [D loss: 0.691810, acc.: 56.25%] [G loss: 0.942376]\n",
      "287 [D loss: 0.607173, acc.: 65.62%] [G loss: 0.949038]\n",
      "288 [D loss: 0.673674, acc.: 57.81%] [G loss: 1.011462]\n",
      "289 [D loss: 0.699573, acc.: 59.38%] [G loss: 0.803339]\n",
      "290 [D loss: 0.635904, acc.: 60.94%] [G loss: 1.034584]\n",
      "291 [D loss: 0.662331, acc.: 53.12%] [G loss: 0.852729]\n",
      "292 [D loss: 0.664710, acc.: 65.62%] [G loss: 0.859405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "293 [D loss: 0.651627, acc.: 67.19%] [G loss: 0.957119]\n",
      "294 [D loss: 0.686679, acc.: 57.81%] [G loss: 0.961906]\n",
      "295 [D loss: 0.729168, acc.: 57.81%] [G loss: 0.861188]\n",
      "296 [D loss: 0.769717, acc.: 48.44%] [G loss: 0.915955]\n",
      "297 [D loss: 0.657793, acc.: 56.25%] [G loss: 0.767146]\n",
      "298 [D loss: 0.705653, acc.: 46.88%] [G loss: 0.765454]\n",
      "299 [D loss: 0.667809, acc.: 50.00%] [G loss: 0.916474]\n",
      "300 [D loss: 0.635858, acc.: 62.50%] [G loss: 0.823309]\n",
      "301 [D loss: 0.658579, acc.: 56.25%] [G loss: 0.814728]\n",
      "302 [D loss: 0.643366, acc.: 60.94%] [G loss: 1.092468]\n",
      "303 [D loss: 0.619048, acc.: 65.62%] [G loss: 0.966584]\n",
      "304 [D loss: 0.639210, acc.: 59.38%] [G loss: 0.944224]\n",
      "305 [D loss: 0.600970, acc.: 68.75%] [G loss: 1.095331]\n",
      "306 [D loss: 0.601249, acc.: 65.62%] [G loss: 0.943440]\n",
      "307 [D loss: 0.621926, acc.: 65.62%] [G loss: 1.154295]\n",
      "308 [D loss: 0.637989, acc.: 67.19%] [G loss: 1.001488]\n",
      "309 [D loss: 0.605847, acc.: 60.94%] [G loss: 1.123668]\n",
      "310 [D loss: 0.634894, acc.: 60.94%] [G loss: 0.919371]\n",
      "311 [D loss: 0.683480, acc.: 51.56%] [G loss: 1.266805]\n",
      "312 [D loss: 0.697982, acc.: 53.12%] [G loss: 1.043064]\n",
      "313 [D loss: 0.623058, acc.: 60.94%] [G loss: 0.830749]\n",
      "314 [D loss: 0.619617, acc.: 64.06%] [G loss: 0.863038]\n",
      "315 [D loss: 0.648067, acc.: 53.12%] [G loss: 0.855252]\n",
      "316 [D loss: 0.643752, acc.: 53.12%] [G loss: 0.813947]\n",
      "317 [D loss: 0.691104, acc.: 43.75%] [G loss: 0.937874]\n",
      "318 [D loss: 0.666603, acc.: 56.25%] [G loss: 0.810918]\n",
      "319 [D loss: 0.643543, acc.: 56.25%] [G loss: 0.849854]\n",
      "320 [D loss: 0.641298, acc.: 54.69%] [G loss: 0.928837]\n",
      "321 [D loss: 0.670895, acc.: 60.94%] [G loss: 0.942717]\n",
      "322 [D loss: 0.654141, acc.: 62.50%] [G loss: 1.225373]\n",
      "323 [D loss: 0.645558, acc.: 57.81%] [G loss: 0.879443]\n",
      "324 [D loss: 0.674436, acc.: 56.25%] [G loss: 1.101189]\n",
      "325 [D loss: 0.652153, acc.: 60.94%] [G loss: 1.176526]\n",
      "326 [D loss: 0.608879, acc.: 62.50%] [G loss: 1.065927]\n",
      "327 [D loss: 0.543891, acc.: 70.31%] [G loss: 1.015434]\n",
      "328 [D loss: 0.604814, acc.: 62.50%] [G loss: 1.056035]\n",
      "329 [D loss: 0.628492, acc.: 53.12%] [G loss: 1.250093]\n",
      "330 [D loss: 0.656769, acc.: 46.88%] [G loss: 1.279152]\n",
      "331 [D loss: 0.694507, acc.: 45.31%] [G loss: 1.149386]\n",
      "332 [D loss: 0.611511, acc.: 60.94%] [G loss: 1.017290]\n",
      "333 [D loss: 0.698769, acc.: 46.88%] [G loss: 1.084191]\n",
      "334 [D loss: 0.712374, acc.: 51.56%] [G loss: 0.977816]\n",
      "335 [D loss: 0.673535, acc.: 53.12%] [G loss: 0.774677]\n",
      "336 [D loss: 0.663293, acc.: 50.00%] [G loss: 0.937185]\n",
      "337 [D loss: 0.672906, acc.: 59.38%] [G loss: 0.765324]\n",
      "338 [D loss: 0.620042, acc.: 62.50%] [G loss: 0.756743]\n",
      "339 [D loss: 0.643756, acc.: 59.38%] [G loss: 0.872492]\n",
      "340 [D loss: 0.685622, acc.: 48.44%] [G loss: 0.901967]\n",
      "341 [D loss: 0.697667, acc.: 51.56%] [G loss: 0.821927]\n",
      "342 [D loss: 0.689841, acc.: 48.44%] [G loss: 1.014770]\n",
      "343 [D loss: 0.664756, acc.: 53.12%] [G loss: 0.872865]\n",
      "344 [D loss: 0.666775, acc.: 48.44%] [G loss: 0.778134]\n",
      "345 [D loss: 0.646339, acc.: 60.94%] [G loss: 0.884953]\n",
      "346 [D loss: 0.642377, acc.: 64.06%] [G loss: 1.025115]\n",
      "347 [D loss: 0.683230, acc.: 56.25%] [G loss: 0.955348]\n",
      "348 [D loss: 0.659583, acc.: 60.94%] [G loss: 0.921191]\n",
      "349 [D loss: 0.679519, acc.: 48.44%] [G loss: 0.953750]\n",
      "350 [D loss: 0.678588, acc.: 51.56%] [G loss: 0.874468]\n",
      "351 [D loss: 0.667846, acc.: 62.50%] [G loss: 0.841253]\n",
      "352 [D loss: 0.660586, acc.: 53.12%] [G loss: 0.885613]\n",
      "353 [D loss: 0.694199, acc.: 50.00%] [G loss: 0.886973]\n",
      "354 [D loss: 0.626440, acc.: 59.38%] [G loss: 0.924586]\n",
      "355 [D loss: 0.593007, acc.: 67.19%] [G loss: 0.974256]\n",
      "356 [D loss: 0.654620, acc.: 53.12%] [G loss: 0.998405]\n",
      "357 [D loss: 0.686965, acc.: 53.12%] [G loss: 0.948039]\n",
      "358 [D loss: 0.626129, acc.: 64.06%] [G loss: 0.884442]\n",
      "359 [D loss: 0.639092, acc.: 56.25%] [G loss: 1.124224]\n",
      "360 [D loss: 0.688050, acc.: 46.88%] [G loss: 0.791109]\n",
      "361 [D loss: 0.630456, acc.: 56.25%] [G loss: 0.868723]\n",
      "362 [D loss: 0.740001, acc.: 43.75%] [G loss: 0.782567]\n",
      "363 [D loss: 0.654527, acc.: 50.00%] [G loss: 0.921685]\n",
      "364 [D loss: 0.660812, acc.: 50.00%] [G loss: 0.826735]\n",
      "365 [D loss: 0.651648, acc.: 62.50%] [G loss: 0.880406]\n",
      "366 [D loss: 0.612017, acc.: 56.25%] [G loss: 1.063759]\n",
      "367 [D loss: 0.614606, acc.: 60.94%] [G loss: 1.009573]\n",
      "368 [D loss: 0.626849, acc.: 60.94%] [G loss: 1.054487]\n",
      "369 [D loss: 0.677187, acc.: 51.56%] [G loss: 0.984182]\n",
      "370 [D loss: 0.671779, acc.: 54.69%] [G loss: 0.855735]\n",
      "371 [D loss: 0.621499, acc.: 64.06%] [G loss: 0.830159]\n",
      "372 [D loss: 0.646896, acc.: 62.50%] [G loss: 0.896354]\n",
      "373 [D loss: 0.635813, acc.: 65.62%] [G loss: 0.949318]\n",
      "374 [D loss: 0.680110, acc.: 51.56%] [G loss: 1.007681]\n",
      "375 [D loss: 0.650095, acc.: 51.56%] [G loss: 0.928826]\n",
      "376 [D loss: 0.686978, acc.: 53.12%] [G loss: 0.921815]\n",
      "377 [D loss: 0.677724, acc.: 53.12%] [G loss: 0.812028]\n",
      "378 [D loss: 0.652397, acc.: 53.12%] [G loss: 1.040981]\n",
      "379 [D loss: 0.647045, acc.: 60.94%] [G loss: 1.109730]\n",
      "380 [D loss: 0.649339, acc.: 64.06%] [G loss: 0.845112]\n",
      "381 [D loss: 0.671411, acc.: 54.69%] [G loss: 0.952852]\n",
      "382 [D loss: 0.718866, acc.: 45.31%] [G loss: 0.850971]\n",
      "383 [D loss: 0.716593, acc.: 48.44%] [G loss: 0.830786]\n",
      "384 [D loss: 0.677794, acc.: 46.88%] [G loss: 0.794096]\n",
      "385 [D loss: 0.682081, acc.: 43.75%] [G loss: 0.716228]\n",
      "386 [D loss: 0.660168, acc.: 56.25%] [G loss: 0.862912]\n",
      "387 [D loss: 0.656434, acc.: 51.56%] [G loss: 0.855487]\n",
      "388 [D loss: 0.707380, acc.: 48.44%] [G loss: 0.881711]\n",
      "389 [D loss: 0.670694, acc.: 56.25%] [G loss: 0.851299]\n",
      "390 [D loss: 0.665001, acc.: 51.56%] [G loss: 0.928061]\n",
      "391 [D loss: 0.637759, acc.: 67.19%] [G loss: 0.940919]\n",
      "392 [D loss: 0.675581, acc.: 57.81%] [G loss: 0.867479]\n",
      "393 [D loss: 0.640869, acc.: 57.81%] [G loss: 0.841239]\n",
      "394 [D loss: 0.654868, acc.: 60.94%] [G loss: 0.829204]\n",
      "395 [D loss: 0.654103, acc.: 56.25%] [G loss: 0.902520]\n",
      "396 [D loss: 0.679558, acc.: 62.50%] [G loss: 0.808223]\n",
      "397 [D loss: 0.644193, acc.: 62.50%] [G loss: 0.787867]\n",
      "398 [D loss: 0.669567, acc.: 51.56%] [G loss: 0.803250]\n",
      "399 [D loss: 0.658444, acc.: 59.38%] [G loss: 0.890589]\n",
      "400 [D loss: 0.667093, acc.: 50.00%] [G loss: 0.886836]\n",
      "1 âsá» : [0]\n",
      "1 giÃ¡ cam hÃ  ÄÃºc hb : [1]\n",
      "25 1111 02 on b rm 272 05 : [2]\n",
      "1 30 1 000 ÄÆ¡n 3 giá» 3 : [3]\n",
      "sl 1 35 bk : [4]\n",
      "020 âtax 0 toÃ n ÄÆ¡n : [5]\n",
      "401 [D loss: 0.690007, acc.: 40.62%] [G loss: 0.757549]\n",
      "402 [D loss: 0.705183, acc.: 48.44%] [G loss: 0.722176]\n",
      "403 [D loss: 0.662862, acc.: 56.25%] [G loss: 0.783658]\n",
      "404 [D loss: 0.695149, acc.: 53.12%] [G loss: 0.834661]\n",
      "405 [D loss: 0.671687, acc.: 60.94%] [G loss: 0.776050]\n",
      "406 [D loss: 0.635379, acc.: 60.94%] [G loss: 0.998577]\n",
      "407 [D loss: 0.675742, acc.: 51.56%] [G loss: 0.747038]\n",
      "408 [D loss: 0.637980, acc.: 60.94%] [G loss: 0.931333]\n",
      "409 [D loss: 0.673950, acc.: 54.69%] [G loss: 0.908213]\n",
      "410 [D loss: 0.691837, acc.: 56.25%] [G loss: 0.716995]\n",
      "411 [D loss: 0.635495, acc.: 56.25%] [G loss: 0.803176]\n",
      "412 [D loss: 0.648618, acc.: 54.69%] [G loss: 0.991901]\n",
      "413 [D loss: 0.677823, acc.: 43.75%] [G loss: 0.764865]\n",
      "414 [D loss: 0.667323, acc.: 56.25%] [G loss: 0.863756]\n",
      "415 [D loss: 0.645213, acc.: 54.69%] [G loss: 0.807993]\n",
      "416 [D loss: 0.680091, acc.: 56.25%] [G loss: 0.798343]\n",
      "417 [D loss: 0.615054, acc.: 68.75%] [G loss: 0.858685]\n",
      "418 [D loss: 0.692824, acc.: 56.25%] [G loss: 0.823380]\n",
      "419 [D loss: 0.607317, acc.: 71.88%] [G loss: 0.704026]\n",
      "420 [D loss: 0.622489, acc.: 67.19%] [G loss: 0.849656]\n",
      "421 [D loss: 0.683481, acc.: 54.69%] [G loss: 0.874514]\n",
      "422 [D loss: 0.653326, acc.: 68.75%] [G loss: 0.872503]\n",
      "423 [D loss: 0.632439, acc.: 57.81%] [G loss: 0.792465]\n",
      "424 [D loss: 0.629261, acc.: 62.50%] [G loss: 0.805502]\n",
      "425 [D loss: 0.692880, acc.: 65.62%] [G loss: 0.835930]\n",
      "426 [D loss: 0.691394, acc.: 57.81%] [G loss: 0.803587]\n",
      "427 [D loss: 0.632190, acc.: 60.94%] [G loss: 0.774965]\n",
      "428 [D loss: 0.641786, acc.: 59.38%] [G loss: 0.717186]\n",
      "429 [D loss: 0.646816, acc.: 59.38%] [G loss: 0.945093]\n",
      "430 [D loss: 0.644727, acc.: 57.81%] [G loss: 0.808500]\n",
      "431 [D loss: 0.649720, acc.: 59.38%] [G loss: 0.732344]\n",
      "432 [D loss: 0.628123, acc.: 53.12%] [G loss: 0.834277]\n",
      "433 [D loss: 0.660641, acc.: 50.00%] [G loss: 0.818028]\n",
      "434 [D loss: 0.654170, acc.: 59.38%] [G loss: 0.812051]\n",
      "435 [D loss: 0.643766, acc.: 70.31%] [G loss: 0.742902]\n",
      "436 [D loss: 0.651640, acc.: 59.38%] [G loss: 0.887935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437 [D loss: 0.633212, acc.: 62.50%] [G loss: 0.712187]\n",
      "438 [D loss: 0.626322, acc.: 65.62%] [G loss: 0.783740]\n",
      "439 [D loss: 0.648858, acc.: 67.19%] [G loss: 0.758065]\n",
      "440 [D loss: 0.667444, acc.: 59.38%] [G loss: 0.894024]\n",
      "441 [D loss: 0.640753, acc.: 57.81%] [G loss: 0.735364]\n",
      "442 [D loss: 0.706144, acc.: 53.12%] [G loss: 0.780226]\n",
      "443 [D loss: 0.661038, acc.: 51.56%] [G loss: 0.812747]\n",
      "444 [D loss: 0.647652, acc.: 54.69%] [G loss: 0.810735]\n",
      "445 [D loss: 0.638809, acc.: 60.94%] [G loss: 0.804912]\n",
      "446 [D loss: 0.639736, acc.: 48.44%] [G loss: 0.820836]\n",
      "447 [D loss: 0.718262, acc.: 53.12%] [G loss: 0.741407]\n",
      "448 [D loss: 0.693690, acc.: 42.19%] [G loss: 1.035236]\n",
      "449 [D loss: 0.654014, acc.: 64.06%] [G loss: 0.945997]\n",
      "450 [D loss: 0.648401, acc.: 64.06%] [G loss: 1.001210]\n",
      "451 [D loss: 0.633861, acc.: 59.38%] [G loss: 1.090546]\n",
      "452 [D loss: 0.690725, acc.: 57.81%] [G loss: 0.993211]\n",
      "453 [D loss: 0.617802, acc.: 67.19%] [G loss: 0.913911]\n",
      "454 [D loss: 0.631733, acc.: 57.81%] [G loss: 1.108774]\n",
      "455 [D loss: 0.632921, acc.: 70.31%] [G loss: 0.907230]\n",
      "456 [D loss: 0.689894, acc.: 50.00%] [G loss: 0.987374]\n",
      "457 [D loss: 0.639389, acc.: 62.50%] [G loss: 0.868859]\n",
      "458 [D loss: 0.639300, acc.: 64.06%] [G loss: 0.819322]\n",
      "459 [D loss: 0.653689, acc.: 60.94%] [G loss: 0.828140]\n",
      "460 [D loss: 0.668143, acc.: 59.38%] [G loss: 0.829494]\n",
      "461 [D loss: 0.670566, acc.: 59.38%] [G loss: 0.951277]\n",
      "462 [D loss: 0.673375, acc.: 54.69%] [G loss: 0.880615]\n",
      "463 [D loss: 0.669277, acc.: 53.12%] [G loss: 0.982371]\n",
      "464 [D loss: 0.731400, acc.: 43.75%] [G loss: 0.972292]\n",
      "465 [D loss: 0.631156, acc.: 67.19%] [G loss: 1.093498]\n",
      "466 [D loss: 0.642413, acc.: 60.94%] [G loss: 0.916895]\n",
      "467 [D loss: 0.710206, acc.: 45.31%] [G loss: 1.052056]\n",
      "468 [D loss: 0.698985, acc.: 46.88%] [G loss: 1.029894]\n",
      "469 [D loss: 0.687588, acc.: 59.38%] [G loss: 0.916134]\n",
      "470 [D loss: 0.651309, acc.: 64.06%] [G loss: 0.927071]\n",
      "471 [D loss: 0.679587, acc.: 46.88%] [G loss: 0.952595]\n",
      "472 [D loss: 0.590524, acc.: 67.19%] [G loss: 0.888640]\n",
      "473 [D loss: 0.665498, acc.: 60.94%] [G loss: 0.940296]\n",
      "474 [D loss: 0.625454, acc.: 64.06%] [G loss: 0.941465]\n",
      "475 [D loss: 0.684813, acc.: 59.38%] [G loss: 0.727392]\n",
      "476 [D loss: 0.623763, acc.: 56.25%] [G loss: 0.837842]\n",
      "477 [D loss: 0.690160, acc.: 48.44%] [G loss: 0.770993]\n",
      "478 [D loss: 0.638482, acc.: 65.62%] [G loss: 0.649782]\n",
      "479 [D loss: 0.631472, acc.: 60.94%] [G loss: 0.792927]\n",
      "480 [D loss: 0.654891, acc.: 54.69%] [G loss: 0.866896]\n",
      "481 [D loss: 0.649782, acc.: 54.69%] [G loss: 1.139957]\n",
      "482 [D loss: 0.623654, acc.: 56.25%] [G loss: 1.079791]\n",
      "483 [D loss: 0.624132, acc.: 60.94%] [G loss: 1.076561]\n",
      "484 [D loss: 0.742820, acc.: 48.44%] [G loss: 0.916828]\n",
      "485 [D loss: 0.641239, acc.: 67.19%] [G loss: 0.848325]\n",
      "486 [D loss: 0.617413, acc.: 64.06%] [G loss: 0.894421]\n",
      "487 [D loss: 0.663088, acc.: 60.94%] [G loss: 0.985387]\n",
      "488 [D loss: 0.640527, acc.: 70.31%] [G loss: 0.954606]\n",
      "489 [D loss: 0.640141, acc.: 59.38%] [G loss: 0.939668]\n",
      "490 [D loss: 0.671340, acc.: 54.69%] [G loss: 0.881550]\n",
      "491 [D loss: 0.614310, acc.: 68.75%] [G loss: 0.855178]\n",
      "492 [D loss: 0.687874, acc.: 54.69%] [G loss: 0.705905]\n",
      "493 [D loss: 0.611340, acc.: 65.62%] [G loss: 0.844031]\n",
      "494 [D loss: 0.589610, acc.: 70.31%] [G loss: 0.773853]\n",
      "495 [D loss: 0.694958, acc.: 51.56%] [G loss: 0.869012]\n",
      "496 [D loss: 0.619659, acc.: 64.06%] [G loss: 0.733222]\n",
      "497 [D loss: 0.646443, acc.: 57.81%] [G loss: 0.811784]\n",
      "498 [D loss: 0.686197, acc.: 56.25%] [G loss: 0.775795]\n",
      "499 [D loss: 0.634367, acc.: 64.06%] [G loss: 0.842280]\n",
      "500 [D loss: 0.684999, acc.: 51.56%] [G loss: 0.708793]\n",
      "501 [D loss: 0.643892, acc.: 62.50%] [G loss: 0.883546]\n",
      "502 [D loss: 0.676285, acc.: 50.00%] [G loss: 0.849851]\n",
      "503 [D loss: 0.602420, acc.: 76.56%] [G loss: 0.767429]\n",
      "504 [D loss: 0.619443, acc.: 65.62%] [G loss: 0.858187]\n",
      "505 [D loss: 0.626071, acc.: 67.19%] [G loss: 0.944048]\n",
      "506 [D loss: 0.692267, acc.: 60.94%] [G loss: 0.761418]\n",
      "507 [D loss: 0.696927, acc.: 53.12%] [G loss: 0.815556]\n",
      "508 [D loss: 0.648934, acc.: 64.06%] [G loss: 0.848779]\n",
      "509 [D loss: 0.616540, acc.: 64.06%] [G loss: 1.035986]\n",
      "510 [D loss: 0.663612, acc.: 57.81%] [G loss: 0.934666]\n",
      "511 [D loss: 0.691864, acc.: 48.44%] [G loss: 0.983460]\n",
      "512 [D loss: 0.691055, acc.: 48.44%] [G loss: 0.857771]\n",
      "513 [D loss: 0.629780, acc.: 57.81%] [G loss: 1.015958]\n",
      "514 [D loss: 0.637805, acc.: 64.06%] [G loss: 0.868197]\n",
      "515 [D loss: 0.624617, acc.: 71.88%] [G loss: 0.875356]\n",
      "516 [D loss: 0.706859, acc.: 53.12%] [G loss: 0.938343]\n",
      "517 [D loss: 0.702948, acc.: 42.19%] [G loss: 1.088178]\n",
      "518 [D loss: 0.618185, acc.: 59.38%] [G loss: 0.745034]\n",
      "519 [D loss: 0.711925, acc.: 51.56%] [G loss: 0.772177]\n",
      "520 [D loss: 0.656183, acc.: 56.25%] [G loss: 0.954912]\n",
      "521 [D loss: 0.637157, acc.: 60.94%] [G loss: 0.771164]\n",
      "522 [D loss: 0.678278, acc.: 50.00%] [G loss: 0.689029]\n",
      "523 [D loss: 0.710196, acc.: 56.25%] [G loss: 0.827831]\n",
      "524 [D loss: 0.640953, acc.: 57.81%] [G loss: 0.854657]\n",
      "525 [D loss: 0.632834, acc.: 57.81%] [G loss: 0.725693]\n",
      "526 [D loss: 0.652200, acc.: 59.38%] [G loss: 0.815526]\n",
      "527 [D loss: 0.600706, acc.: 73.44%] [G loss: 0.675564]\n",
      "528 [D loss: 0.611513, acc.: 67.19%] [G loss: 0.817540]\n",
      "529 [D loss: 0.615894, acc.: 62.50%] [G loss: 0.719203]\n",
      "530 [D loss: 0.675130, acc.: 62.50%] [G loss: 0.986024]\n",
      "531 [D loss: 0.657017, acc.: 53.12%] [G loss: 0.806400]\n",
      "532 [D loss: 0.698602, acc.: 56.25%] [G loss: 0.822668]\n",
      "533 [D loss: 0.660486, acc.: 57.81%] [G loss: 0.784654]\n",
      "534 [D loss: 0.684263, acc.: 50.00%] [G loss: 0.776697]\n",
      "535 [D loss: 0.686056, acc.: 53.12%] [G loss: 0.753543]\n",
      "536 [D loss: 0.646755, acc.: 59.38%] [G loss: 0.842868]\n",
      "537 [D loss: 0.692488, acc.: 53.12%] [G loss: 0.786821]\n",
      "538 [D loss: 0.701127, acc.: 56.25%] [G loss: 0.742197]\n",
      "539 [D loss: 0.652888, acc.: 62.50%] [G loss: 0.828088]\n",
      "540 [D loss: 0.688612, acc.: 50.00%] [G loss: 0.828937]\n",
      "541 [D loss: 0.658028, acc.: 59.38%] [G loss: 0.846675]\n",
      "542 [D loss: 0.684990, acc.: 59.38%] [G loss: 0.868787]\n",
      "543 [D loss: 0.679542, acc.: 57.81%] [G loss: 0.741607]\n",
      "544 [D loss: 0.699433, acc.: 50.00%] [G loss: 0.843595]\n",
      "545 [D loss: 0.670032, acc.: 57.81%] [G loss: 0.665338]\n",
      "546 [D loss: 0.669491, acc.: 59.38%] [G loss: 0.779408]\n",
      "547 [D loss: 0.695189, acc.: 50.00%] [G loss: 0.786831]\n",
      "548 [D loss: 0.691609, acc.: 51.56%] [G loss: 0.777600]\n",
      "549 [D loss: 0.709466, acc.: 51.56%] [G loss: 1.024058]\n",
      "550 [D loss: 0.667829, acc.: 56.25%] [G loss: 0.758597]\n",
      "551 [D loss: 0.676133, acc.: 56.25%] [G loss: 0.797939]\n",
      "552 [D loss: 0.671548, acc.: 59.38%] [G loss: 0.763505]\n",
      "553 [D loss: 0.677198, acc.: 54.69%] [G loss: 0.854117]\n",
      "554 [D loss: 0.659212, acc.: 56.25%] [G loss: 0.806155]\n",
      "555 [D loss: 0.650276, acc.: 62.50%] [G loss: 0.816743]\n",
      "556 [D loss: 0.632745, acc.: 71.88%] [G loss: 0.883370]\n",
      "557 [D loss: 0.625108, acc.: 64.06%] [G loss: 0.824842]\n",
      "558 [D loss: 0.694123, acc.: 53.12%] [G loss: 0.795110]\n",
      "559 [D loss: 0.672306, acc.: 64.06%] [G loss: 0.762477]\n",
      "560 [D loss: 0.669804, acc.: 59.38%] [G loss: 0.704321]\n",
      "561 [D loss: 0.655059, acc.: 62.50%] [G loss: 0.732651]\n",
      "562 [D loss: 0.656600, acc.: 64.06%] [G loss: 0.714322]\n",
      "563 [D loss: 0.658787, acc.: 57.81%] [G loss: 0.724859]\n",
      "564 [D loss: 0.690314, acc.: 46.88%] [G loss: 0.823478]\n",
      "565 [D loss: 0.692578, acc.: 54.69%] [G loss: 0.739364]\n",
      "566 [D loss: 0.670060, acc.: 59.38%] [G loss: 0.829035]\n",
      "567 [D loss: 0.686509, acc.: 56.25%] [G loss: 0.801032]\n",
      "568 [D loss: 0.676009, acc.: 59.38%] [G loss: 0.811540]\n",
      "569 [D loss: 0.679238, acc.: 60.94%] [G loss: 0.779518]\n",
      "570 [D loss: 0.687996, acc.: 53.12%] [G loss: 0.831735]\n",
      "571 [D loss: 0.651383, acc.: 64.06%] [G loss: 0.807514]\n",
      "572 [D loss: 0.618163, acc.: 64.06%] [G loss: 0.931807]\n",
      "573 [D loss: 0.654356, acc.: 62.50%] [G loss: 1.077473]\n",
      "574 [D loss: 0.645084, acc.: 54.69%] [G loss: 0.994431]\n",
      "575 [D loss: 0.605068, acc.: 68.75%] [G loss: 0.854245]\n",
      "576 [D loss: 0.616278, acc.: 65.62%] [G loss: 1.024961]\n",
      "577 [D loss: 0.608986, acc.: 70.31%] [G loss: 0.828215]\n",
      "578 [D loss: 0.689052, acc.: 59.38%] [G loss: 0.732576]\n",
      "579 [D loss: 0.646290, acc.: 57.81%] [G loss: 0.833669]\n",
      "580 [D loss: 0.691201, acc.: 51.56%] [G loss: 0.776660]\n",
      "581 [D loss: 0.660870, acc.: 54.69%] [G loss: 0.885198]\n",
      "582 [D loss: 0.660921, acc.: 68.75%] [G loss: 0.779057]\n",
      "583 [D loss: 0.686970, acc.: 54.69%] [G loss: 0.789806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584 [D loss: 0.636628, acc.: 70.31%] [G loss: 0.775253]\n",
      "585 [D loss: 0.619562, acc.: 64.06%] [G loss: 0.768499]\n",
      "586 [D loss: 0.699735, acc.: 51.56%] [G loss: 0.729666]\n",
      "587 [D loss: 0.696247, acc.: 60.94%] [G loss: 0.760225]\n",
      "588 [D loss: 0.665536, acc.: 60.94%] [G loss: 0.813685]\n",
      "589 [D loss: 0.647503, acc.: 67.19%] [G loss: 0.696779]\n",
      "590 [D loss: 0.705370, acc.: 54.69%] [G loss: 0.748379]\n",
      "591 [D loss: 0.680239, acc.: 60.94%] [G loss: 0.785436]\n",
      "592 [D loss: 0.665225, acc.: 53.12%] [G loss: 0.697977]\n",
      "593 [D loss: 0.709345, acc.: 43.75%] [G loss: 0.813553]\n",
      "594 [D loss: 0.688038, acc.: 59.38%] [G loss: 0.751330]\n",
      "595 [D loss: 0.671233, acc.: 54.69%] [G loss: 0.803331]\n",
      "596 [D loss: 0.615544, acc.: 59.38%] [G loss: 1.152260]\n",
      "597 [D loss: 0.675820, acc.: 53.12%] [G loss: 0.880345]\n",
      "598 [D loss: 0.638104, acc.: 56.25%] [G loss: 1.012588]\n",
      "599 [D loss: 0.630908, acc.: 65.62%] [G loss: 1.007157]\n",
      "600 [D loss: 0.724693, acc.: 56.25%] [G loss: 0.812797]\n",
      "ngÃ y tax tao : [0]\n",
      "administrator p xin socola 96 giá» trÄm : [1]\n",
      "láº» âvinntaba 8970669 1 trÃ  tá»ng tinh 19 : [2]\n",
      "hÃ  xin ca 0 kiwi stt bien rm : [3]\n",
      "chanh 925 : [4]\n",
      "canh : [5]\n",
      "601 [D loss: 0.709032, acc.: 57.81%] [G loss: 0.700186]\n",
      "602 [D loss: 0.659252, acc.: 57.81%] [G loss: 0.774473]\n",
      "603 [D loss: 0.689680, acc.: 43.75%] [G loss: 0.841167]\n",
      "604 [D loss: 0.642712, acc.: 57.81%] [G loss: 0.834301]\n",
      "605 [D loss: 0.711449, acc.: 48.44%] [G loss: 0.666709]\n",
      "606 [D loss: 0.684460, acc.: 43.75%] [G loss: 0.723893]\n",
      "607 [D loss: 0.741829, acc.: 39.06%] [G loss: 0.985896]\n",
      "608 [D loss: 0.677721, acc.: 45.31%] [G loss: 0.966794]\n",
      "609 [D loss: 0.686223, acc.: 50.00%] [G loss: 0.930647]\n",
      "610 [D loss: 0.643335, acc.: 59.38%] [G loss: 0.847385]\n",
      "611 [D loss: 0.634152, acc.: 64.06%] [G loss: 0.836817]\n",
      "612 [D loss: 0.672485, acc.: 54.69%] [G loss: 0.833805]\n",
      "613 [D loss: 0.657167, acc.: 53.12%] [G loss: 0.876381]\n",
      "614 [D loss: 0.641899, acc.: 68.75%] [G loss: 0.842295]\n",
      "615 [D loss: 0.704002, acc.: 53.12%] [G loss: 0.795228]\n",
      "616 [D loss: 0.666234, acc.: 60.94%] [G loss: 0.771211]\n",
      "617 [D loss: 0.655242, acc.: 59.38%] [G loss: 0.639296]\n",
      "618 [D loss: 0.618312, acc.: 67.19%] [G loss: 0.818065]\n",
      "619 [D loss: 0.647529, acc.: 60.94%] [G loss: 0.854777]\n",
      "620 [D loss: 0.621121, acc.: 65.62%] [G loss: 0.893344]\n",
      "621 [D loss: 0.636614, acc.: 67.19%] [G loss: 0.852909]\n",
      "622 [D loss: 0.651009, acc.: 67.19%] [G loss: 0.882149]\n",
      "623 [D loss: 0.593340, acc.: 68.75%] [G loss: 0.895648]\n",
      "624 [D loss: 0.637305, acc.: 59.38%] [G loss: 0.813580]\n",
      "625 [D loss: 0.662006, acc.: 60.94%] [G loss: 0.863463]\n",
      "626 [D loss: 0.619822, acc.: 62.50%] [G loss: 0.883211]\n",
      "627 [D loss: 0.666941, acc.: 54.69%] [G loss: 0.767520]\n",
      "628 [D loss: 0.690237, acc.: 46.88%] [G loss: 0.791962]\n",
      "629 [D loss: 0.659213, acc.: 62.50%] [G loss: 0.870360]\n",
      "630 [D loss: 0.709118, acc.: 57.81%] [G loss: 0.920201]\n",
      "631 [D loss: 0.652912, acc.: 60.94%] [G loss: 0.830657]\n",
      "632 [D loss: 0.683285, acc.: 54.69%] [G loss: 0.819092]\n",
      "633 [D loss: 0.667874, acc.: 60.94%] [G loss: 0.877107]\n",
      "634 [D loss: 0.686509, acc.: 65.62%] [G loss: 0.861360]\n",
      "635 [D loss: 0.692477, acc.: 50.00%] [G loss: 0.799727]\n",
      "636 [D loss: 0.684970, acc.: 56.25%] [G loss: 0.829645]\n",
      "637 [D loss: 0.725048, acc.: 57.81%] [G loss: 0.820674]\n",
      "638 [D loss: 0.697084, acc.: 50.00%] [G loss: 0.901818]\n",
      "639 [D loss: 0.660247, acc.: 50.00%] [G loss: 0.904619]\n",
      "640 [D loss: 0.691900, acc.: 51.56%] [G loss: 0.920310]\n",
      "641 [D loss: 0.654316, acc.: 62.50%] [G loss: 0.908764]\n",
      "642 [D loss: 0.697364, acc.: 56.25%] [G loss: 0.877579]\n",
      "643 [D loss: 0.643973, acc.: 56.25%] [G loss: 0.988520]\n",
      "644 [D loss: 0.659424, acc.: 60.94%] [G loss: 0.793212]\n",
      "645 [D loss: 0.639375, acc.: 60.94%] [G loss: 0.802782]\n",
      "646 [D loss: 0.698192, acc.: 59.38%] [G loss: 0.841789]\n",
      "647 [D loss: 0.684184, acc.: 59.38%] [G loss: 0.806223]\n",
      "648 [D loss: 0.665552, acc.: 65.62%] [G loss: 0.788523]\n",
      "649 [D loss: 0.652204, acc.: 56.25%] [G loss: 0.806995]\n",
      "650 [D loss: 0.672844, acc.: 51.56%] [G loss: 0.874844]\n",
      "651 [D loss: 0.680028, acc.: 48.44%] [G loss: 0.746529]\n",
      "652 [D loss: 0.651737, acc.: 62.50%] [G loss: 0.752239]\n",
      "653 [D loss: 0.707034, acc.: 50.00%] [G loss: 0.697225]\n",
      "654 [D loss: 0.679294, acc.: 53.12%] [G loss: 0.792353]\n",
      "655 [D loss: 0.679754, acc.: 60.94%] [G loss: 0.775124]\n",
      "656 [D loss: 0.679005, acc.: 60.94%] [G loss: 0.795115]\n",
      "657 [D loss: 0.705590, acc.: 51.56%] [G loss: 0.756666]\n",
      "658 [D loss: 0.669783, acc.: 64.06%] [G loss: 0.765398]\n",
      "659 [D loss: 0.711656, acc.: 54.69%] [G loss: 0.706377]\n",
      "660 [D loss: 0.669376, acc.: 57.81%] [G loss: 0.756057]\n",
      "661 [D loss: 0.656945, acc.: 53.12%] [G loss: 0.798986]\n",
      "662 [D loss: 0.652235, acc.: 56.25%] [G loss: 0.838541]\n",
      "663 [D loss: 0.678296, acc.: 53.12%] [G loss: 0.862070]\n",
      "664 [D loss: 0.655966, acc.: 59.38%] [G loss: 0.735722]\n",
      "665 [D loss: 0.651240, acc.: 62.50%] [G loss: 0.832120]\n",
      "666 [D loss: 0.687268, acc.: 59.38%] [G loss: 0.783565]\n",
      "667 [D loss: 0.643395, acc.: 67.19%] [G loss: 0.857873]\n",
      "668 [D loss: 0.737503, acc.: 56.25%] [G loss: 0.741279]\n",
      "669 [D loss: 0.709399, acc.: 54.69%] [G loss: 0.840131]\n",
      "670 [D loss: 0.655639, acc.: 64.06%] [G loss: 0.823025]\n",
      "671 [D loss: 0.667441, acc.: 56.25%] [G loss: 0.657513]\n",
      "672 [D loss: 0.624548, acc.: 60.94%] [G loss: 0.811613]\n",
      "673 [D loss: 0.686199, acc.: 50.00%] [G loss: 0.800831]\n",
      "674 [D loss: 0.677164, acc.: 53.12%] [G loss: 0.793249]\n",
      "675 [D loss: 0.701998, acc.: 48.44%] [G loss: 0.719933]\n",
      "676 [D loss: 0.653321, acc.: 68.75%] [G loss: 0.795066]\n",
      "677 [D loss: 0.689546, acc.: 51.56%] [G loss: 0.751364]\n",
      "678 [D loss: 0.654304, acc.: 64.06%] [G loss: 0.807311]\n",
      "679 [D loss: 0.625149, acc.: 68.75%] [G loss: 0.728799]\n",
      "680 [D loss: 0.642241, acc.: 67.19%] [G loss: 0.714215]\n",
      "681 [D loss: 0.697186, acc.: 60.94%] [G loss: 0.707522]\n",
      "682 [D loss: 0.675187, acc.: 54.69%] [G loss: 0.792015]\n",
      "683 [D loss: 0.622560, acc.: 68.75%] [G loss: 0.893858]\n",
      "684 [D loss: 0.745180, acc.: 51.56%] [G loss: 0.794478]\n",
      "685 [D loss: 0.675551, acc.: 65.62%] [G loss: 0.780040]\n",
      "686 [D loss: 0.697244, acc.: 60.94%] [G loss: 0.864416]\n",
      "687 [D loss: 0.627458, acc.: 71.88%] [G loss: 0.797874]\n",
      "688 [D loss: 0.627722, acc.: 68.75%] [G loss: 0.743566]\n",
      "689 [D loss: 0.708104, acc.: 60.94%] [G loss: 0.756827]\n",
      "690 [D loss: 0.682793, acc.: 62.50%] [G loss: 0.831270]\n",
      "691 [D loss: 0.701923, acc.: 53.12%] [G loss: 0.847541]\n",
      "692 [D loss: 0.665322, acc.: 48.44%] [G loss: 0.752084]\n",
      "693 [D loss: 0.642742, acc.: 60.94%] [G loss: 0.896197]\n",
      "694 [D loss: 0.703573, acc.: 54.69%] [G loss: 0.750816]\n",
      "695 [D loss: 0.703748, acc.: 50.00%] [G loss: 0.771685]\n",
      "696 [D loss: 0.661174, acc.: 65.62%] [G loss: 0.739052]\n",
      "697 [D loss: 0.680852, acc.: 64.06%] [G loss: 0.762093]\n",
      "698 [D loss: 0.671408, acc.: 59.38%] [G loss: 0.836807]\n",
      "699 [D loss: 0.715079, acc.: 64.06%] [G loss: 0.822378]\n",
      "700 [D loss: 0.654079, acc.: 67.19%] [G loss: 0.769026]\n",
      "701 [D loss: 0.648798, acc.: 62.50%] [G loss: 0.776381]\n",
      "702 [D loss: 0.710138, acc.: 45.31%] [G loss: 0.771777]\n",
      "703 [D loss: 0.625644, acc.: 67.19%] [G loss: 0.825347]\n",
      "704 [D loss: 0.652092, acc.: 67.19%] [G loss: 0.750127]\n",
      "705 [D loss: 0.646351, acc.: 53.12%] [G loss: 0.787475]\n",
      "706 [D loss: 0.612651, acc.: 70.31%] [G loss: 0.888942]\n",
      "707 [D loss: 0.585034, acc.: 68.75%] [G loss: 0.771150]\n",
      "708 [D loss: 0.676380, acc.: 53.12%] [G loss: 0.713983]\n",
      "709 [D loss: 0.672557, acc.: 56.25%] [G loss: 0.755036]\n",
      "710 [D loss: 0.666066, acc.: 56.25%] [G loss: 0.948524]\n",
      "711 [D loss: 0.662299, acc.: 54.69%] [G loss: 0.903647]\n",
      "712 [D loss: 0.690950, acc.: 54.69%] [G loss: 1.059996]\n",
      "713 [D loss: 0.641532, acc.: 57.81%] [G loss: 1.115815]\n",
      "714 [D loss: 0.680483, acc.: 54.69%] [G loss: 0.827450]\n",
      "715 [D loss: 0.725355, acc.: 43.75%] [G loss: 0.941670]\n",
      "716 [D loss: 0.661458, acc.: 59.38%] [G loss: 0.868652]\n",
      "717 [D loss: 0.713320, acc.: 42.19%] [G loss: 0.810253]\n",
      "718 [D loss: 0.766426, acc.: 45.31%] [G loss: 0.779490]\n",
      "719 [D loss: 0.654277, acc.: 67.19%] [G loss: 0.756972]\n",
      "720 [D loss: 0.713521, acc.: 48.44%] [G loss: 0.723120]\n",
      "721 [D loss: 0.677536, acc.: 59.38%] [G loss: 0.793005]\n",
      "722 [D loss: 0.654715, acc.: 65.62%] [G loss: 0.803720]\n",
      "723 [D loss: 0.618602, acc.: 75.00%] [G loss: 0.891036]\n",
      "724 [D loss: 0.657564, acc.: 68.75%] [G loss: 0.755369]\n",
      "725 [D loss: 0.708775, acc.: 53.12%] [G loss: 0.796417]\n",
      "726 [D loss: 0.665849, acc.: 65.62%] [G loss: 0.771040]\n",
      "727 [D loss: 0.667607, acc.: 70.31%] [G loss: 0.778998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728 [D loss: 0.679143, acc.: 62.50%] [G loss: 0.688488]\n",
      "729 [D loss: 0.617537, acc.: 75.00%] [G loss: 0.812118]\n",
      "730 [D loss: 0.627789, acc.: 65.62%] [G loss: 0.782855]\n",
      "731 [D loss: 0.661833, acc.: 60.94%] [G loss: 0.734443]\n",
      "732 [D loss: 0.669286, acc.: 53.12%] [G loss: 0.777917]\n",
      "733 [D loss: 0.693772, acc.: 48.44%] [G loss: 1.010529]\n",
      "734 [D loss: 0.601294, acc.: 70.31%] [G loss: 0.911821]\n",
      "735 [D loss: 0.661862, acc.: 51.56%] [G loss: 0.906765]\n",
      "736 [D loss: 0.683103, acc.: 57.81%] [G loss: 0.899082]\n",
      "737 [D loss: 0.656815, acc.: 73.44%] [G loss: 0.852194]\n",
      "738 [D loss: 0.671778, acc.: 62.50%] [G loss: 0.878032]\n",
      "739 [D loss: 0.629066, acc.: 62.50%] [G loss: 0.809543]\n",
      "740 [D loss: 0.655806, acc.: 64.06%] [G loss: 0.744316]\n",
      "741 [D loss: 0.636255, acc.: 65.62%] [G loss: 0.824491]\n",
      "742 [D loss: 0.674593, acc.: 62.50%] [G loss: 0.801342]\n",
      "743 [D loss: 0.697357, acc.: 59.38%] [G loss: 0.832422]\n",
      "744 [D loss: 0.702992, acc.: 53.12%] [G loss: 0.825487]\n",
      "745 [D loss: 0.666238, acc.: 56.25%] [G loss: 0.769001]\n",
      "746 [D loss: 0.696788, acc.: 56.25%] [G loss: 0.771619]\n",
      "747 [D loss: 0.679233, acc.: 59.38%] [G loss: 0.731317]\n",
      "748 [D loss: 0.645214, acc.: 67.19%] [G loss: 0.771301]\n",
      "749 [D loss: 0.650142, acc.: 60.94%] [G loss: 0.717371]\n",
      "750 [D loss: 0.690798, acc.: 56.25%] [G loss: 0.686987]\n",
      "751 [D loss: 0.677301, acc.: 53.12%] [G loss: 0.910674]\n",
      "752 [D loss: 0.699061, acc.: 51.56%] [G loss: 0.788301]\n",
      "753 [D loss: 0.672397, acc.: 54.69%] [G loss: 0.937975]\n",
      "754 [D loss: 0.664844, acc.: 57.81%] [G loss: 1.048613]\n",
      "755 [D loss: 0.674206, acc.: 53.12%] [G loss: 0.865184]\n",
      "756 [D loss: 0.652116, acc.: 64.06%] [G loss: 0.994800]\n",
      "757 [D loss: 0.691353, acc.: 56.25%] [G loss: 0.820121]\n",
      "758 [D loss: 0.672559, acc.: 54.69%] [G loss: 0.969831]\n",
      "759 [D loss: 0.736533, acc.: 46.88%] [G loss: 0.851085]\n",
      "760 [D loss: 0.657022, acc.: 59.38%] [G loss: 0.801084]\n",
      "761 [D loss: 0.696054, acc.: 56.25%] [G loss: 0.728511]\n",
      "762 [D loss: 0.675857, acc.: 59.38%] [G loss: 0.732213]\n",
      "763 [D loss: 0.662346, acc.: 56.25%] [G loss: 0.765118]\n",
      "764 [D loss: 0.704992, acc.: 56.25%] [G loss: 0.825112]\n",
      "765 [D loss: 0.662478, acc.: 56.25%] [G loss: 1.031890]\n",
      "766 [D loss: 0.639090, acc.: 62.50%] [G loss: 0.990289]\n",
      "767 [D loss: 0.575905, acc.: 75.00%] [G loss: 0.811322]\n",
      "768 [D loss: 0.622950, acc.: 67.19%] [G loss: 0.921395]\n",
      "769 [D loss: 0.652502, acc.: 60.94%] [G loss: 1.036959]\n",
      "770 [D loss: 0.716581, acc.: 59.38%] [G loss: 0.846450]\n",
      "771 [D loss: 0.675314, acc.: 54.69%] [G loss: 0.832031]\n",
      "772 [D loss: 0.658395, acc.: 62.50%] [G loss: 0.871319]\n",
      "773 [D loss: 0.719302, acc.: 53.12%] [G loss: 0.751859]\n",
      "774 [D loss: 0.603040, acc.: 67.19%] [G loss: 0.847994]\n",
      "775 [D loss: 0.658925, acc.: 54.69%] [G loss: 0.841483]\n",
      "776 [D loss: 0.660294, acc.: 57.81%] [G loss: 1.051979]\n",
      "777 [D loss: 0.702374, acc.: 62.50%] [G loss: 0.807144]\n",
      "778 [D loss: 0.684754, acc.: 64.06%] [G loss: 0.840450]\n",
      "779 [D loss: 0.668864, acc.: 59.38%] [G loss: 0.844166]\n",
      "780 [D loss: 0.656487, acc.: 64.06%] [G loss: 0.847587]\n",
      "781 [D loss: 0.665931, acc.: 60.94%] [G loss: 0.791246]\n",
      "782 [D loss: 0.680978, acc.: 54.69%] [G loss: 0.791072]\n",
      "783 [D loss: 0.682688, acc.: 53.12%] [G loss: 0.709938]\n",
      "784 [D loss: 0.644246, acc.: 64.06%] [G loss: 0.762001]\n",
      "785 [D loss: 0.652495, acc.: 60.94%] [G loss: 0.751033]\n",
      "786 [D loss: 0.683461, acc.: 57.81%] [G loss: 0.798011]\n",
      "787 [D loss: 0.646634, acc.: 56.25%] [G loss: 0.735518]\n",
      "788 [D loss: 0.659622, acc.: 60.94%] [G loss: 0.930391]\n",
      "789 [D loss: 0.654471, acc.: 59.38%] [G loss: 0.864227]\n",
      "790 [D loss: 0.702705, acc.: 50.00%] [G loss: 0.849000]\n",
      "791 [D loss: 0.683174, acc.: 67.19%] [G loss: 1.007722]\n",
      "792 [D loss: 0.616615, acc.: 65.62%] [G loss: 0.841422]\n",
      "793 [D loss: 0.669254, acc.: 57.81%] [G loss: 0.875816]\n",
      "794 [D loss: 0.684219, acc.: 60.94%] [G loss: 0.758390]\n",
      "795 [D loss: 0.711121, acc.: 54.69%] [G loss: 0.833276]\n",
      "796 [D loss: 0.673303, acc.: 56.25%] [G loss: 0.903717]\n",
      "797 [D loss: 0.645126, acc.: 64.06%] [G loss: 0.723321]\n",
      "798 [D loss: 0.667652, acc.: 57.81%] [G loss: 0.718974]\n",
      "799 [D loss: 0.656477, acc.: 62.50%] [G loss: 0.677651]\n",
      "800 [D loss: 0.701786, acc.: 51.56%] [G loss: 0.689693]\n",
      "20 bahru mo sÃ¡ : [0]\n",
      "1 10 10 ÄÆ¡n 000 03 : [1]\n",
      "40 4x nguyá»n cÃ¢u : [2]\n",
      "000 tiá»n 3 ca nÆ°á»c cá»c 151104 báº¡c : [3]\n",
      "summary : [4]\n",
      "tÃªn ca háº¡t : [5]\n",
      "801 [D loss: 0.658822, acc.: 57.81%] [G loss: 0.662633]\n",
      "802 [D loss: 0.700883, acc.: 59.38%] [G loss: 0.796007]\n",
      "803 [D loss: 0.697945, acc.: 57.81%] [G loss: 0.780042]\n",
      "804 [D loss: 0.647099, acc.: 73.44%] [G loss: 0.787400]\n",
      "805 [D loss: 0.665606, acc.: 56.25%] [G loss: 0.668264]\n",
      "806 [D loss: 0.735894, acc.: 46.88%] [G loss: 0.694994]\n",
      "807 [D loss: 0.696742, acc.: 60.94%] [G loss: 0.799532]\n",
      "808 [D loss: 0.637031, acc.: 70.31%] [G loss: 0.738117]\n",
      "809 [D loss: 0.632277, acc.: 62.50%] [G loss: 0.695600]\n",
      "810 [D loss: 0.682473, acc.: 57.81%] [G loss: 0.792312]\n",
      "811 [D loss: 0.658309, acc.: 57.81%] [G loss: 0.845595]\n",
      "812 [D loss: 0.673890, acc.: 57.81%] [G loss: 0.800938]\n",
      "813 [D loss: 0.698430, acc.: 54.69%] [G loss: 0.754456]\n",
      "814 [D loss: 0.703492, acc.: 54.69%] [G loss: 0.765727]\n",
      "815 [D loss: 0.695422, acc.: 51.56%] [G loss: 0.732808]\n",
      "816 [D loss: 0.716962, acc.: 56.25%] [G loss: 0.815054]\n",
      "817 [D loss: 0.669509, acc.: 59.38%] [G loss: 0.717014]\n",
      "818 [D loss: 0.701734, acc.: 56.25%] [G loss: 0.710888]\n",
      "819 [D loss: 0.676056, acc.: 57.81%] [G loss: 0.719119]\n",
      "820 [D loss: 0.718017, acc.: 54.69%] [G loss: 0.757517]\n",
      "821 [D loss: 0.650400, acc.: 60.94%] [G loss: 0.757344]\n",
      "822 [D loss: 0.686912, acc.: 51.56%] [G loss: 0.739187]\n",
      "823 [D loss: 0.677048, acc.: 59.38%] [G loss: 0.807969]\n",
      "824 [D loss: 0.692295, acc.: 70.31%] [G loss: 0.679719]\n",
      "825 [D loss: 0.714637, acc.: 48.44%] [G loss: 0.759821]\n",
      "826 [D loss: 0.676445, acc.: 56.25%] [G loss: 0.735522]\n",
      "827 [D loss: 0.625023, acc.: 73.44%] [G loss: 0.879684]\n",
      "828 [D loss: 0.678552, acc.: 53.12%] [G loss: 0.834598]\n",
      "829 [D loss: 0.627415, acc.: 68.75%] [G loss: 0.727813]\n",
      "830 [D loss: 0.705672, acc.: 56.25%] [G loss: 0.879322]\n",
      "831 [D loss: 0.667697, acc.: 57.81%] [G loss: 0.950366]\n",
      "832 [D loss: 0.670053, acc.: 56.25%] [G loss: 0.699368]\n",
      "833 [D loss: 0.730829, acc.: 45.31%] [G loss: 0.855002]\n",
      "834 [D loss: 0.659951, acc.: 60.94%] [G loss: 0.891287]\n",
      "835 [D loss: 0.686107, acc.: 65.62%] [G loss: 0.900798]\n",
      "836 [D loss: 0.684608, acc.: 62.50%] [G loss: 0.728716]\n",
      "837 [D loss: 0.661996, acc.: 67.19%] [G loss: 0.799553]\n",
      "838 [D loss: 0.641546, acc.: 68.75%] [G loss: 0.737894]\n",
      "839 [D loss: 0.639758, acc.: 60.94%] [G loss: 0.735458]\n",
      "840 [D loss: 0.697111, acc.: 50.00%] [G loss: 0.841751]\n",
      "841 [D loss: 0.663795, acc.: 62.50%] [G loss: 0.838413]\n",
      "842 [D loss: 0.648005, acc.: 65.62%] [G loss: 0.749089]\n",
      "843 [D loss: 0.673282, acc.: 53.12%] [G loss: 0.754836]\n",
      "844 [D loss: 0.699993, acc.: 53.12%] [G loss: 0.833177]\n",
      "845 [D loss: 0.654872, acc.: 59.38%] [G loss: 0.993406]\n",
      "846 [D loss: 0.687332, acc.: 51.56%] [G loss: 1.069033]\n",
      "847 [D loss: 0.671749, acc.: 57.81%] [G loss: 1.000123]\n",
      "848 [D loss: 0.675979, acc.: 62.50%] [G loss: 0.996053]\n",
      "849 [D loss: 0.622925, acc.: 64.06%] [G loss: 0.995857]\n",
      "850 [D loss: 0.640883, acc.: 67.19%] [G loss: 1.072179]\n",
      "851 [D loss: 0.648987, acc.: 64.06%] [G loss: 0.990755]\n",
      "852 [D loss: 0.652258, acc.: 57.81%] [G loss: 0.841257]\n",
      "853 [D loss: 0.645777, acc.: 65.62%] [G loss: 0.733868]\n",
      "854 [D loss: 0.619471, acc.: 71.88%] [G loss: 0.758267]\n",
      "855 [D loss: 0.649262, acc.: 62.50%] [G loss: 0.737852]\n",
      "856 [D loss: 0.632528, acc.: 68.75%] [G loss: 0.825863]\n",
      "857 [D loss: 0.632503, acc.: 67.19%] [G loss: 0.913999]\n",
      "858 [D loss: 0.645664, acc.: 70.31%] [G loss: 1.044951]\n",
      "859 [D loss: 0.684278, acc.: 56.25%] [G loss: 0.856617]\n",
      "860 [D loss: 0.710018, acc.: 59.38%] [G loss: 0.847844]\n",
      "861 [D loss: 0.625033, acc.: 70.31%] [G loss: 0.919440]\n",
      "862 [D loss: 0.658941, acc.: 68.75%] [G loss: 0.928062]\n",
      "863 [D loss: 0.681698, acc.: 53.12%] [G loss: 0.701500]\n",
      "864 [D loss: 0.760359, acc.: 53.12%] [G loss: 0.858061]\n",
      "865 [D loss: 0.673340, acc.: 54.69%] [G loss: 0.739901]\n",
      "866 [D loss: 0.673780, acc.: 51.56%] [G loss: 0.929542]\n",
      "867 [D loss: 0.667097, acc.: 59.38%] [G loss: 0.791834]\n",
      "868 [D loss: 0.683657, acc.: 59.38%] [G loss: 0.976246]\n",
      "869 [D loss: 0.668556, acc.: 64.06%] [G loss: 0.864883]\n",
      "870 [D loss: 0.679825, acc.: 60.94%] [G loss: 0.909211]\n",
      "871 [D loss: 0.690753, acc.: 43.75%] [G loss: 0.856979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872 [D loss: 0.695549, acc.: 59.38%] [G loss: 0.718710]\n",
      "873 [D loss: 0.670418, acc.: 59.38%] [G loss: 0.779187]\n",
      "874 [D loss: 0.707847, acc.: 50.00%] [G loss: 0.805193]\n",
      "875 [D loss: 0.656220, acc.: 60.94%] [G loss: 0.777000]\n",
      "876 [D loss: 0.688275, acc.: 56.25%] [G loss: 0.712685]\n",
      "877 [D loss: 0.659286, acc.: 65.62%] [G loss: 0.713764]\n",
      "878 [D loss: 0.642792, acc.: 62.50%] [G loss: 0.737257]\n",
      "879 [D loss: 0.651159, acc.: 64.06%] [G loss: 0.777423]\n",
      "880 [D loss: 0.646267, acc.: 64.06%] [G loss: 0.713688]\n",
      "881 [D loss: 0.702593, acc.: 46.88%] [G loss: 0.831377]\n",
      "882 [D loss: 0.667757, acc.: 59.38%] [G loss: 0.828430]\n",
      "883 [D loss: 0.671191, acc.: 64.06%] [G loss: 0.772139]\n",
      "884 [D loss: 0.697538, acc.: 54.69%] [G loss: 0.769961]\n",
      "885 [D loss: 0.691578, acc.: 54.69%] [G loss: 0.730572]\n",
      "886 [D loss: 0.696362, acc.: 56.25%] [G loss: 0.850429]\n",
      "887 [D loss: 0.654750, acc.: 56.25%] [G loss: 0.928519]\n",
      "888 [D loss: 0.683120, acc.: 57.81%] [G loss: 0.839177]\n",
      "889 [D loss: 0.673060, acc.: 57.81%] [G loss: 0.985576]\n",
      "890 [D loss: 0.688536, acc.: 53.12%] [G loss: 0.894859]\n",
      "891 [D loss: 0.683764, acc.: 59.38%] [G loss: 0.791153]\n",
      "892 [D loss: 0.694301, acc.: 57.81%] [G loss: 0.740205]\n",
      "893 [D loss: 0.657910, acc.: 64.06%] [G loss: 0.772365]\n",
      "894 [D loss: 0.659701, acc.: 62.50%] [G loss: 0.771224]\n",
      "895 [D loss: 0.652238, acc.: 68.75%] [G loss: 0.740237]\n",
      "896 [D loss: 0.628044, acc.: 67.19%] [G loss: 0.693525]\n",
      "897 [D loss: 0.674679, acc.: 56.25%] [G loss: 0.727643]\n",
      "898 [D loss: 0.756350, acc.: 50.00%] [G loss: 0.792699]\n",
      "899 [D loss: 0.647117, acc.: 56.25%] [G loss: 0.740542]\n",
      "900 [D loss: 0.676410, acc.: 51.56%] [G loss: 0.769410]\n",
      "901 [D loss: 0.714017, acc.: 42.19%] [G loss: 0.820979]\n",
      "902 [D loss: 0.696509, acc.: 51.56%] [G loss: 0.868177]\n",
      "903 [D loss: 0.688029, acc.: 53.12%] [G loss: 0.743819]\n",
      "904 [D loss: 0.652345, acc.: 57.81%] [G loss: 0.770247]\n",
      "905 [D loss: 0.672795, acc.: 59.38%] [G loss: 0.879313]\n",
      "906 [D loss: 0.686533, acc.: 54.69%] [G loss: 0.802216]\n",
      "907 [D loss: 0.697156, acc.: 54.69%] [G loss: 0.798456]\n",
      "908 [D loss: 0.671811, acc.: 56.25%] [G loss: 0.823637]\n",
      "909 [D loss: 0.680996, acc.: 54.69%] [G loss: 0.718948]\n",
      "910 [D loss: 0.676544, acc.: 62.50%] [G loss: 0.843410]\n",
      "911 [D loss: 0.659174, acc.: 60.94%] [G loss: 0.836033]\n",
      "912 [D loss: 0.637590, acc.: 57.81%] [G loss: 0.800059]\n",
      "913 [D loss: 0.674924, acc.: 59.38%] [G loss: 0.682831]\n",
      "914 [D loss: 0.661850, acc.: 60.94%] [G loss: 0.807268]\n",
      "915 [D loss: 0.693678, acc.: 48.44%] [G loss: 0.791458]\n",
      "916 [D loss: 0.646509, acc.: 62.50%] [G loss: 0.807420]\n",
      "917 [D loss: 0.675986, acc.: 59.38%] [G loss: 0.701129]\n",
      "918 [D loss: 0.617854, acc.: 70.31%] [G loss: 0.784961]\n",
      "919 [D loss: 0.653195, acc.: 62.50%] [G loss: 0.824395]\n",
      "920 [D loss: 0.679666, acc.: 53.12%] [G loss: 0.887273]\n",
      "921 [D loss: 0.655561, acc.: 57.81%] [G loss: 0.833652]\n",
      "922 [D loss: 0.682518, acc.: 59.38%] [G loss: 0.761053]\n",
      "923 [D loss: 0.669679, acc.: 53.12%] [G loss: 0.769624]\n",
      "924 [D loss: 0.685832, acc.: 53.12%] [G loss: 0.745919]\n",
      "925 [D loss: 0.669280, acc.: 68.75%] [G loss: 0.713969]\n",
      "926 [D loss: 0.701654, acc.: 48.44%] [G loss: 0.723150]\n",
      "927 [D loss: 0.662183, acc.: 53.12%] [G loss: 0.837718]\n",
      "928 [D loss: 0.661021, acc.: 60.94%] [G loss: 0.799276]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a6a11216b0da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Train the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# Plot the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Traing\n",
    "epochs = 20001\n",
    "batch_size=32\n",
    "sample_interval=200\n",
    "\n",
    "# Load the dataset\n",
    "# Load the dataset\n",
    "# (X_train, y_train), (_, _) = mnist.load_data()\n",
    "X_train = x_train\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Configure input\n",
    "half_vocab = Tokenizer_vocab_size/2\n",
    "X_train = (X_train.astype(np.float32) - half_vocab) / half_vocab\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "# Adversarial ground truths\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "\n",
    "    # Select a random half batch of images\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    imgs, labels = X_train[idx], y_train[idx]\n",
    "\n",
    "    # Sample noise as generator input\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "    # Generate a half batch of new images\n",
    "    gen_imgs = generator.predict([noise, labels])\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss_real = discriminator.train_on_batch([imgs, labels], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    # Condition on labels\n",
    "    sampled_labels = np.random.randint(0, len(y_col), batch_size).reshape(-1, 1)\n",
    "\n",
    "    # Train the generator\n",
    "    g_loss = combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "    # Plot the progress\n",
    "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "    # If at save interval => save generated image samples\n",
    "    if epoch % sample_interval == 0:\n",
    "        sample_images(epoch, generator)\n",
    "        save_model(generator,discriminator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
