{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get receipt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "df_col = [\"sentence\",\"brand_name\",\"info\",\"index\",\"content\",\"total\",\"thank_you\"]\n",
    "y_col = [\"brand_name\",\"info\",\"index\",\"content\",\"total\",\"thank_you\"]\n",
    "train_df = pd.read_csv('../text_classification/31-07-vigroupped.csv',   encoding='utf-8')\n",
    "\n",
    "seed = 120\n",
    "np.random.seed(seed)\n",
    "train_df = shuffle(train_df)\n",
    "train_df.head()\n",
    "\n",
    "X_train = train_df[\"sentence\"].fillna(\"fillna\").values\n",
    "Y_train = train_df[['brand_name', 'info', 'index', 'content', 'total', 'thank_you']].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "texts = X_train\n",
    "\n",
    "tokenizer.fit_on_texts(texts) \n",
    "Tokenizer_vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train_encoded_words = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "maxWordCount= 16\n",
    "maxDictionary_size=Tokenizer_vocab_size\n",
    "X_train_encoded_padded_words = sequence.pad_sequences(X_train_encoded_words, maxlen=maxWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded_padded_words.shape\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['ngá»«']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644\n"
     ]
    }
   ],
   "source": [
    "# Convert Y_train\n",
    "# range(0,len(y_col))[5]\n",
    "y_train = []\n",
    "for row in Y_train:\n",
    "    for index,col in enumerate(range(0,len(y_col))):\n",
    "        if row[col] == 1:\n",
    "            y_train.append(index)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_shape, latent_dim,num_classes):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 1 * 1, activation=\"relu\", input_dim=latent_dim))\n",
    "        model.add(Reshape((1, 1, 128)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(1, kernel_size=3, padding='same'))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(num_classes, 10)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape, num_classes):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(Tokenizer_vocab_size*2, input_dim=np.prod(img_shape)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(Tokenizer_vocab_size*2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(Tokenizer_vocab_size*2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "        label_embedding = Flatten()(Embedding(num_classes, np.prod(img_shape))(label))\n",
    "        flat_img = Flatten()(img)\n",
    "\n",
    "        model_input = multiply([flat_img, label_embedding])\n",
    "\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model([img, label], validity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_80 (Dense)             (None, 2392)              40664     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)   (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 2392)              5724056   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)   (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 2392)              5724056   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)   (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 2393      \n",
      "=================================================================\n",
      "Total params: 11,491,169\n",
      "Trainable params: 11,491,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 128)               1408      \n",
      "_________________________________________________________________\n",
      "reshape_16 (Reshape)         (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_27 (UpSampling (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 2, 2, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 2, 2, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_28 (UpSampling (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 4, 4, 64)          73792     \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 4, 4, 1)           577       \n",
      "_________________________________________________________________\n",
      "activation_41 (Activation)   (None, 4, 4, 1)           0         \n",
      "=================================================================\n",
      "Total params: 224,641\n",
      "Trainable params: 224,001\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "(?, 4, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "# Setting variable\n",
    "\n",
    "img_rows = 4\n",
    "img_cols = 4\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "num_classes = len(y_col)\n",
    "latent_dim = 10\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(img_shape,num_classes)\n",
    "discriminator.compile(loss=['binary_crossentropy'],\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator(img_shape,latent_dim,num_classes)\n",
    "\n",
    "# The generator takes noise and the target label as input\n",
    "# and generates the corresponding digit of that label\n",
    "noise = Input(shape=(latent_dim,))\n",
    "label = Input(shape=(1,))\n",
    "img = generator([noise, label])\n",
    "print(img.shape)\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated image as input and determines validity\n",
    "# and the label of that image\n",
    "valid = discriminator([img, label])\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains generator to fool discriminator\n",
    "combined = Model([noise, label], valid)\n",
    "combined.compile(loss=['binary_crossentropy'],\n",
    "    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model function\n",
    "\n",
    "def save_model(generator,discriminator):\n",
    "    def save(model, model_name):\n",
    "        model_path = \"saved_model/%s.json\" % model_name\n",
    "        weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "        options = {\"file_arch\": model_path,\n",
    "                    \"file_weight\": weights_path}\n",
    "        json_string = model.to_json()\n",
    "        open(options['file_arch'], 'w').write(json_string)\n",
    "        model.save_weights(options['file_weight'])\n",
    "\n",
    "    save(generator, \"generator\")\n",
    "    save(discriminator, \"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 227, 13, 228, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_encoded_words[0])\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_sequence_to_text(int_arr):\n",
    "    \n",
    "    padded_sequence = int_arr.reshape((maxWordCount))\n",
    "    padded_sequence = padded_sequence.tolist()\n",
    "#     print(padded_sequence)\n",
    "    started = False\n",
    "    word_seq = []\n",
    "    for word in padded_sequence:\n",
    "        if started:\n",
    "            word_seq.append(word)\n",
    "        else:\n",
    "            if word != 0:\n",
    "                started = True\n",
    "                word_seq.append(word)\n",
    "    \n",
    "    sentences = list(map(sequence_to_text, [word_seq]))\n",
    "    if len(sentences)>0:\n",
    "        my_texts = []\n",
    "        for word in sentences[0]:\n",
    "            if word:\n",
    "                my_texts.append(word)\n",
    "            \n",
    "        return ' '.join(my_texts)\n",
    "    return None\n",
    "# print(X_train_encoded_padded_words[0])\n",
    "# print(padded_sequence_to_text(X_train_encoded_padded_words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(y):\n",
    "    result = []\n",
    "    for index, col in enumerate(y_col):\n",
    "        if index == y:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "#     print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_\n",
    "\n",
    "def sample_images(epoch, generator):\n",
    "        csvfile = 'cgan2d.csv'\n",
    "        c = len(y_col)\n",
    "        noise = np.random.normal(0, 1, (c, 10))\n",
    "        sampled_labels = np.arange(0, len(y_col)).reshape(-1, 1)\n",
    "\n",
    "        gen_imgs = generator.predict([noise, sampled_labels])\n",
    "        # Rescale images 0 - 1\n",
    "#         print(sampled_labels)\n",
    "        gen_imgs = 1-(0.5 * gen_imgs + 0.5)\n",
    "        gen_imgs = Tokenizer_vocab_size*gen_imgs\n",
    "        \n",
    "        \n",
    "        int_arr = np.array(gen_imgs, dtype='int')\n",
    "#         print(int_arr[0])\n",
    "        \n",
    "        \n",
    "#         print(len(int_arr[0,:,:,0]))\n",
    "#         fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "\n",
    "        for j in range(c):\n",
    "            sentence = padded_sequence_to_text(int_arr[cnt])\n",
    "            result = convert_y(sampled_labels[cnt])\n",
    "            if len(sentence) <= 0:\n",
    "                continue\n",
    "            print(sentence,':',sampled_labels[cnt])\n",
    "            cnt += 1\n",
    "#             df = pd.read_csv(csvfile)# Loading a csv file with headers \n",
    "#             data = {\n",
    "#                 'sentence':sentence,\n",
    "#             }\n",
    "#             for index, col in enumerate(y_col):\n",
    "#                 data[col] = result[index]\n",
    "#             df = df.append(data, ignore_index=True)\n",
    "#             df.to_csv(csvfile, index = False,  encoding='utf-8')\n",
    "#                 axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
    "#                 axs[i,j].axis('off')\n",
    "#                 cnt += 1\n",
    "#         fig.savefig(\"images/%d.png\" % epoch)\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,  13, 228,   1],\n",
       "       [  0,   0,   0, ...,   0, 415, 416],\n",
       "       [  0,   0,   0, ..., 417, 418, 419],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   0, 377,   1],\n",
       "       [  0,   0,   0, ...,   0,   0,  13],\n",
       "       [  0,   0,   0, ...,   1,  19,   1]], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded_padded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "for row in X_train_encoded_padded_words:\n",
    "    aa = np.array(row)\n",
    "    \n",
    "    aa = np.reshape(aa,(4,4))\n",
    "#     print(aa)\n",
    "    x_train.append(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 5, 2)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.685560, acc.: 35.94%] [G loss: 0.683342]\n",
      "muoi Äiá»n gst g8000 uá»³nh not vat trá»©ng hen 55 phe cua 0001 âââ lail dang : [0]\n",
      "á»©c giá» 16 2x ngá»t tÃ¢y 2q ie 25 sÃ³ lÆ°Æ¡ng háº½n 2dbtn 0971 náºµng con : [1]\n",
      "py Äáº±ng dam háº¹n cc book b4n farm tv khÃ¡nh nguyá»m cÃ¢y 1111 ngá»t 0902096 vÃ  : [2]\n",
      "khuyáº¿n 900 ch 020 Ã­ bÃ² menu há» 941 xÃ¨o muá»i haineken tÃªnhÃ ng phs kdc bÃ n : [3]\n",
      "f13q4 ÄongiÃ¡ thank 420 tea boom tÃ¢y 1851 100 12 80 matcha 48 tphcm 121 taxrm : [4]\n",
      "tháº£o gá»ng 093 mr namu náºµng 020 165 cong 125 nÃ´i come daie 540000 sling tax : [5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [D loss: 0.406744, acc.: 50.00%] [G loss: 0.685641]\n",
      "2 [D loss: 0.377031, acc.: 50.00%] [G loss: 0.692050]\n",
      "3 [D loss: 0.375310, acc.: 82.81%] [G loss: 0.700904]\n",
      "4 [D loss: 0.360473, acc.: 100.00%] [G loss: 0.710604]\n",
      "5 [D loss: 0.364887, acc.: 100.00%] [G loss: 0.722484]\n",
      "6 [D loss: 0.355819, acc.: 100.00%] [G loss: 0.738301]\n",
      "7 [D loss: 0.341995, acc.: 100.00%] [G loss: 0.755489]\n",
      "8 [D loss: 0.344479, acc.: 100.00%] [G loss: 0.775899]\n",
      "9 [D loss: 0.327681, acc.: 100.00%] [G loss: 0.800324]\n",
      "10 [D loss: 0.298904, acc.: 100.00%] [G loss: 0.834894]\n",
      "11 [D loss: 0.289212, acc.: 100.00%] [G loss: 0.880019]\n",
      "12 [D loss: 0.271650, acc.: 100.00%] [G loss: 0.940845]\n",
      "13 [D loss: 0.249953, acc.: 100.00%] [G loss: 1.014797]\n",
      "14 [D loss: 0.232133, acc.: 100.00%] [G loss: 1.134345]\n",
      "15 [D loss: 0.210655, acc.: 98.44%] [G loss: 1.281694]\n",
      "16 [D loss: 0.161189, acc.: 100.00%] [G loss: 1.471349]\n",
      "17 [D loss: 0.136825, acc.: 100.00%] [G loss: 1.717657]\n",
      "18 [D loss: 0.116112, acc.: 100.00%] [G loss: 2.000389]\n",
      "19 [D loss: 0.073903, acc.: 100.00%] [G loss: 2.298989]\n",
      "20 [D loss: 0.057366, acc.: 100.00%] [G loss: 2.601518]\n",
      "21 [D loss: 0.041382, acc.: 100.00%] [G loss: 2.896854]\n",
      "22 [D loss: 0.045274, acc.: 98.44%] [G loss: 3.133619]\n",
      "23 [D loss: 0.057094, acc.: 98.44%] [G loss: 3.364498]\n",
      "24 [D loss: 0.021893, acc.: 100.00%] [G loss: 3.474175]\n",
      "25 [D loss: 0.101360, acc.: 96.88%] [G loss: 3.481932]\n",
      "26 [D loss: 0.044005, acc.: 96.88%] [G loss: 3.396035]\n",
      "27 [D loss: 0.021429, acc.: 100.00%] [G loss: 3.623172]\n",
      "28 [D loss: 0.079273, acc.: 98.44%] [G loss: 3.542585]\n",
      "29 [D loss: 0.014823, acc.: 100.00%] [G loss: 3.733260]\n",
      "30 [D loss: 0.076144, acc.: 98.44%] [G loss: 3.724693]\n",
      "31 [D loss: 0.031127, acc.: 100.00%] [G loss: 3.790367]\n",
      "32 [D loss: 0.040461, acc.: 98.44%] [G loss: 3.783837]\n",
      "33 [D loss: 0.037619, acc.: 98.44%] [G loss: 3.837159]\n",
      "34 [D loss: 0.015742, acc.: 100.00%] [G loss: 4.001954]\n",
      "35 [D loss: 0.032275, acc.: 98.44%] [G loss: 4.026330]\n",
      "36 [D loss: 0.009374, acc.: 100.00%] [G loss: 4.181985]\n",
      "37 [D loss: 0.012789, acc.: 100.00%] [G loss: 4.302082]\n",
      "38 [D loss: 0.015617, acc.: 100.00%] [G loss: 4.385558]\n",
      "39 [D loss: 0.007748, acc.: 100.00%] [G loss: 4.379184]\n",
      "40 [D loss: 0.005587, acc.: 100.00%] [G loss: 4.605373]\n",
      "41 [D loss: 0.006851, acc.: 100.00%] [G loss: 4.764122]\n",
      "42 [D loss: 0.003828, acc.: 100.00%] [G loss: 4.895185]\n",
      "43 [D loss: 0.022345, acc.: 98.44%] [G loss: 4.796721]\n",
      "44 [D loss: 0.004274, acc.: 100.00%] [G loss: 4.884098]\n",
      "45 [D loss: 0.005101, acc.: 100.00%] [G loss: 4.952560]\n",
      "46 [D loss: 0.005056, acc.: 100.00%] [G loss: 5.165351]\n",
      "47 [D loss: 0.002953, acc.: 100.00%] [G loss: 5.236367]\n",
      "48 [D loss: 0.086650, acc.: 98.44%] [G loss: 4.859007]\n",
      "49 [D loss: 0.004840, acc.: 100.00%] [G loss: 4.910471]\n",
      "50 [D loss: 0.005381, acc.: 100.00%] [G loss: 4.895334]\n",
      "thanh cuá»n 150 2â¬0 cÃ¡ ngÃ y 000 usn 000 máº·t : [0]\n",
      "4732 Äáº±ng xuÃ¢n mÃ£ cuá»n ngá»« ngá»« tax háº½n hang 1 giÃ¡ 000 : [1]\n",
      "cÃ¡ lanh 150 00 ngá»« station 25 1 tiá»n tiá»n : [2]\n",
      "ngá»« free sá» ngá»« 2 âtá»ng ngá»« hÃ ng : [3]\n",
      "chua 00098 duck tráº§n lÆ°Æ¡ng ngá»« leather ngÃ o qua cÃ´ng giÃ¡ giá» 0 : [4]\n",
      "10 ÄÃ  ngá»« ngá»« 123 ngá»« ngá»« ngá»« ngÃ o sl 000 : [5]\n",
      "51 [D loss: 0.032403, acc.: 98.44%] [G loss: 4.560619]\n",
      "52 [D loss: 0.005349, acc.: 100.00%] [G loss: 4.671157]\n",
      "53 [D loss: 0.004505, acc.: 100.00%] [G loss: 4.773710]\n",
      "54 [D loss: 0.004502, acc.: 100.00%] [G loss: 5.017286]\n",
      "55 [D loss: 0.006418, acc.: 100.00%] [G loss: 5.064427]\n",
      "56 [D loss: 0.004384, acc.: 100.00%] [G loss: 5.255271]\n",
      "57 [D loss: 0.003527, acc.: 100.00%] [G loss: 5.319423]\n",
      "58 [D loss: 0.005952, acc.: 100.00%] [G loss: 5.376033]\n",
      "59 [D loss: 0.002123, acc.: 100.00%] [G loss: 5.496469]\n",
      "60 [D loss: 0.015098, acc.: 98.44%] [G loss: 5.257947]\n",
      "61 [D loss: 0.006149, acc.: 100.00%] [G loss: 5.156533]\n",
      "62 [D loss: 0.002554, acc.: 100.00%] [G loss: 5.404864]\n",
      "63 [D loss: 0.002278, acc.: 100.00%] [G loss: 5.496463]\n",
      "64 [D loss: 0.004633, acc.: 100.00%] [G loss: 5.562683]\n",
      "65 [D loss: 0.001984, acc.: 100.00%] [G loss: 5.507081]\n",
      "66 [D loss: 0.007153, acc.: 100.00%] [G loss: 5.658102]\n",
      "67 [D loss: 0.003125, acc.: 100.00%] [G loss: 5.740380]\n",
      "68 [D loss: 0.003311, acc.: 100.00%] [G loss: 5.838552]\n",
      "69 [D loss: 0.001547, acc.: 100.00%] [G loss: 5.869930]\n",
      "70 [D loss: 0.001463, acc.: 100.00%] [G loss: 5.935545]\n",
      "71 [D loss: 0.006147, acc.: 100.00%] [G loss: 5.924333]\n",
      "72 [D loss: 0.001197, acc.: 100.00%] [G loss: 6.075315]\n",
      "73 [D loss: 0.001420, acc.: 100.00%] [G loss: 5.965797]\n",
      "74 [D loss: 0.001123, acc.: 100.00%] [G loss: 6.063905]\n",
      "75 [D loss: 0.001305, acc.: 100.00%] [G loss: 6.252783]\n",
      "76 [D loss: 0.099016, acc.: 98.44%] [G loss: 5.410898]\n",
      "77 [D loss: 0.002769, acc.: 100.00%] [G loss: 5.312720]\n",
      "78 [D loss: 0.002553, acc.: 100.00%] [G loss: 5.361397]\n",
      "79 [D loss: 0.002947, acc.: 100.00%] [G loss: 5.679021]\n",
      "80 [D loss: 0.001915, acc.: 100.00%] [G loss: 5.834885]\n",
      "81 [D loss: 0.001441, acc.: 100.00%] [G loss: 5.895824]\n",
      "82 [D loss: 0.001309, acc.: 100.00%] [G loss: 5.896132]\n",
      "83 [D loss: 0.002875, acc.: 100.00%] [G loss: 6.024587]\n",
      "84 [D loss: 0.001133, acc.: 100.00%] [G loss: 6.136916]\n",
      "85 [D loss: 0.001562, acc.: 100.00%] [G loss: 6.241306]\n",
      "86 [D loss: 0.004914, acc.: 100.00%] [G loss: 6.191198]\n",
      "87 [D loss: 0.004910, acc.: 100.00%] [G loss: 6.065303]\n",
      "88 [D loss: 0.090528, acc.: 98.44%] [G loss: 5.160379]\n",
      "89 [D loss: 0.004102, acc.: 100.00%] [G loss: 5.120669]\n",
      "90 [D loss: 0.003332, acc.: 100.00%] [G loss: 5.243401]\n",
      "91 [D loss: 0.003484, acc.: 100.00%] [G loss: 5.421243]\n",
      "92 [D loss: 0.004626, acc.: 100.00%] [G loss: 5.580856]\n",
      "93 [D loss: 0.002629, acc.: 100.00%] [G loss: 5.707173]\n",
      "94 [D loss: 0.002114, acc.: 100.00%] [G loss: 6.040265]\n",
      "95 [D loss: 0.001679, acc.: 100.00%] [G loss: 6.006095]\n",
      "96 [D loss: 0.001118, acc.: 100.00%] [G loss: 6.191532]\n",
      "97 [D loss: 0.002254, acc.: 100.00%] [G loss: 6.236418]\n",
      "98 [D loss: 0.000942, acc.: 100.00%] [G loss: 6.339926]\n",
      "99 [D loss: 0.001081, acc.: 100.00%] [G loss: 6.440436]\n",
      "100 [D loss: 0.000964, acc.: 100.00%] [G loss: 6.520534]\n",
      "ngá»« 2 t ngá»« 999 0 ngá»« t : [0]\n",
      "36 00098 12 muoi 1385 11720 cuá»n tá»· 9 1 ngÃ y cá»§ : [1]\n",
      "ngá»« ngá»« 60 0 ngá»« 25 ngá»« ngá»« ngá»« ngá»« ÄÃ  : [2]\n",
      "don ha cÃ¡ ngá»« 80 sl gáº·p : [3]\n",
      "ngá»« ngá»« ngá»« ngá»« ngá»« and nghÄ©a 04 sl hÆ°ng ÄÃ  : [4]\n",
      "tinh ngá»« ngá»« 20 ngá»« ngá»« ngá»« ngá»« 1989 1 2 sl sl : [5]\n",
      "101 [D loss: 0.001064, acc.: 100.00%] [G loss: 6.664643]\n",
      "102 [D loss: 0.000629, acc.: 100.00%] [G loss: 6.615286]\n",
      "103 [D loss: 0.000770, acc.: 100.00%] [G loss: 6.670314]\n",
      "104 [D loss: 0.000705, acc.: 100.00%] [G loss: 6.773901]\n",
      "105 [D loss: 0.000545, acc.: 100.00%] [G loss: 6.799949]\n",
      "106 [D loss: 0.000558, acc.: 100.00%] [G loss: 6.809812]\n",
      "107 [D loss: 0.000452, acc.: 100.00%] [G loss: 6.994448]\n",
      "108 [D loss: 0.001849, acc.: 100.00%] [G loss: 6.887592]\n",
      "109 [D loss: 0.000506, acc.: 100.00%] [G loss: 6.928226]\n",
      "110 [D loss: 0.000424, acc.: 100.00%] [G loss: 6.997239]\n",
      "111 [D loss: 0.100480, acc.: 98.44%] [G loss: 5.776469]\n",
      "112 [D loss: 0.002261, acc.: 100.00%] [G loss: 5.490227]\n",
      "113 [D loss: 0.002687, acc.: 100.00%] [G loss: 5.694907]\n",
      "114 [D loss: 0.002102, acc.: 100.00%] [G loss: 5.860426]\n",
      "115 [D loss: 0.001387, acc.: 100.00%] [G loss: 6.166058]\n",
      "116 [D loss: 0.001275, acc.: 100.00%] [G loss: 6.219024]\n",
      "117 [D loss: 0.001120, acc.: 100.00%] [G loss: 6.210723]\n",
      "118 [D loss: 0.003095, acc.: 100.00%] [G loss: 6.426661]\n",
      "119 [D loss: 0.093987, acc.: 98.44%] [G loss: 5.197337]\n",
      "120 [D loss: 0.003048, acc.: 100.00%] [G loss: 5.160820]\n",
      "121 [D loss: 0.002919, acc.: 100.00%] [G loss: 5.302675]\n",
      "122 [D loss: 0.002133, acc.: 100.00%] [G loss: 5.636817]\n",
      "123 [D loss: 0.010601, acc.: 100.00%] [G loss: 5.434515]\n",
      "124 [D loss: 0.002466, acc.: 100.00%] [G loss: 5.645361]\n",
      "125 [D loss: 0.001887, acc.: 100.00%] [G loss: 5.775743]\n",
      "126 [D loss: 0.086572, acc.: 98.44%] [G loss: 4.663374]\n",
      "127 [D loss: 0.005291, acc.: 100.00%] [G loss: 4.911670]\n",
      "128 [D loss: 0.003780, acc.: 100.00%] [G loss: 5.342956]\n",
      "129 [D loss: 0.002590, acc.: 100.00%] [G loss: 5.659670]\n",
      "130 [D loss: 0.003272, acc.: 100.00%] [G loss: 5.811884]\n",
      "131 [D loss: 0.001411, acc.: 100.00%] [G loss: 5.976154]\n",
      "132 [D loss: 0.076460, acc.: 98.44%] [G loss: 4.787241]\n",
      "133 [D loss: 0.004839, acc.: 100.00%] [G loss: 4.875026]\n",
      "134 [D loss: 0.003178, acc.: 100.00%] [G loss: 5.278836]\n",
      "135 [D loss: 0.002439, acc.: 100.00%] [G loss: 5.530452]\n",
      "136 [D loss: 0.001601, acc.: 100.00%] [G loss: 5.951347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 [D loss: 0.001525, acc.: 100.00%] [G loss: 5.929322]\n",
      "138 [D loss: 0.074191, acc.: 98.44%] [G loss: 4.900976]\n",
      "139 [D loss: 0.005857, acc.: 100.00%] [G loss: 4.989885]\n",
      "140 [D loss: 0.003260, acc.: 100.00%] [G loss: 5.285584]\n",
      "141 [D loss: 0.002328, acc.: 100.00%] [G loss: 5.612805]\n",
      "142 [D loss: 0.001699, acc.: 100.00%] [G loss: 5.882111]\n",
      "143 [D loss: 0.063449, acc.: 98.44%] [G loss: 4.541778]\n",
      "144 [D loss: 0.005914, acc.: 100.00%] [G loss: 4.770958]\n",
      "145 [D loss: 0.004051, acc.: 100.00%] [G loss: 5.293525]\n",
      "146 [D loss: 0.002479, acc.: 100.00%] [G loss: 5.689938]\n",
      "147 [D loss: 0.001744, acc.: 100.00%] [G loss: 5.913934]\n",
      "148 [D loss: 0.001225, acc.: 100.00%] [G loss: 6.131908]\n",
      "149 [D loss: 0.059081, acc.: 98.44%] [G loss: 4.882552]\n",
      "150 [D loss: 0.006647, acc.: 100.00%] [G loss: 5.026805]\n",
      "ngá»« hÃ n sl 000 ngá»« trÃ­ch 4732 10 cÃ¡ cá»c cuá»n 000 : [0]\n",
      "tiá»n invoice price bÄp dao ngá»« ngá»« ÄÃ  ngá»« ngá»« ngá»« cÃ¡ 1 1 ÄÆ¡n : [1]\n",
      "00098 mantis lai khoai ngá»« py ÄÆ¡n cÃ¡ 4l puddlngtiá»§ng ngá»« ngá»« : [2]\n",
      "11 buom quáº£n 17 h nau gst in 124 trá»i cÃ´ng 000 000 á»©c ngá»« : [3]\n",
      "py ngá»« ngá»« ngá»« báº¡ch ngá»« ngá»« hÄ tt bank phiáº¿u shop 60 000 dgia ngá»« : [4]\n",
      "ngá»« 17 báº¡ch Äáº±ng sl : [5]\n",
      "151 [D loss: 0.003730, acc.: 100.00%] [G loss: 5.457951]\n",
      "152 [D loss: 0.002166, acc.: 100.00%] [G loss: 5.951514]\n",
      "153 [D loss: 0.001729, acc.: 100.00%] [G loss: 6.071228]\n",
      "154 [D loss: 0.001218, acc.: 100.00%] [G loss: 6.341546]\n",
      "155 [D loss: 0.000957, acc.: 100.00%] [G loss: 6.507055]\n",
      "156 [D loss: 0.000822, acc.: 100.00%] [G loss: 6.528922]\n",
      "157 [D loss: 0.000806, acc.: 100.00%] [G loss: 6.672589]\n",
      "158 [D loss: 0.000683, acc.: 100.00%] [G loss: 6.737140]\n",
      "159 [D loss: 0.000627, acc.: 100.00%] [G loss: 6.829708]\n",
      "160 [D loss: 0.001691, acc.: 100.00%] [G loss: 6.845500]\n",
      "161 [D loss: 0.000540, acc.: 100.00%] [G loss: 6.959978]\n",
      "162 [D loss: 0.000572, acc.: 100.00%] [G loss: 7.189138]\n",
      "163 [D loss: 0.000561, acc.: 100.00%] [G loss: 7.047697]\n",
      "164 [D loss: 0.000943, acc.: 100.00%] [G loss: 7.108923]\n",
      "165 [D loss: 0.000446, acc.: 100.00%] [G loss: 7.063745]\n",
      "166 [D loss: 0.000459, acc.: 100.00%] [G loss: 7.311879]\n",
      "167 [D loss: 0.000415, acc.: 100.00%] [G loss: 7.441022]\n",
      "168 [D loss: 0.000373, acc.: 100.00%] [G loss: 7.414815]\n",
      "169 [D loss: 0.000347, acc.: 100.00%] [G loss: 7.434059]\n",
      "170 [D loss: 0.000327, acc.: 100.00%] [G loss: 7.441816]\n",
      "171 [D loss: 0.000323, acc.: 100.00%] [G loss: 7.519205]\n",
      "172 [D loss: 0.000305, acc.: 100.00%] [G loss: 7.552865]\n",
      "173 [D loss: 0.000261, acc.: 100.00%] [G loss: 7.595415]\n",
      "174 [D loss: 0.000280, acc.: 100.00%] [G loss: 7.669885]\n",
      "175 [D loss: 0.000756, acc.: 100.00%] [G loss: 7.453270]\n",
      "176 [D loss: 0.069223, acc.: 98.44%] [G loss: 5.646390]\n",
      "177 [D loss: 0.003065, acc.: 100.00%] [G loss: 5.583528]\n",
      "178 [D loss: 0.002259, acc.: 100.00%] [G loss: 5.851475]\n",
      "179 [D loss: 0.001574, acc.: 100.00%] [G loss: 6.111608]\n",
      "180 [D loss: 0.001798, acc.: 100.00%] [G loss: 6.254789]\n",
      "181 [D loss: 0.001056, acc.: 100.00%] [G loss: 6.351752]\n",
      "182 [D loss: 0.000866, acc.: 100.00%] [G loss: 6.711816]\n",
      "183 [D loss: 0.000696, acc.: 100.00%] [G loss: 6.730799]\n",
      "184 [D loss: 0.000633, acc.: 100.00%] [G loss: 6.949375]\n",
      "185 [D loss: 0.000583, acc.: 100.00%] [G loss: 7.089914]\n",
      "186 [D loss: 0.000517, acc.: 100.00%] [G loss: 7.049839]\n",
      "187 [D loss: 0.000430, acc.: 100.00%] [G loss: 7.220725]\n",
      "188 [D loss: 0.003294, acc.: 100.00%] [G loss: 6.980441]\n",
      "189 [D loss: 0.000507, acc.: 100.00%] [G loss: 7.114838]\n",
      "190 [D loss: 0.000497, acc.: 100.00%] [G loss: 7.208208]\n",
      "191 [D loss: 0.000484, acc.: 100.00%] [G loss: 7.336080]\n",
      "192 [D loss: 0.000506, acc.: 100.00%] [G loss: 7.231404]\n",
      "193 [D loss: 0.000434, acc.: 100.00%] [G loss: 7.214440]\n",
      "194 [D loss: 0.000375, acc.: 100.00%] [G loss: 7.385367]\n",
      "195 [D loss: 0.001164, acc.: 100.00%] [G loss: 7.439275]\n",
      "196 [D loss: 0.000414, acc.: 100.00%] [G loss: 7.554653]\n",
      "197 [D loss: 0.000877, acc.: 100.00%] [G loss: 7.631507]\n",
      "198 [D loss: 0.000324, acc.: 100.00%] [G loss: 7.529287]\n",
      "199 [D loss: 0.000348, acc.: 100.00%] [G loss: 7.615225]\n",
      "200 [D loss: 0.000832, acc.: 100.00%] [G loss: 7.571987]\n",
      "âsá» 000 742 1 ran ngá»« hÃ ng 000 : [0]\n",
      "fashion ngá»« cÃ¡ zzuuu biÃ©t ngá»« ngá»« s ngá»« ngá»« : [1]\n",
      "Ãªn mon tiá»n ngá»« 30 trÃ¢u ngá»« ngá»« : [2]\n",
      "mantis book 23ll5Ã­ cuá»n ngá»« cuá»n Æ¡n gáº·p 000 : [3]\n",
      "66 nguyá»n lÆ°Æ¡ng náºµng ngá»« 1989 nÃ´i tiá»n l lÃºc 000 ngá»« : [4]\n",
      "cuá»n ngá»« tá»ng aif ngá»« ngá»« ngá»« ngá»« : [5]\n",
      "201 [D loss: 0.000264, acc.: 100.00%] [G loss: 7.534693]\n",
      "202 [D loss: 0.044734, acc.: 98.44%] [G loss: 5.667842]\n",
      "203 [D loss: 0.002582, acc.: 100.00%] [G loss: 5.474952]\n",
      "204 [D loss: 0.002630, acc.: 100.00%] [G loss: 5.788165]\n",
      "205 [D loss: 0.001608, acc.: 100.00%] [G loss: 5.987775]\n",
      "206 [D loss: 0.001018, acc.: 100.00%] [G loss: 6.413991]\n",
      "207 [D loss: 0.001041, acc.: 100.00%] [G loss: 6.696574]\n",
      "208 [D loss: 0.000649, acc.: 100.00%] [G loss: 6.599119]\n",
      "209 [D loss: 0.000637, acc.: 100.00%] [G loss: 6.846821]\n",
      "210 [D loss: 0.000570, acc.: 100.00%] [G loss: 6.963522]\n",
      "211 [D loss: 0.000508, acc.: 100.00%] [G loss: 7.137192]\n",
      "212 [D loss: 0.000387, acc.: 100.00%] [G loss: 7.030614]\n",
      "213 [D loss: 0.001702, acc.: 100.00%] [G loss: 7.163852]\n",
      "214 [D loss: 0.000367, acc.: 100.00%] [G loss: 7.131899]\n",
      "215 [D loss: 0.000386, acc.: 100.00%] [G loss: 7.145816]\n",
      "216 [D loss: 0.000405, acc.: 100.00%] [G loss: 7.305034]\n",
      "217 [D loss: 0.000359, acc.: 100.00%] [G loss: 7.394727]\n",
      "218 [D loss: 0.000383, acc.: 100.00%] [G loss: 7.406803]\n",
      "219 [D loss: 0.000324, acc.: 100.00%] [G loss: 7.335859]\n",
      "220 [D loss: 0.000924, acc.: 100.00%] [G loss: 7.606457]\n",
      "221 [D loss: 0.000262, acc.: 100.00%] [G loss: 7.426759]\n",
      "222 [D loss: 0.000288, acc.: 100.00%] [G loss: 7.549360]\n",
      "223 [D loss: 0.000253, acc.: 100.00%] [G loss: 7.544118]\n",
      "224 [D loss: 0.075464, acc.: 98.44%] [G loss: 5.121040]\n",
      "225 [D loss: 0.004831, acc.: 100.00%] [G loss: 5.156979]\n",
      "226 [D loss: 0.002492, acc.: 100.00%] [G loss: 5.726955]\n",
      "227 [D loss: 0.001577, acc.: 100.00%] [G loss: 6.257987]\n",
      "228 [D loss: 0.001123, acc.: 100.00%] [G loss: 6.405916]\n",
      "229 [D loss: 0.026296, acc.: 98.44%] [G loss: 4.817096]\n",
      "230 [D loss: 0.004423, acc.: 100.00%] [G loss: 5.174588]\n",
      "231 [D loss: 0.002684, acc.: 100.00%] [G loss: 5.845993]\n",
      "232 [D loss: 0.001276, acc.: 100.00%] [G loss: 6.426328]\n",
      "233 [D loss: 0.000927, acc.: 100.00%] [G loss: 6.638484]\n",
      "234 [D loss: 0.000696, acc.: 100.00%] [G loss: 6.960245]\n",
      "235 [D loss: 0.000569, acc.: 100.00%] [G loss: 7.014494]\n",
      "236 [D loss: 0.000520, acc.: 100.00%] [G loss: 7.150976]\n",
      "237 [D loss: 0.000638, acc.: 100.00%] [G loss: 7.235749]\n",
      "238 [D loss: 0.000408, acc.: 100.00%] [G loss: 7.384049]\n",
      "239 [D loss: 0.000476, acc.: 100.00%] [G loss: 7.365376]\n",
      "240 [D loss: 0.000480, acc.: 100.00%] [G loss: 7.449646]\n",
      "241 [D loss: 0.000565, acc.: 100.00%] [G loss: 7.584048]\n",
      "242 [D loss: 0.000331, acc.: 100.00%] [G loss: 7.621381]\n",
      "243 [D loss: 0.000311, acc.: 100.00%] [G loss: 7.633567]\n",
      "244 [D loss: 0.000285, acc.: 100.00%] [G loss: 7.662262]\n",
      "245 [D loss: 0.000267, acc.: 100.00%] [G loss: 7.799764]\n",
      "246 [D loss: 0.000597, acc.: 100.00%] [G loss: 7.756939]\n",
      "247 [D loss: 0.000223, acc.: 100.00%] [G loss: 7.890853]\n",
      "248 [D loss: 0.000234, acc.: 100.00%] [G loss: 7.772954]\n",
      "249 [D loss: 0.000230, acc.: 100.00%] [G loss: 7.903714]\n",
      "250 [D loss: 0.000197, acc.: 100.00%] [G loss: 7.817697]\n",
      "hÃ ng tiá»n 4 1 ngá»« ngá»« t ngá»« láº¡i : [0]\n",
      "2014 ngá»« ngá»« ngá»« jin ngá»« ngá»« ngá»« sl : [1]\n",
      "ngá»« ngá»« báº¡ch ÄiÃªn Æ¡n cÃ¡ hha : [2]\n",
      "sl 3 cuá»n cÃ¡ ngá»« ngá»« t ngá»« : [3]\n",
      "qua tu 000 bon ngá»« : [4]\n",
      "Äá» cuá»n cuá»n cuá»n ngá»« ngá»« ngá»« ngá»« 3 : [5]\n",
      "251 [D loss: 0.000427, acc.: 100.00%] [G loss: 7.935960]\n",
      "252 [D loss: 0.000215, acc.: 100.00%] [G loss: 8.064560]\n",
      "253 [D loss: 0.000208, acc.: 100.00%] [G loss: 8.026403]\n",
      "254 [D loss: 0.000171, acc.: 100.00%] [G loss: 7.984388]\n",
      "255 [D loss: 0.000178, acc.: 100.00%] [G loss: 8.011439]\n",
      "256 [D loss: 0.000180, acc.: 100.00%] [G loss: 8.020742]\n",
      "257 [D loss: 0.000180, acc.: 100.00%] [G loss: 8.169586]\n",
      "258 [D loss: 0.000193, acc.: 100.00%] [G loss: 8.273167]\n",
      "259 [D loss: 0.000859, acc.: 100.00%] [G loss: 8.023091]\n",
      "260 [D loss: 0.000200, acc.: 100.00%] [G loss: 8.273121]\n",
      "261 [D loss: 0.000162, acc.: 100.00%] [G loss: 8.342124]\n",
      "262 [D loss: 0.000151, acc.: 100.00%] [G loss: 8.121197]\n",
      "263 [D loss: 0.000141, acc.: 100.00%] [G loss: 8.250774]\n",
      "264 [D loss: 0.000136, acc.: 100.00%] [G loss: 8.238100]\n",
      "265 [D loss: 0.000312, acc.: 100.00%] [G loss: 8.262730]\n",
      "266 [D loss: 0.000295, acc.: 100.00%] [G loss: 8.264225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267 [D loss: 0.000151, acc.: 100.00%] [G loss: 8.345077]\n",
      "268 [D loss: 0.000330, acc.: 100.00%] [G loss: 8.475836]\n",
      "269 [D loss: 0.000117, acc.: 100.00%] [G loss: 8.360518]\n",
      "270 [D loss: 0.000132, acc.: 100.00%] [G loss: 8.384274]\n",
      "271 [D loss: 0.000141, acc.: 100.00%] [G loss: 8.622971]\n",
      "272 [D loss: 0.000128, acc.: 100.00%] [G loss: 8.402697]\n",
      "273 [D loss: 0.000100, acc.: 100.00%] [G loss: 8.589273]\n",
      "274 [D loss: 0.000137, acc.: 100.00%] [G loss: 8.396505]\n",
      "275 [D loss: 0.000561, acc.: 100.00%] [G loss: 8.592674]\n",
      "276 [D loss: 0.000112, acc.: 100.00%] [G loss: 8.715261]\n",
      "277 [D loss: 0.000109, acc.: 100.00%] [G loss: 8.624945]\n",
      "278 [D loss: 0.000114, acc.: 100.00%] [G loss: 8.656914]\n",
      "279 [D loss: 0.000298, acc.: 100.00%] [G loss: 8.610714]\n",
      "280 [D loss: 0.000104, acc.: 100.00%] [G loss: 8.647027]\n",
      "281 [D loss: 0.000367, acc.: 100.00%] [G loss: 8.699375]\n",
      "282 [D loss: 0.000102, acc.: 100.00%] [G loss: 8.609598]\n",
      "283 [D loss: 0.000180, acc.: 100.00%] [G loss: 8.641330]\n",
      "284 [D loss: 0.000097, acc.: 100.00%] [G loss: 8.740801]\n",
      "285 [D loss: 0.000135, acc.: 100.00%] [G loss: 8.746761]\n",
      "286 [D loss: 0.000109, acc.: 100.00%] [G loss: 8.676721]\n",
      "287 [D loss: 0.000102, acc.: 100.00%] [G loss: 8.782713]\n",
      "288 [D loss: 0.000260, acc.: 100.00%] [G loss: 8.732908]\n",
      "289 [D loss: 0.000096, acc.: 100.00%] [G loss: 8.793856]\n",
      "290 [D loss: 0.000215, acc.: 100.00%] [G loss: 8.620661]\n",
      "291 [D loss: 0.000098, acc.: 100.00%] [G loss: 8.692164]\n",
      "292 [D loss: 0.000078, acc.: 100.00%] [G loss: 8.728975]\n",
      "293 [D loss: 0.000089, acc.: 100.00%] [G loss: 8.889561]\n",
      "294 [D loss: 0.000323, acc.: 100.00%] [G loss: 8.776404]\n",
      "295 [D loss: 0.000078, acc.: 100.00%] [G loss: 8.719592]\n",
      "296 [D loss: 0.000085, acc.: 100.00%] [G loss: 8.959017]\n",
      "297 [D loss: 0.049328, acc.: 98.44%] [G loss: 5.382131]\n",
      "298 [D loss: 0.008326, acc.: 100.00%] [G loss: 5.824781]\n",
      "299 [D loss: 0.000749, acc.: 100.00%] [G loss: 6.863803]\n",
      "300 [D loss: 0.000561, acc.: 100.00%] [G loss: 7.193928]\n",
      "0 1 ngá»« ngá»« náºµng cÃ¡ ngá»« ngá»« ngá»« : [0]\n",
      "1 mudi qty ngá»« u ngá»« ngá»« : [1]\n",
      "tiá»n 1989 qua 1 and bÃ¨ ngá»« ngá»« : [2]\n",
      "ngá»« dic ngá»« ngá»« prowct 000 : [3]\n",
      "sl 020 tÃ´m cÃ¡ ngá»« ngá»« qty sl : [4]\n",
      "ngá»« sn mm ngá»« ngá»« 0001 cuá»n tiá»n 000 : [5]\n",
      "301 [D loss: 0.000491, acc.: 100.00%] [G loss: 7.434317]\n",
      "302 [D loss: 0.000305, acc.: 100.00%] [G loss: 7.514340]\n",
      "303 [D loss: 0.000529, acc.: 100.00%] [G loss: 7.713035]\n",
      "304 [D loss: 0.000308, acc.: 100.00%] [G loss: 7.792504]\n",
      "305 [D loss: 0.000254, acc.: 100.00%] [G loss: 7.798297]\n",
      "306 [D loss: 0.000257, acc.: 100.00%] [G loss: 7.844289]\n",
      "307 [D loss: 0.000242, acc.: 100.00%] [G loss: 7.846330]\n",
      "308 [D loss: 0.000241, acc.: 100.00%] [G loss: 7.924539]\n",
      "309 [D loss: 0.002690, acc.: 100.00%] [G loss: 7.745745]\n",
      "310 [D loss: 0.000379, acc.: 100.00%] [G loss: 7.660639]\n",
      "311 [D loss: 0.000313, acc.: 100.00%] [G loss: 7.720394]\n",
      "312 [D loss: 0.000280, acc.: 100.00%] [G loss: 7.648753]\n",
      "313 [D loss: 0.000256, acc.: 100.00%] [G loss: 7.860396]\n",
      "314 [D loss: 0.000272, acc.: 100.00%] [G loss: 7.961121]\n",
      "315 [D loss: 0.000244, acc.: 100.00%] [G loss: 8.175854]\n",
      "316 [D loss: 0.000218, acc.: 100.00%] [G loss: 7.965586]\n",
      "317 [D loss: 0.000198, acc.: 100.00%] [G loss: 8.100782]\n",
      "318 [D loss: 0.000201, acc.: 100.00%] [G loss: 8.089262]\n",
      "319 [D loss: 0.000167, acc.: 100.00%] [G loss: 8.169961]\n",
      "320 [D loss: 0.000153, acc.: 100.00%] [G loss: 8.096022]\n",
      "321 [D loss: 0.000148, acc.: 100.00%] [G loss: 8.358878]\n",
      "322 [D loss: 0.000142, acc.: 100.00%] [G loss: 8.204086]\n",
      "323 [D loss: 0.000254, acc.: 100.00%] [G loss: 8.323435]\n",
      "324 [D loss: 0.000565, acc.: 100.00%] [G loss: 8.402903]\n",
      "325 [D loss: 0.000158, acc.: 100.00%] [G loss: 8.456089]\n",
      "326 [D loss: 0.000143, acc.: 100.00%] [G loss: 8.509141]\n",
      "327 [D loss: 0.000130, acc.: 100.00%] [G loss: 8.455073]\n",
      "328 [D loss: 0.000154, acc.: 100.00%] [G loss: 8.397735]\n",
      "329 [D loss: 0.000146, acc.: 100.00%] [G loss: 8.511856]\n",
      "330 [D loss: 0.000259, acc.: 100.00%] [G loss: 8.419867]\n",
      "331 [D loss: 0.000123, acc.: 100.00%] [G loss: 8.703651]\n",
      "332 [D loss: 0.000118, acc.: 100.00%] [G loss: 8.733009]\n",
      "333 [D loss: 0.000108, acc.: 100.00%] [G loss: 8.525381]\n",
      "334 [D loss: 0.000141, acc.: 100.00%] [G loss: 8.540607]\n",
      "335 [D loss: 0.000232, acc.: 100.00%] [G loss: 8.724084]\n",
      "336 [D loss: 0.000116, acc.: 100.00%] [G loss: 8.639284]\n",
      "337 [D loss: 0.000102, acc.: 100.00%] [G loss: 8.708118]\n",
      "338 [D loss: 0.000111, acc.: 100.00%] [G loss: 8.902163]\n",
      "339 [D loss: 0.000097, acc.: 100.00%] [G loss: 8.724872]\n",
      "340 [D loss: 0.000214, acc.: 100.00%] [G loss: 8.848689]\n",
      "341 [D loss: 0.000088, acc.: 100.00%] [G loss: 8.809340]\n",
      "342 [D loss: 0.000135, acc.: 100.00%] [G loss: 8.889336]\n",
      "343 [D loss: 0.001013, acc.: 100.00%] [G loss: 8.696779]\n",
      "344 [D loss: 0.000088, acc.: 100.00%] [G loss: 8.636518]\n",
      "345 [D loss: 0.000082, acc.: 100.00%] [G loss: 8.744677]\n",
      "346 [D loss: 0.000110, acc.: 100.00%] [G loss: 8.626556]\n",
      "347 [D loss: 0.000179, acc.: 100.00%] [G loss: 8.666611]\n",
      "348 [D loss: 0.000088, acc.: 100.00%] [G loss: 8.844059]\n",
      "349 [D loss: 0.000083, acc.: 100.00%] [G loss: 8.793896]\n",
      "350 [D loss: 0.000089, acc.: 100.00%] [G loss: 8.913280]\n",
      "phÃª 1e cuá»n cÃ¡ ngá»« 1e ngá»« rá»«ng ngá»« : [0]\n",
      "sl 999 ngá»« ngá»« 1989 3t 04 : [1]\n",
      "trÃ­ch náºµng ngá»« hÃ nh ngá»« báº¡ch ngá»« ngá»« xuÃ¢n ngá»« ngá»« : [2]\n",
      "náº¥m sso bao dÆ°á»ng ngá»« ngá»« ngá»« ngá»« cÃ  000 ngá»« : [3]\n",
      "vien tháº¡chtrÃ¡icÃ¢y 30 eitnidl ÄÃ  chivas gá»ng ngá»« phuong 1 10 : [4]\n",
      "398 tax xÃ o ngá»« restaurant 2 tiá»n : [5]\n",
      "351 [D loss: 0.000088, acc.: 100.00%] [G loss: 8.794285]\n",
      "352 [D loss: 0.000080, acc.: 100.00%] [G loss: 9.050411]\n",
      "353 [D loss: 0.000075, acc.: 100.00%] [G loss: 9.002874]\n",
      "354 [D loss: 0.000097, acc.: 100.00%] [G loss: 8.927271]\n",
      "355 [D loss: 0.000084, acc.: 100.00%] [G loss: 8.941603]\n",
      "356 [D loss: 0.006369, acc.: 100.00%] [G loss: 7.581664]\n",
      "357 [D loss: 0.000419, acc.: 100.00%] [G loss: 7.364086]\n",
      "358 [D loss: 0.000723, acc.: 100.00%] [G loss: 7.532875]\n",
      "359 [D loss: 0.000439, acc.: 100.00%] [G loss: 7.395209]\n",
      "360 [D loss: 0.000382, acc.: 100.00%] [G loss: 7.788470]\n",
      "361 [D loss: 0.000354, acc.: 100.00%] [G loss: 7.737828]\n",
      "362 [D loss: 0.000308, acc.: 100.00%] [G loss: 7.882107]\n",
      "363 [D loss: 0.000219, acc.: 100.00%] [G loss: 8.070229]\n",
      "364 [D loss: 0.000213, acc.: 100.00%] [G loss: 8.190574]\n",
      "365 [D loss: 0.000220, acc.: 100.00%] [G loss: 8.338871]\n",
      "366 [D loss: 0.000194, acc.: 100.00%] [G loss: 8.316992]\n",
      "367 [D loss: 0.000166, acc.: 100.00%] [G loss: 8.003960]\n",
      "368 [D loss: 0.000172, acc.: 100.00%] [G loss: 8.385838]\n",
      "369 [D loss: 0.000186, acc.: 100.00%] [G loss: 8.679050]\n",
      "370 [D loss: 0.000146, acc.: 100.00%] [G loss: 8.341496]\n",
      "371 [D loss: 0.000121, acc.: 100.00%] [G loss: 8.566367]\n",
      "372 [D loss: 0.000130, acc.: 100.00%] [G loss: 8.553658]\n",
      "373 [D loss: 0.001305, acc.: 100.00%] [G loss: 8.428797]\n",
      "374 [D loss: 0.000930, acc.: 100.00%] [G loss: 8.094841]\n",
      "375 [D loss: 0.000181, acc.: 100.00%] [G loss: 8.310730]\n",
      "376 [D loss: 0.000142, acc.: 100.00%] [G loss: 8.271231]\n",
      "377 [D loss: 0.000161, acc.: 100.00%] [G loss: 8.295719]\n",
      "378 [D loss: 0.000168, acc.: 100.00%] [G loss: 8.381091]\n",
      "379 [D loss: 0.000186, acc.: 100.00%] [G loss: 8.384511]\n",
      "380 [D loss: 0.000866, acc.: 100.00%] [G loss: 8.267157]\n",
      "381 [D loss: 0.000168, acc.: 100.00%] [G loss: 8.346640]\n",
      "382 [D loss: 0.000209, acc.: 100.00%] [G loss: 8.369008]\n",
      "383 [D loss: 0.000200, acc.: 100.00%] [G loss: 8.384155]\n",
      "384 [D loss: 0.000161, acc.: 100.00%] [G loss: 8.387131]\n",
      "385 [D loss: 0.000129, acc.: 100.00%] [G loss: 8.640065]\n",
      "386 [D loss: 0.000138, acc.: 100.00%] [G loss: 8.459387]\n",
      "387 [D loss: 0.000128, acc.: 100.00%] [G loss: 8.816452]\n",
      "388 [D loss: 0.000172, acc.: 100.00%] [G loss: 8.546693]\n",
      "389 [D loss: 0.000100, acc.: 100.00%] [G loss: 8.769221]\n",
      "390 [D loss: 0.000161, acc.: 100.00%] [G loss: 8.688349]\n",
      "391 [D loss: 0.000107, acc.: 100.00%] [G loss: 8.695426]\n",
      "392 [D loss: 0.000321, acc.: 100.00%] [G loss: 8.899561]\n",
      "393 [D loss: 0.000094, acc.: 100.00%] [G loss: 8.916761]\n",
      "394 [D loss: 0.000097, acc.: 100.00%] [G loss: 9.034874]\n",
      "395 [D loss: 0.000087, acc.: 100.00%] [G loss: 9.007263]\n",
      "396 [D loss: 0.000137, acc.: 100.00%] [G loss: 8.787426]\n",
      "397 [D loss: 0.000074, acc.: 100.00%] [G loss: 9.143007]\n",
      "398 [D loss: 0.000082, acc.: 100.00%] [G loss: 9.120537]\n",
      "399 [D loss: 0.000081, acc.: 100.00%] [G loss: 9.037634]\n",
      "400 [D loss: 0.000075, acc.: 100.00%] [G loss: 8.977810]\n",
      "100 1989 hha 71 ngá»« ngá»« quÃ½ ngá»« 003 ÄÃ  : [0]\n",
      "biÃ©t bll ngá»« cÃ¡ cÃ¡ tra ngÃµ : [1]\n",
      "báº£y 123 2dbtn 2dbtn 000 sá»¯a tiá»n t bill 0 ngá»« : [2]\n",
      "ngá»« ngá»« ngá»« ngá»« 000 ngá»« ngá»« 1 tiá»n : [3]\n",
      "95 mÃ³n ngá»« bia ngá»« chá»§amnghÃªu pon : [4]\n",
      "5 hÃ  ngá»« cÃ¡ ngá»« ngá»« 1 sl : [5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 [D loss: 0.000162, acc.: 100.00%] [G loss: 9.107002]\n",
      "402 [D loss: 0.000073, acc.: 100.00%] [G loss: 9.403670]\n",
      "403 [D loss: 0.000112, acc.: 100.00%] [G loss: 9.241503]\n",
      "404 [D loss: 0.000143, acc.: 100.00%] [G loss: 9.231047]\n",
      "405 [D loss: 0.000062, acc.: 100.00%] [G loss: 9.174343]\n",
      "406 [D loss: 0.000056, acc.: 100.00%] [G loss: 9.181536]\n",
      "407 [D loss: 0.000064, acc.: 100.00%] [G loss: 9.098770]\n",
      "408 [D loss: 0.000066, acc.: 100.00%] [G loss: 9.487074]\n",
      "409 [D loss: 0.000058, acc.: 100.00%] [G loss: 9.210809]\n",
      "410 [D loss: 0.000075, acc.: 100.00%] [G loss: 9.343449]\n",
      "411 [D loss: 0.000056, acc.: 100.00%] [G loss: 9.134583]\n",
      "412 [D loss: 0.000065, acc.: 100.00%] [G loss: 9.477962]\n",
      "413 [D loss: 0.000053, acc.: 100.00%] [G loss: 9.531595]\n",
      "414 [D loss: 0.000049, acc.: 100.00%] [G loss: 9.508815]\n",
      "415 [D loss: 0.000053, acc.: 100.00%] [G loss: 9.535431]\n",
      "416 [D loss: 0.000062, acc.: 100.00%] [G loss: 9.285282]\n",
      "417 [D loss: 0.000063, acc.: 100.00%] [G loss: 9.287171]\n",
      "418 [D loss: 0.000050, acc.: 100.00%] [G loss: 9.504247]\n",
      "419 [D loss: 0.000095, acc.: 100.00%] [G loss: 9.562887]\n",
      "420 [D loss: 0.000056, acc.: 100.00%] [G loss: 9.379444]\n",
      "421 [D loss: 0.000086, acc.: 100.00%] [G loss: 9.552191]\n",
      "422 [D loss: 0.000051, acc.: 100.00%] [G loss: 9.495905]\n",
      "423 [D loss: 0.000051, acc.: 100.00%] [G loss: 9.704947]\n",
      "424 [D loss: 0.000051, acc.: 100.00%] [G loss: 9.734186]\n",
      "425 [D loss: 0.000130, acc.: 100.00%] [G loss: 9.557201]\n",
      "426 [D loss: 0.000098, acc.: 100.00%] [G loss: 9.522825]\n",
      "427 [D loss: 0.000042, acc.: 100.00%] [G loss: 9.552626]\n",
      "428 [D loss: 0.000131, acc.: 100.00%] [G loss: 9.649572]\n",
      "429 [D loss: 0.000071, acc.: 100.00%] [G loss: 9.797080]\n",
      "430 [D loss: 0.000051, acc.: 100.00%] [G loss: 9.506524]\n",
      "431 [D loss: 0.000048, acc.: 100.00%] [G loss: 9.585232]\n",
      "432 [D loss: 0.000039, acc.: 100.00%] [G loss: 9.588612]\n",
      "433 [D loss: 0.000198, acc.: 100.00%] [G loss: 9.658067]\n",
      "434 [D loss: 0.000047, acc.: 100.00%] [G loss: 9.752369]\n",
      "435 [D loss: 0.000190, acc.: 100.00%] [G loss: 9.740488]\n",
      "436 [D loss: 0.000043, acc.: 100.00%] [G loss: 9.630711]\n",
      "437 [D loss: 0.000039, acc.: 100.00%] [G loss: 9.663124]\n",
      "438 [D loss: 0.000041, acc.: 100.00%] [G loss: 9.766591]\n",
      "439 [D loss: 0.000038, acc.: 100.00%] [G loss: 9.892250]\n",
      "440 [D loss: 0.000041, acc.: 100.00%] [G loss: 9.661332]\n",
      "441 [D loss: 0.000043, acc.: 100.00%] [G loss: 9.745479]\n",
      "442 [D loss: 0.000034, acc.: 100.00%] [G loss: 9.774544]\n",
      "443 [D loss: 0.000035, acc.: 100.00%] [G loss: 10.048861]\n",
      "444 [D loss: 0.000039, acc.: 100.00%] [G loss: 9.813373]\n",
      "445 [D loss: 0.000094, acc.: 100.00%] [G loss: 9.866795]\n",
      "446 [D loss: 0.000069, acc.: 100.00%] [G loss: 9.814699]\n",
      "447 [D loss: 0.000044, acc.: 100.00%] [G loss: 9.971894]\n",
      "448 [D loss: 0.000031, acc.: 100.00%] [G loss: 9.898077]\n",
      "449 [D loss: 0.000087, acc.: 100.00%] [G loss: 9.899856]\n",
      "450 [D loss: 0.000031, acc.: 100.00%] [G loss: 9.762804]\n",
      "table 604 604 lá» lÆ°Æ¡ng cÃ¡ ngá»« ngá»« cÃ¡ 17 888 cuá»n náºµng 0 : [0]\n",
      "1 Äáº§u ngá»« 0902018996 ngá»« 2711 ngá»« 500 000 1 : [1]\n",
      "cuá»n náºµng 1989 604 ngá»« ngá»« cuá»n : [2]\n",
      "Ã¢mount cuá»n hÃ nh nguyá»n Äáº±ng 8cm u hÃ n ngá»« 000 ngá»« : [3]\n",
      "cÃ¡ ngá»« rok sá» ngá»« 4 báº¡ch phe 10 sl : [4]\n",
      "again Äáº±ng remebrulae vy nÆ°á»ng cuá»n chin 1 000 : [5]\n",
      "451 [D loss: 0.000198, acc.: 100.00%] [G loss: 9.574824]\n",
      "452 [D loss: 0.000045, acc.: 100.00%] [G loss: 9.693562]\n",
      "453 [D loss: 0.000037, acc.: 100.00%] [G loss: 9.613495]\n",
      "454 [D loss: 0.000039, acc.: 100.00%] [G loss: 9.911813]\n",
      "455 [D loss: 0.000036, acc.: 100.00%] [G loss: 9.881815]\n",
      "456 [D loss: 0.000034, acc.: 100.00%] [G loss: 9.743422]\n",
      "457 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.104938]\n",
      "458 [D loss: 0.000030, acc.: 100.00%] [G loss: 10.056009]\n",
      "459 [D loss: 0.000033, acc.: 100.00%] [G loss: 9.874792]\n",
      "460 [D loss: 0.000032, acc.: 100.00%] [G loss: 9.794353]\n",
      "461 [D loss: 0.000141, acc.: 100.00%] [G loss: 10.014957]\n",
      "462 [D loss: 0.000041, acc.: 100.00%] [G loss: 9.882523]\n",
      "463 [D loss: 0.000039, acc.: 100.00%] [G loss: 9.904346]\n",
      "464 [D loss: 0.000034, acc.: 100.00%] [G loss: 9.898081]\n",
      "465 [D loss: 0.000029, acc.: 100.00%] [G loss: 10.011929]\n",
      "466 [D loss: 0.001317, acc.: 100.00%] [G loss: 9.458439]\n",
      "467 [D loss: 0.000067, acc.: 100.00%] [G loss: 9.484652]\n",
      "468 [D loss: 0.000058, acc.: 100.00%] [G loss: 9.592001]\n",
      "469 [D loss: 0.000043, acc.: 100.00%] [G loss: 9.576945]\n",
      "470 [D loss: 0.000064, acc.: 100.00%] [G loss: 9.728578]\n",
      "471 [D loss: 0.000062, acc.: 100.00%] [G loss: 9.659278]\n",
      "472 [D loss: 0.000043, acc.: 100.00%] [G loss: 9.571306]\n",
      "473 [D loss: 0.000049, acc.: 100.00%] [G loss: 9.580633]\n",
      "474 [D loss: 0.000056, acc.: 100.00%] [G loss: 9.737810]\n",
      "475 [D loss: 0.000054, acc.: 100.00%] [G loss: 9.815659]\n",
      "476 [D loss: 0.000079, acc.: 100.00%] [G loss: 9.535694]\n",
      "477 [D loss: 0.000043, acc.: 100.00%] [G loss: 9.345881]\n",
      "478 [D loss: 0.000063, acc.: 100.00%] [G loss: 9.646689]\n",
      "479 [D loss: 0.000043, acc.: 100.00%] [G loss: 9.850825]\n",
      "480 [D loss: 0.000507, acc.: 100.00%] [G loss: 9.529451]\n",
      "481 [D loss: 0.000079, acc.: 100.00%] [G loss: 9.384600]\n",
      "482 [D loss: 0.000047, acc.: 100.00%] [G loss: 9.548981]\n",
      "483 [D loss: 0.000068, acc.: 100.00%] [G loss: 9.709198]\n",
      "484 [D loss: 0.000042, acc.: 100.00%] [G loss: 9.466192]\n",
      "485 [D loss: 0.000040, acc.: 100.00%] [G loss: 9.687868]\n",
      "486 [D loss: 0.000068, acc.: 100.00%] [G loss: 9.660574]\n",
      "487 [D loss: 0.000037, acc.: 100.00%] [G loss: 9.754807]\n",
      "488 [D loss: 0.000041, acc.: 100.00%] [G loss: 9.802597]\n",
      "489 [D loss: 0.000031, acc.: 100.00%] [G loss: 9.808118]\n",
      "490 [D loss: 0.000037, acc.: 100.00%] [G loss: 9.583260]\n",
      "491 [D loss: 0.000072, acc.: 100.00%] [G loss: 9.601120]\n",
      "492 [D loss: 0.000041, acc.: 100.00%] [G loss: 9.904645]\n",
      "493 [D loss: 0.000048, acc.: 100.00%] [G loss: 9.845117]\n",
      "494 [D loss: 0.000028, acc.: 100.00%] [G loss: 9.832838]\n",
      "495 [D loss: 0.000059, acc.: 100.00%] [G loss: 9.928411]\n",
      "496 [D loss: 0.000037, acc.: 100.00%] [G loss: 9.884596]\n",
      "497 [D loss: 0.000032, acc.: 100.00%] [G loss: 10.048079]\n",
      "498 [D loss: 0.000037, acc.: 100.00%] [G loss: 9.970480]\n",
      "499 [D loss: 0.000033, acc.: 100.00%] [G loss: 10.065727]\n",
      "500 [D loss: 0.000070, acc.: 100.00%] [G loss: 9.806743]\n",
      "20 ngá»« náºµng ngá»« ngá»« ngá»« 03 tráº§n ngá»« ngá»« ngá»« : [0]\n",
      "ngá»« ngá»« ngá»« cuá»n ngá»« 1 ngá»« 0971 ngá»« 000 : [1]\n",
      "000 80000 cÃ¡ ttiÃ©n ngá»« change ngá»« : [2]\n",
      "5 cÃ¡ cÃ¡ prowct naan a1 ngá»« : [3]\n",
      "ngá»« 3005 pov ngá»« ngá»« con ngá»« 000 ngá»« : [4]\n",
      "ngá»« cÃ¡ ngá»« ngá»« ngá»« hÃ ng : [5]\n",
      "501 [D loss: 0.000037, acc.: 100.00%] [G loss: 9.821016]\n",
      "502 [D loss: 0.000096, acc.: 100.00%] [G loss: 9.958795]\n",
      "503 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.029978]\n",
      "504 [D loss: 0.000036, acc.: 100.00%] [G loss: 9.965656]\n",
      "505 [D loss: 0.000035, acc.: 100.00%] [G loss: 9.906458]\n",
      "506 [D loss: 0.000266, acc.: 100.00%] [G loss: 9.902521]\n",
      "507 [D loss: 0.000048, acc.: 100.00%] [G loss: 9.863623]\n",
      "508 [D loss: 0.000039, acc.: 100.00%] [G loss: 9.852864]\n",
      "509 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.055677]\n",
      "510 [D loss: 0.000051, acc.: 100.00%] [G loss: 10.306494]\n",
      "511 [D loss: 0.000029, acc.: 100.00%] [G loss: 9.764559]\n",
      "512 [D loss: 0.000025, acc.: 100.00%] [G loss: 10.142476]\n",
      "513 [D loss: 0.000027, acc.: 100.00%] [G loss: 9.911400]\n",
      "514 [D loss: 0.000055, acc.: 100.00%] [G loss: 9.989622]\n",
      "515 [D loss: 0.000033, acc.: 100.00%] [G loss: 10.006744]\n",
      "516 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.043571]\n",
      "517 [D loss: 0.000028, acc.: 100.00%] [G loss: 10.156391]\n",
      "518 [D loss: 0.000072, acc.: 100.00%] [G loss: 10.095323]\n",
      "519 [D loss: 0.000029, acc.: 100.00%] [G loss: 10.227658]\n",
      "520 [D loss: 0.000029, acc.: 100.00%] [G loss: 10.227401]\n",
      "521 [D loss: 0.000029, acc.: 100.00%] [G loss: 10.210584]\n",
      "522 [D loss: 0.000028, acc.: 100.00%] [G loss: 10.161879]\n",
      "523 [D loss: 0.000027, acc.: 100.00%] [G loss: 10.229345]\n",
      "524 [D loss: 0.000029, acc.: 100.00%] [G loss: 10.171726]\n",
      "525 [D loss: 0.000024, acc.: 100.00%] [G loss: 10.172132]\n",
      "526 [D loss: 0.000024, acc.: 100.00%] [G loss: 10.232430]\n",
      "527 [D loss: 0.000026, acc.: 100.00%] [G loss: 9.993176]\n",
      "528 [D loss: 0.000043, acc.: 100.00%] [G loss: 9.980492]\n",
      "529 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.145414]\n",
      "530 [D loss: 0.000078, acc.: 100.00%] [G loss: 10.213165]\n",
      "531 [D loss: 0.000022, acc.: 100.00%] [G loss: 10.272660]\n",
      "532 [D loss: 0.000024, acc.: 100.00%] [G loss: 10.162495]\n",
      "533 [D loss: 0.000025, acc.: 100.00%] [G loss: 10.165108]\n",
      "534 [D loss: 0.000027, acc.: 100.00%] [G loss: 10.220233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "535 [D loss: 0.000022, acc.: 100.00%] [G loss: 10.213668]\n",
      "536 [D loss: 0.000054, acc.: 100.00%] [G loss: 10.406489]\n",
      "537 [D loss: 0.000021, acc.: 100.00%] [G loss: 10.134104]\n",
      "538 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.394485]\n",
      "539 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.193195]\n",
      "540 [D loss: 0.000020, acc.: 100.00%] [G loss: 10.266439]\n",
      "541 [D loss: 0.000021, acc.: 100.00%] [G loss: 10.506254]\n",
      "542 [D loss: 0.000076, acc.: 100.00%] [G loss: 10.387527]\n",
      "543 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.422640]\n",
      "544 [D loss: 0.000024, acc.: 100.00%] [G loss: 10.280684]\n",
      "545 [D loss: 0.000024, acc.: 100.00%] [G loss: 10.444990]\n",
      "546 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.373248]\n",
      "547 [D loss: 0.000044, acc.: 100.00%] [G loss: 10.375472]\n",
      "548 [D loss: 0.000038, acc.: 100.00%] [G loss: 10.379084]\n",
      "549 [D loss: 0.000020, acc.: 100.00%] [G loss: 10.418591]\n",
      "550 [D loss: 0.000021, acc.: 100.00%] [G loss: 10.325291]\n",
      "23 ngá»« ngá»« cuá»n cuá»n ngá»« 000 1 biÃ©t dÆ°Æ¡ng ngá»« : [0]\n",
      "2 ngá»« khoa Äáº±ng ngá»« 000 ngá»« ngá»« : [1]\n",
      "dáº» ngá»« sl sl lim ngá»« 1 ngá»« : [2]\n",
      "70 15 ÄÆ¡n dao ngÃ y : [3]\n",
      "that ngá»« ngá»« sso : [4]\n",
      "ngá»« ngá»« giá» tá»ng ngá»« Äá» 4 : [5]\n",
      "551 [D loss: 0.000021, acc.: 100.00%] [G loss: 10.368052]\n",
      "552 [D loss: 0.000020, acc.: 100.00%] [G loss: 10.638927]\n",
      "553 [D loss: 0.000020, acc.: 100.00%] [G loss: 10.446888]\n",
      "554 [D loss: 0.000018, acc.: 100.00%] [G loss: 10.606495]\n",
      "555 [D loss: 0.000022, acc.: 100.00%] [G loss: 10.546719]\n",
      "556 [D loss: 0.000022, acc.: 100.00%] [G loss: 10.497928]\n",
      "557 [D loss: 0.000053, acc.: 100.00%] [G loss: 10.432272]\n",
      "558 [D loss: 0.000021, acc.: 100.00%] [G loss: 10.490987]\n",
      "559 [D loss: 0.000021, acc.: 100.00%] [G loss: 10.360772]\n",
      "560 [D loss: 0.000017, acc.: 100.00%] [G loss: 10.451501]\n",
      "561 [D loss: 0.000036, acc.: 100.00%] [G loss: 10.649058]\n",
      "562 [D loss: 0.000018, acc.: 100.00%] [G loss: 10.450315]\n",
      "563 [D loss: 0.000042, acc.: 100.00%] [G loss: 10.518612]\n",
      "564 [D loss: 0.000045, acc.: 100.00%] [G loss: 10.506575]\n",
      "565 [D loss: 0.000016, acc.: 100.00%] [G loss: 10.524887]\n",
      "566 [D loss: 0.000109, acc.: 100.00%] [G loss: 10.645468]\n",
      "567 [D loss: 0.000018, acc.: 100.00%] [G loss: 10.474211]\n",
      "568 [D loss: 0.000063, acc.: 100.00%] [G loss: 10.688797]\n",
      "569 [D loss: 0.000019, acc.: 100.00%] [G loss: 10.476114]\n",
      "570 [D loss: 0.000015, acc.: 100.00%] [G loss: 10.423970]\n",
      "571 [D loss: 0.000016, acc.: 100.00%] [G loss: 10.615909]\n",
      "572 [D loss: 0.000016, acc.: 100.00%] [G loss: 10.596180]\n",
      "573 [D loss: 0.000023, acc.: 100.00%] [G loss: 10.681503]\n",
      "574 [D loss: 0.000017, acc.: 100.00%] [G loss: 10.867269]\n",
      "575 [D loss: 0.001296, acc.: 100.00%] [G loss: 10.092509]\n",
      "576 [D loss: 0.000030, acc.: 100.00%] [G loss: 9.868153]\n",
      "577 [D loss: 0.000045, acc.: 100.00%] [G loss: 9.973940]\n",
      "578 [D loss: 0.000036, acc.: 100.00%] [G loss: 9.807863]\n",
      "579 [D loss: 0.000056, acc.: 100.00%] [G loss: 9.841821]\n",
      "580 [D loss: 0.000037, acc.: 100.00%] [G loss: 9.900657]\n",
      "581 [D loss: 0.000054, acc.: 100.00%] [G loss: 9.960232]\n",
      "582 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.053229]\n",
      "583 [D loss: 0.000039, acc.: 100.00%] [G loss: 9.993125]\n",
      "584 [D loss: 0.000030, acc.: 100.00%] [G loss: 10.233595]\n",
      "585 [D loss: 0.000032, acc.: 100.00%] [G loss: 9.945375]\n",
      "586 [D loss: 0.000044, acc.: 100.00%] [G loss: 9.993242]\n",
      "587 [D loss: 0.000436, acc.: 100.00%] [G loss: 10.016617]\n",
      "588 [D loss: 0.000064, acc.: 100.00%] [G loss: 9.881668]\n",
      "589 [D loss: 0.000040, acc.: 100.00%] [G loss: 10.039482]\n",
      "590 [D loss: 0.000038, acc.: 100.00%] [G loss: 10.024486]\n",
      "591 [D loss: 0.000038, acc.: 100.00%] [G loss: 10.097208]\n",
      "592 [D loss: 0.000030, acc.: 100.00%] [G loss: 9.732569]\n",
      "593 [D loss: 0.000031, acc.: 100.00%] [G loss: 10.041512]\n",
      "594 [D loss: 0.000033, acc.: 100.00%] [G loss: 10.245058]\n",
      "595 [D loss: 0.000050, acc.: 100.00%] [G loss: 9.878590]\n",
      "596 [D loss: 0.000079, acc.: 100.00%] [G loss: 9.981112]\n",
      "597 [D loss: 0.000542, acc.: 100.00%] [G loss: 9.878948]\n",
      "598 [D loss: 0.000040, acc.: 100.00%] [G loss: 9.624050]\n",
      "599 [D loss: 0.000039, acc.: 100.00%] [G loss: 9.621449]\n",
      "600 [D loss: 0.000036, acc.: 100.00%] [G loss: 10.000599]\n",
      "ngá»« cÃ¡ ngá»« ngá»« ngá»« ma lanh ngá»« allowed ngá»« 5 : [0]\n",
      "ngá»« ngá»« ngá»« cuá»n ngá»« ngá»« frequilasunce ngá»« : [1]\n",
      "88603 ngá»« gá»mi 30 ngá»« : [2]\n",
      "tiá»n 1 administrator ngá»« ngá»« 5 sl : [3]\n",
      "604 ngá»« ngá»« nguyá»m 000 tiá»n ngá»« : [4]\n",
      "ngá»« ngá»« nÃ¡o rats ngá»« 5cl 1 dt 30 : [5]\n"
     ]
    }
   ],
   "source": [
    "# Traing\n",
    "epochs = 601\n",
    "batch_size=32\n",
    "sample_interval=50\n",
    "\n",
    "# Load the dataset\n",
    "# Load the dataset\n",
    "# (X_train, y_train), (_, _) = mnist.load_data()\n",
    "X_train = x_train\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Configure input\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "# Adversarial ground truths\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "\n",
    "    # Select a random half batch of images\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    imgs, labels = X_train[idx], y_train[idx]\n",
    "\n",
    "    # Sample noise as generator input\n",
    "    noise = np.random.normal(0, 1, (batch_size, 10))\n",
    "\n",
    "    # Generate a half batch of new images\n",
    "    gen_imgs = generator.predict([noise, labels])\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss_real = discriminator.train_on_batch([imgs, labels], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    # Condition on labels\n",
    "    sampled_labels = np.random.randint(0, len(y_col), batch_size).reshape(-1, 1)\n",
    "\n",
    "    # Train the generator\n",
    "    g_loss = combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "    # Plot the progress\n",
    "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "    # If at save interval => save generated image samples\n",
    "    if epoch % sample_interval == 0:\n",
    "        sample_images(epoch, generator)\n",
    "        save_model(generator,discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
