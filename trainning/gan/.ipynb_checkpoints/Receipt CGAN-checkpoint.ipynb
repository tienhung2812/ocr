{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get receipt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "df_col = [\"sentence\",\"brand_name\",\"info\",\"index\",\"content\",\"total\",\"thank_you\"]\n",
    "y_col = [\"brand_name\",\"info\",\"index\",\"content\",\"total\",\"thank_you\"]\n",
    "train_df = pd.read_csv('../text_classification/31-07-vigroupped.csv',   encoding='utf-8')\n",
    "\n",
    "seed = 120\n",
    "np.random.seed(seed)\n",
    "train_df = shuffle(train_df)\n",
    "train_df.head()\n",
    "\n",
    "X_train = train_df[\"sentence\"].fillna(\"fillna\").values\n",
    "Y_train = train_df[['brand_name', 'info', 'index', 'content', 'total', 'thank_you']].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "texts = X_train\n",
    "\n",
    "tokenizer.fit_on_texts(texts) \n",
    "Tokenizer_vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train_encoded_words = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "maxWordCount= 10\n",
    "maxDictionary_size=Tokenizer_vocab_size\n",
    "X_train_encoded_padded_words = sequence.pad_sequences(X_train_encoded_words, maxlen=maxWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded_padded_words.shape\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['ngá»«']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644\n"
     ]
    }
   ],
   "source": [
    "# Convert Y_train\n",
    "# range(0,len(y_col))[5]\n",
    "y_train = []\n",
    "for row in Y_train:\n",
    "    for index,col in enumerate(range(0,len(y_col))):\n",
    "        if row[col] == 1:\n",
    "            y_train.append(index)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(img_shape, latent_dim,num_classes):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(Tokenizer_vocab_size, input_dim=latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(Tokenizer_vocab_size*2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(Tokenizer_vocab_size*4))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "        model.add(Reshape(img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(num_classes, latent_dim)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(img_shape, num_classes):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(Tokenizer_vocab_size*2, input_dim=np.prod(img_shape)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(Tokenizer_vocab_size*2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(Tokenizer_vocab_size*2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "        label_embedding = Flatten()(Embedding(num_classes, np.prod(img_shape))(label))\n",
    "        flat_img = Flatten()(img)\n",
    "\n",
    "        model_input = multiply([flat_img, label_embedding])\n",
    "\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model([img, label], validity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0811 10:38:29.666753 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0811 10:38:29.668124 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0811 10:38:29.670672 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0811 10:38:29.697594 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0811 10:38:29.703322 4367386048 deprecation.py:506] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0811 10:38:29.807307 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0811 10:38:29.812649 4367386048 deprecation_wrapper.py:119] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0811 10:38:29.817989 4367386048 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 2392)              26312     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2392)              5724056   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2392)              5724056   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 2393      \n",
      "=================================================================\n",
      "Total params: 11,476,817\n",
      "Trainable params: 11,476,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 1196)              120796    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1196)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 1196)              4784      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2392)              2863224   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 2392)              9568      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4784)              11448112  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 4784)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4784)              19136     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                47850     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 1, 10, 1)          0         \n",
      "=================================================================\n",
      "Total params: 14,513,470\n",
      "Trainable params: 14,496,726\n",
      "Non-trainable params: 16,744\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Setting variable\n",
    "\n",
    "img_rows = 1\n",
    "img_cols = 10\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "num_classes = len(y_col)\n",
    "latent_dim = 100\n",
    "\n",
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(img_shape,num_classes)\n",
    "discriminator.compile(loss=['binary_crossentropy'],\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator(img_shape,latent_dim,num_classes)\n",
    "\n",
    "# The generator takes noise and the target label as input\n",
    "# and generates the corresponding digit of that label\n",
    "noise = Input(shape=(latent_dim,))\n",
    "label = Input(shape=(1,))\n",
    "img = generator([noise, label])\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated image as input and determines validity\n",
    "# and the label of that image\n",
    "valid = discriminator([img, label])\n",
    "\n",
    "# The combined model  (stacked generator and discriminator)\n",
    "# Trains generator to fool discriminator\n",
    "combined = Model([noise, label], valid)\n",
    "combined.compile(loss=['binary_crossentropy'],\n",
    "    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model function\n",
    "\n",
    "def save_model(generator,discriminator):\n",
    "    def save(model, model_name):\n",
    "        model_path = \"saved_model/%s.json\" % model_name\n",
    "        weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "        options = {\"file_arch\": model_path,\n",
    "                    \"file_weight\": weights_path}\n",
    "        json_string = model.to_json()\n",
    "        open(options['file_arch'], 'w').write(json_string)\n",
    "        model.save_weights(options['file_weight'])\n",
    "\n",
    "    save(generator, \"generator\")\n",
    "    save(discriminator, \"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 227, 13, 228, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_encoded_words[0])\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_sequence_to_text(int_arr):\n",
    "    \n",
    "    padded_sequence = int_arr.reshape((maxWordCount))\n",
    "    padded_sequence = padded_sequence.tolist()\n",
    "#     print(padded_sequence)\n",
    "    started = False\n",
    "    word_seq = []\n",
    "    for word in padded_sequence:\n",
    "        if started:\n",
    "            word_seq.append(word)\n",
    "        else:\n",
    "            if word != 0:\n",
    "                started = True\n",
    "                word_seq.append(word)\n",
    "    \n",
    "    sentences = list(map(sequence_to_text, [word_seq]))\n",
    "    if len(sentences)>0:\n",
    "        my_texts = []\n",
    "        for word in sentences[0]:\n",
    "            if word:\n",
    "                my_texts.append(word)\n",
    "            \n",
    "        return ' '.join(my_texts)\n",
    "    return None\n",
    "# print(X_train_encoded_padded_words[0])\n",
    "# print(padded_sequence_to_text(X_train_encoded_padded_words[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_y(y):\n",
    "    result = []\n",
    "    for index, col in enumerate(y_col):\n",
    "        if index == y:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "#     print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_\n",
    "\n",
    "def sample_images(epoch, generator):\n",
    "        csvfile = 'cgan2d.csv'\n",
    "        c = len(y_col)\n",
    "        noise = np.random.normal(0, 1, (c, 100))\n",
    "        sampled_labels = np.arange(0, len(y_col)).reshape(-1, 1)\n",
    "\n",
    "        gen_imgs = generator.predict([noise, sampled_labels])\n",
    "        # Rescale images 0 - 1\n",
    "#         print(sampled_labels)\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        gen_imgs = Tokenizer_vocab_size*gen_imgs\n",
    "        \n",
    "        \n",
    "        int_arr = np.array(gen_imgs, dtype='int')\n",
    "#         print(int_arr[0])\n",
    "        \n",
    "        \n",
    "#         print(len(int_arr[0,:,:,0]))\n",
    "#         fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "\n",
    "        for j in range(c):\n",
    "            sentence = padded_sequence_to_text(int_arr[cnt])\n",
    "            result = convert_y(sampled_labels[cnt])\n",
    "            if len(sentence) <= 0:\n",
    "                continue\n",
    "            print(sentence,':',sampled_labels[cnt])\n",
    "            cnt += 1\n",
    "            df = pd.read_csv(csvfile)# Loading a csv file with headers \n",
    "            data = {\n",
    "                'sentence':sentence,\n",
    "            }\n",
    "            for index, col in enumerate(y_col):\n",
    "                data[col] = result[index]\n",
    "            df = df.append(data, ignore_index=True)\n",
    "            df.to_csv(csvfile, index = False,  encoding='utf-8')\n",
    "#                 axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
    "#                 axs[i,j].axis('off')\n",
    "#                 cnt += 1\n",
    "#         fig.savefig(\"images/%d.png\" % epoch)\n",
    "#         plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   13,  228,    1],\n",
       "       [   0,    0,    0, ...,    0,  415,  416],\n",
       "       [   0,    0,    0, ...,  417,  418,  419],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,    0,  377,    1],\n",
       "       [   0,    0,    0, ...,    0,    0,   13],\n",
       "       [ 212, 1193, 1194, ...,    1,   19,    1]], dtype=int32)"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded_padded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "for row in X_train_encoded_padded_words:\n",
    "    aa = np.array(row)\n",
    "    \n",
    "    aa = np.reshape(aa,(1,10))\n",
    "#     print(aa)\n",
    "    x_train.append(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 1, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.322284, acc.: 89.06%] [G loss: 2.268089]\n",
      "1 [D loss: 0.376707, acc.: 85.94%] [G loss: 3.054292]\n",
      "2 [D loss: 0.166919, acc.: 95.31%] [G loss: 3.367300]\n",
      "3 [D loss: 0.456136, acc.: 81.25%] [G loss: 3.662208]\n",
      "4 [D loss: 0.176976, acc.: 95.31%] [G loss: 3.067615]\n",
      "5 [D loss: 0.245787, acc.: 89.06%] [G loss: 2.988374]\n",
      "6 [D loss: 0.337473, acc.: 87.50%] [G loss: 3.625131]\n",
      "7 [D loss: 0.283979, acc.: 87.50%] [G loss: 2.914041]\n",
      "8 [D loss: 0.285679, acc.: 90.62%] [G loss: 3.359870]\n",
      "9 [D loss: 0.274582, acc.: 89.06%] [G loss: 3.721633]\n",
      "10 [D loss: 0.253553, acc.: 89.06%] [G loss: 3.545729]\n",
      "11 [D loss: 0.416284, acc.: 84.38%] [G loss: 3.007247]\n",
      "12 [D loss: 0.167922, acc.: 98.44%] [G loss: 3.071003]\n",
      "13 [D loss: 0.308602, acc.: 87.50%] [G loss: 3.387944]\n",
      "14 [D loss: 0.130620, acc.: 98.44%] [G loss: 3.286084]\n",
      "15 [D loss: 0.309493, acc.: 92.19%] [G loss: 3.153708]\n",
      "16 [D loss: 0.153339, acc.: 96.88%] [G loss: 3.056521]\n",
      "17 [D loss: 0.240581, acc.: 93.75%] [G loss: 2.985516]\n",
      "18 [D loss: 0.320417, acc.: 87.50%] [G loss: 3.281392]\n",
      "19 [D loss: 0.357947, acc.: 85.94%] [G loss: 3.133519]\n",
      "20 [D loss: 0.293938, acc.: 89.06%] [G loss: 3.086980]\n",
      "21 [D loss: 0.296197, acc.: 89.06%] [G loss: 3.182112]\n",
      "22 [D loss: 0.247239, acc.: 87.50%] [G loss: 3.088570]\n",
      "23 [D loss: 0.117498, acc.: 96.88%] [G loss: 3.802708]\n",
      "24 [D loss: 0.371722, acc.: 90.62%] [G loss: 3.507303]\n",
      "25 [D loss: 0.321645, acc.: 90.62%] [G loss: 3.041810]\n",
      "26 [D loss: 0.408854, acc.: 85.94%] [G loss: 3.208695]\n",
      "27 [D loss: 0.196956, acc.: 92.19%] [G loss: 3.470744]\n",
      "28 [D loss: 0.261669, acc.: 92.19%] [G loss: 2.745144]\n",
      "29 [D loss: 0.138423, acc.: 93.75%] [G loss: 3.214714]\n",
      "30 [D loss: 0.315238, acc.: 85.94%] [G loss: 3.455270]\n",
      "31 [D loss: 0.289961, acc.: 87.50%] [G loss: 3.493831]\n",
      "32 [D loss: 0.255090, acc.: 90.62%] [G loss: 2.953616]\n",
      "33 [D loss: 0.310734, acc.: 87.50%] [G loss: 3.811668]\n",
      "34 [D loss: 0.169125, acc.: 95.31%] [G loss: 3.306637]\n",
      "35 [D loss: 0.227162, acc.: 95.31%] [G loss: 3.994207]\n",
      "36 [D loss: 0.161171, acc.: 96.88%] [G loss: 3.516481]\n",
      "37 [D loss: 0.297324, acc.: 89.06%] [G loss: 3.430744]\n",
      "38 [D loss: 0.245644, acc.: 92.19%] [G loss: 3.478950]\n",
      "39 [D loss: 0.155039, acc.: 95.31%] [G loss: 3.513734]\n",
      "40 [D loss: 0.189943, acc.: 92.19%] [G loss: 3.106027]\n",
      "41 [D loss: 0.365500, acc.: 84.38%] [G loss: 3.519074]\n",
      "42 [D loss: 0.115922, acc.: 96.88%] [G loss: 3.709714]\n",
      "43 [D loss: 0.248587, acc.: 90.62%] [G loss: 3.823259]\n",
      "44 [D loss: 0.302551, acc.: 87.50%] [G loss: 3.910149]\n",
      "45 [D loss: 0.361592, acc.: 90.62%] [G loss: 3.017023]\n",
      "46 [D loss: 0.243784, acc.: 90.62%] [G loss: 3.174765]\n",
      "47 [D loss: 0.181250, acc.: 93.75%] [G loss: 3.686111]\n",
      "48 [D loss: 0.241882, acc.: 87.50%] [G loss: 3.410333]\n",
      "49 [D loss: 0.303205, acc.: 89.06%] [G loss: 2.956914]\n",
      "50 [D loss: 0.149259, acc.: 95.31%] [G loss: 4.142225]\n",
      "tá»ng hha summary : [0]\n",
      "ngá»« ngÃ y : [1]\n",
      "ngá»« 13 biÃ©t : [2]\n",
      "náºµng : [3]\n",
      "45 Äáº±ng : [4]\n",
      "ngá»« den : [5]\n",
      "51 [D loss: 0.370164, acc.: 87.50%] [G loss: 3.210216]\n",
      "52 [D loss: 0.237497, acc.: 92.19%] [G loss: 2.965519]\n",
      "53 [D loss: 0.248887, acc.: 90.62%] [G loss: 2.956677]\n",
      "54 [D loss: 0.210276, acc.: 95.31%] [G loss: 3.278053]\n",
      "55 [D loss: 0.301017, acc.: 92.19%] [G loss: 3.862198]\n",
      "56 [D loss: 0.242788, acc.: 92.19%] [G loss: 2.786488]\n",
      "57 [D loss: 0.134735, acc.: 96.88%] [G loss: 3.512328]\n",
      "58 [D loss: 0.281290, acc.: 89.06%] [G loss: 3.183038]\n",
      "59 [D loss: 0.410183, acc.: 82.81%] [G loss: 3.414334]\n",
      "60 [D loss: 0.213553, acc.: 92.19%] [G loss: 3.849229]\n",
      "61 [D loss: 0.302060, acc.: 89.06%] [G loss: 3.120777]\n",
      "62 [D loss: 0.163916, acc.: 95.31%] [G loss: 3.542466]\n",
      "63 [D loss: 0.431962, acc.: 84.38%] [G loss: 3.436563]\n",
      "64 [D loss: 0.356105, acc.: 85.94%] [G loss: 3.429483]\n",
      "65 [D loss: 0.312528, acc.: 89.06%] [G loss: 2.820192]\n",
      "66 [D loss: 0.302188, acc.: 90.62%] [G loss: 2.861191]\n",
      "67 [D loss: 0.278395, acc.: 90.62%] [G loss: 3.377485]\n",
      "68 [D loss: 0.124807, acc.: 98.44%] [G loss: 3.555545]\n",
      "69 [D loss: 0.305651, acc.: 89.06%] [G loss: 3.689345]\n",
      "70 [D loss: 0.334298, acc.: 87.50%] [G loss: 3.112349]\n",
      "71 [D loss: 0.370430, acc.: 87.50%] [G loss: 3.413142]\n",
      "72 [D loss: 0.196811, acc.: 92.19%] [G loss: 3.708137]\n",
      "73 [D loss: 0.242473, acc.: 89.06%] [G loss: 3.596806]\n",
      "74 [D loss: 0.356617, acc.: 85.94%] [G loss: 3.660656]\n",
      "75 [D loss: 0.226939, acc.: 93.75%] [G loss: 3.143444]\n",
      "76 [D loss: 0.343691, acc.: 90.62%] [G loss: 3.227517]\n",
      "77 [D loss: 0.298984, acc.: 87.50%] [G loss: 3.453625]\n",
      "78 [D loss: 0.161976, acc.: 92.19%] [G loss: 3.645331]\n",
      "79 [D loss: 0.498686, acc.: 71.88%] [G loss: 3.174385]\n",
      "80 [D loss: 0.392757, acc.: 84.38%] [G loss: 3.651592]\n",
      "81 [D loss: 0.335796, acc.: 84.38%] [G loss: 2.922651]\n",
      "82 [D loss: 0.414525, acc.: 82.81%] [G loss: 3.105755]\n",
      "83 [D loss: 0.219929, acc.: 89.06%] [G loss: 3.459846]\n",
      "84 [D loss: 0.345571, acc.: 82.81%] [G loss: 3.065901]\n",
      "85 [D loss: 0.407146, acc.: 85.94%] [G loss: 2.438926]\n",
      "86 [D loss: 0.228651, acc.: 90.62%] [G loss: 3.255932]\n",
      "87 [D loss: 0.196287, acc.: 95.31%] [G loss: 3.388494]\n",
      "88 [D loss: 0.272883, acc.: 90.62%] [G loss: 3.235231]\n",
      "89 [D loss: 0.211892, acc.: 92.19%] [G loss: 3.509608]\n",
      "90 [D loss: 0.219517, acc.: 93.75%] [G loss: 3.064656]\n",
      "91 [D loss: 0.416429, acc.: 79.69%] [G loss: 4.646192]\n",
      "92 [D loss: 0.237469, acc.: 92.19%] [G loss: 4.123741]\n",
      "93 [D loss: 0.229350, acc.: 93.75%] [G loss: 3.732587]\n",
      "94 [D loss: 0.274609, acc.: 92.19%] [G loss: 3.584151]\n",
      "95 [D loss: 0.340630, acc.: 87.50%] [G loss: 3.719114]\n",
      "96 [D loss: 0.257726, acc.: 89.06%] [G loss: 3.344764]\n",
      "97 [D loss: 0.396262, acc.: 82.81%] [G loss: 3.822506]\n",
      "98 [D loss: 0.235698, acc.: 92.19%] [G loss: 3.236925]\n",
      "99 [D loss: 0.169700, acc.: 92.19%] [G loss: 3.770827]\n",
      "100 [D loss: 0.223448, acc.: 95.31%] [G loss: 2.744692]\n",
      "sn tá»ng 1 60 : [0]\n",
      "1 604 ngá»« ngá»« 52001 : [1]\n",
      "ngá»« ÄÃ  : [2]\n",
      "ngá»« tt cÃ¡ 1 ngá»« ngá»« : [3]\n",
      "2 : [4]\n",
      "tÃªn qty : [5]\n",
      "101 [D loss: 0.171506, acc.: 95.31%] [G loss: 3.311544]\n",
      "102 [D loss: 0.180160, acc.: 93.75%] [G loss: 3.677113]\n",
      "103 [D loss: 0.294662, acc.: 90.62%] [G loss: 3.903583]\n",
      "104 [D loss: 0.316964, acc.: 89.06%] [G loss: 4.022678]\n",
      "105 [D loss: 0.205680, acc.: 90.62%] [G loss: 3.830471]\n",
      "106 [D loss: 0.349531, acc.: 84.38%] [G loss: 3.761753]\n",
      "107 [D loss: 0.218352, acc.: 90.62%] [G loss: 3.873636]\n",
      "108 [D loss: 0.313149, acc.: 89.06%] [G loss: 3.669332]\n",
      "109 [D loss: 0.336666, acc.: 87.50%] [G loss: 3.343885]\n",
      "110 [D loss: 0.222646, acc.: 92.19%] [G loss: 3.789940]\n",
      "111 [D loss: 0.218354, acc.: 93.75%] [G loss: 4.071270]\n",
      "112 [D loss: 0.197063, acc.: 93.75%] [G loss: 3.261908]\n",
      "113 [D loss: 0.220318, acc.: 93.75%] [G loss: 3.801839]\n",
      "114 [D loss: 0.301909, acc.: 90.62%] [G loss: 4.291844]\n",
      "115 [D loss: 0.318168, acc.: 84.38%] [G loss: 3.550832]\n",
      "116 [D loss: 0.316385, acc.: 89.06%] [G loss: 3.607379]\n",
      "117 [D loss: 0.290477, acc.: 90.62%] [G loss: 3.216844]\n",
      "118 [D loss: 0.325208, acc.: 87.50%] [G loss: 2.886537]\n",
      "119 [D loss: 0.246879, acc.: 92.19%] [G loss: 3.261855]\n",
      "120 [D loss: 0.245988, acc.: 92.19%] [G loss: 3.192427]\n",
      "121 [D loss: 0.243701, acc.: 90.62%] [G loss: 3.587137]\n",
      "122 [D loss: 0.487759, acc.: 84.38%] [G loss: 3.273273]\n",
      "123 [D loss: 0.280675, acc.: 90.62%] [G loss: 3.220143]\n",
      "124 [D loss: 0.168745, acc.: 95.31%] [G loss: 3.617030]\n",
      "125 [D loss: 0.228137, acc.: 93.75%] [G loss: 3.091563]\n",
      "126 [D loss: 0.229181, acc.: 92.19%] [G loss: 3.517571]\n",
      "127 [D loss: 0.486833, acc.: 85.94%] [G loss: 3.453513]\n",
      "128 [D loss: 0.228063, acc.: 93.75%] [G loss: 3.919942]\n",
      "129 [D loss: 0.308750, acc.: 89.06%] [G loss: 4.205814]\n",
      "130 [D loss: 0.242036, acc.: 93.75%] [G loss: 3.980013]\n",
      "131 [D loss: 0.190459, acc.: 96.88%] [G loss: 3.118862]\n",
      "132 [D loss: 0.176207, acc.: 93.75%] [G loss: 3.143198]\n",
      "133 [D loss: 0.373037, acc.: 87.50%] [G loss: 3.511468]\n",
      "134 [D loss: 0.303480, acc.: 89.06%] [G loss: 3.917353]\n",
      "135 [D loss: 0.277956, acc.: 90.62%] [G loss: 4.450386]\n",
      "136 [D loss: 0.319399, acc.: 89.06%] [G loss: 3.648573]\n",
      "137 [D loss: 0.253484, acc.: 90.62%] [G loss: 3.586143]\n",
      "138 [D loss: 0.342604, acc.: 90.62%] [G loss: 4.001843]\n",
      "139 [D loss: 0.362941, acc.: 82.81%] [G loss: 3.574828]\n",
      "140 [D loss: 0.377763, acc.: 84.38%] [G loss: 3.392454]\n",
      "141 [D loss: 0.215119, acc.: 92.19%] [G loss: 3.838079]\n",
      "142 [D loss: 0.406121, acc.: 79.69%] [G loss: 4.017117]\n",
      "143 [D loss: 0.257748, acc.: 90.62%] [G loss: 3.821230]\n",
      "144 [D loss: 0.440805, acc.: 81.25%] [G loss: 3.139863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 [D loss: 0.333102, acc.: 89.06%] [G loss: 2.708163]\n",
      "146 [D loss: 0.339869, acc.: 87.50%] [G loss: 3.193756]\n",
      "147 [D loss: 0.241824, acc.: 92.19%] [G loss: 2.985665]\n",
      "148 [D loss: 0.241248, acc.: 89.06%] [G loss: 3.921670]\n",
      "149 [D loss: 0.517650, acc.: 79.69%] [G loss: 3.858491]\n",
      "150 [D loss: 0.384658, acc.: 84.38%] [G loss: 3.557752]\n",
      "31000 604 : [0]\n",
      "cuá»n : [1]\n",
      "ngá»« xin : [2]\n",
      "000 4 : [3]\n",
      "giÃ¢y : [4]\n",
      "151 [D loss: 0.367862, acc.: 87.50%] [G loss: 3.789348]\n",
      "152 [D loss: 0.333142, acc.: 87.50%] [G loss: 4.059795]\n",
      "153 [D loss: 0.364895, acc.: 89.06%] [G loss: 3.419548]\n",
      "154 [D loss: 0.281166, acc.: 90.62%] [G loss: 3.464116]\n",
      "155 [D loss: 0.499027, acc.: 81.25%] [G loss: 3.380901]\n",
      "156 [D loss: 0.384914, acc.: 87.50%] [G loss: 3.366820]\n",
      "157 [D loss: 0.660046, acc.: 68.75%] [G loss: 2.082524]\n",
      "158 [D loss: 0.294515, acc.: 90.62%] [G loss: 3.037457]\n",
      "159 [D loss: 0.422452, acc.: 85.94%] [G loss: 3.057096]\n",
      "160 [D loss: 0.430041, acc.: 84.38%] [G loss: 2.815445]\n",
      "161 [D loss: 0.424683, acc.: 81.25%] [G loss: 2.633270]\n",
      "162 [D loss: 0.308668, acc.: 89.06%] [G loss: 3.284112]\n",
      "163 [D loss: 0.409225, acc.: 81.25%] [G loss: 2.784298]\n",
      "164 [D loss: 0.367931, acc.: 87.50%] [G loss: 2.829574]\n",
      "165 [D loss: 0.390289, acc.: 84.38%] [G loss: 3.498767]\n",
      "166 [D loss: 0.451159, acc.: 76.56%] [G loss: 2.696541]\n",
      "167 [D loss: 0.478769, acc.: 76.56%] [G loss: 3.421159]\n",
      "168 [D loss: 0.332213, acc.: 90.62%] [G loss: 2.514852]\n",
      "169 [D loss: 0.286181, acc.: 92.19%] [G loss: 2.969676]\n",
      "170 [D loss: 0.337790, acc.: 87.50%] [G loss: 3.440875]\n",
      "171 [D loss: 0.459337, acc.: 81.25%] [G loss: 3.510854]\n",
      "172 [D loss: 0.348871, acc.: 84.38%] [G loss: 2.780860]\n",
      "173 [D loss: 0.434177, acc.: 79.69%] [G loss: 2.932169]\n",
      "174 [D loss: 0.375950, acc.: 87.50%] [G loss: 2.865595]\n",
      "175 [D loss: 0.423097, acc.: 82.81%] [G loss: 3.312034]\n",
      "176 [D loss: 0.462521, acc.: 81.25%] [G loss: 3.092199]\n",
      "177 [D loss: 0.389054, acc.: 85.94%] [G loss: 3.499690]\n",
      "178 [D loss: 0.365847, acc.: 84.38%] [G loss: 2.612891]\n",
      "179 [D loss: 0.417410, acc.: 79.69%] [G loss: 3.391865]\n",
      "180 [D loss: 0.395970, acc.: 82.81%] [G loss: 3.874757]\n",
      "181 [D loss: 0.340522, acc.: 85.94%] [G loss: 3.575326]\n",
      "182 [D loss: 0.362416, acc.: 89.06%] [G loss: 3.282668]\n",
      "183 [D loss: 0.360276, acc.: 84.38%] [G loss: 3.361887]\n",
      "184 [D loss: 0.400485, acc.: 82.81%] [G loss: 3.791593]\n",
      "185 [D loss: 0.481656, acc.: 78.12%] [G loss: 3.364548]\n",
      "186 [D loss: 0.595522, acc.: 71.88%] [G loss: 2.814775]\n",
      "187 [D loss: 0.430809, acc.: 81.25%] [G loss: 3.673548]\n",
      "188 [D loss: 0.426729, acc.: 82.81%] [G loss: 3.717843]\n",
      "189 [D loss: 0.526182, acc.: 76.56%] [G loss: 3.554746]\n",
      "190 [D loss: 0.354212, acc.: 89.06%] [G loss: 3.108461]\n",
      "191 [D loss: 0.286842, acc.: 92.19%] [G loss: 3.484632]\n",
      "192 [D loss: 0.407752, acc.: 85.94%] [G loss: 3.722543]\n",
      "193 [D loss: 0.383603, acc.: 85.94%] [G loss: 3.451128]\n",
      "194 [D loss: 0.543605, acc.: 73.44%] [G loss: 3.321582]\n",
      "195 [D loss: 0.442868, acc.: 79.69%] [G loss: 3.152206]\n",
      "196 [D loss: 0.432414, acc.: 78.12%] [G loss: 3.001112]\n",
      "197 [D loss: 0.355642, acc.: 85.94%] [G loss: 2.725892]\n",
      "198 [D loss: 0.600584, acc.: 71.88%] [G loss: 4.119548]\n",
      "199 [D loss: 0.520988, acc.: 78.12%] [G loss: 3.192744]\n",
      "200 [D loss: 0.465815, acc.: 79.69%] [G loss: 3.208477]\n",
      "00000066 : [0]\n",
      "136 goods nam thÆ¡m : [1]\n",
      "ngá»« ngá»« joint : [2]\n",
      "sl : [3]\n",
      "99 ngá»« : [4]\n",
      "ngá»« hÃ n : [5]\n",
      "201 [D loss: 0.495625, acc.: 78.12%] [G loss: 2.720478]\n",
      "202 [D loss: 0.483084, acc.: 79.69%] [G loss: 2.821217]\n",
      "203 [D loss: 0.501622, acc.: 78.12%] [G loss: 2.875931]\n",
      "204 [D loss: 0.355843, acc.: 84.38%] [G loss: 2.692867]\n",
      "205 [D loss: 0.326575, acc.: 89.06%] [G loss: 3.858677]\n",
      "206 [D loss: 0.515674, acc.: 78.12%] [G loss: 3.507492]\n",
      "207 [D loss: 0.291656, acc.: 85.94%] [G loss: 3.087656]\n",
      "208 [D loss: 0.382409, acc.: 82.81%] [G loss: 3.709632]\n",
      "209 [D loss: 0.467399, acc.: 81.25%] [G loss: 2.998065]\n",
      "210 [D loss: 0.402415, acc.: 82.81%] [G loss: 2.757468]\n",
      "211 [D loss: 0.380376, acc.: 85.94%] [G loss: 3.353222]\n",
      "212 [D loss: 0.359662, acc.: 84.38%] [G loss: 3.415884]\n",
      "213 [D loss: 0.333784, acc.: 84.38%] [G loss: 3.559762]\n",
      "214 [D loss: 0.477232, acc.: 76.56%] [G loss: 3.456439]\n",
      "215 [D loss: 0.357826, acc.: 85.94%] [G loss: 3.156934]\n",
      "216 [D loss: 0.398286, acc.: 81.25%] [G loss: 2.607818]\n",
      "217 [D loss: 0.351592, acc.: 82.81%] [G loss: 2.811508]\n",
      "218 [D loss: 0.326847, acc.: 87.50%] [G loss: 3.270989]\n",
      "219 [D loss: 0.480962, acc.: 76.56%] [G loss: 3.399130]\n",
      "220 [D loss: 0.391707, acc.: 79.69%] [G loss: 3.872998]\n",
      "221 [D loss: 0.356546, acc.: 85.94%] [G loss: 3.399134]\n",
      "222 [D loss: 0.511175, acc.: 78.12%] [G loss: 2.925405]\n",
      "223 [D loss: 0.492586, acc.: 78.12%] [G loss: 3.232843]\n",
      "224 [D loss: 0.367033, acc.: 81.25%] [G loss: 3.193213]\n",
      "225 [D loss: 0.357156, acc.: 82.81%] [G loss: 2.233277]\n",
      "226 [D loss: 0.541818, acc.: 71.88%] [G loss: 2.645640]\n",
      "227 [D loss: 0.313977, acc.: 90.62%] [G loss: 2.794502]\n",
      "228 [D loss: 0.272621, acc.: 93.75%] [G loss: 3.728923]\n",
      "229 [D loss: 0.504423, acc.: 78.12%] [G loss: 3.109393]\n",
      "230 [D loss: 0.527508, acc.: 70.31%] [G loss: 2.967734]\n",
      "231 [D loss: 0.520188, acc.: 73.44%] [G loss: 2.759467]\n",
      "232 [D loss: 0.334948, acc.: 87.50%] [G loss: 3.124286]\n",
      "233 [D loss: 0.428819, acc.: 85.94%] [G loss: 2.454383]\n",
      "234 [D loss: 0.329193, acc.: 87.50%] [G loss: 2.726001]\n",
      "235 [D loss: 0.417506, acc.: 75.00%] [G loss: 2.976726]\n",
      "236 [D loss: 0.262485, acc.: 92.19%] [G loss: 3.118765]\n",
      "237 [D loss: 0.538235, acc.: 75.00%] [G loss: 3.573508]\n",
      "238 [D loss: 0.318986, acc.: 87.50%] [G loss: 2.871556]\n",
      "239 [D loss: 0.469630, acc.: 78.12%] [G loss: 3.014790]\n",
      "240 [D loss: 0.505899, acc.: 71.88%] [G loss: 3.368140]\n",
      "241 [D loss: 0.405853, acc.: 82.81%] [G loss: 4.285741]\n",
      "242 [D loss: 0.645546, acc.: 65.62%] [G loss: 2.460463]\n",
      "243 [D loss: 0.380605, acc.: 85.94%] [G loss: 2.903037]\n",
      "244 [D loss: 0.536184, acc.: 75.00%] [G loss: 2.581100]\n",
      "245 [D loss: 0.354461, acc.: 85.94%] [G loss: 2.818055]\n",
      "246 [D loss: 0.538351, acc.: 76.56%] [G loss: 2.530259]\n",
      "247 [D loss: 0.395150, acc.: 84.38%] [G loss: 2.908217]\n",
      "248 [D loss: 0.366553, acc.: 84.38%] [G loss: 2.342363]\n",
      "249 [D loss: 0.392914, acc.: 84.38%] [G loss: 2.661528]\n",
      "250 [D loss: 0.374302, acc.: 85.94%] [G loss: 3.060742]\n",
      "251 [D loss: 0.529060, acc.: 76.56%] [G loss: 3.312949]\n",
      "252 [D loss: 0.531421, acc.: 71.88%] [G loss: 3.840098]\n",
      "253 [D loss: 0.356807, acc.: 84.38%] [G loss: 3.570500]\n",
      "254 [D loss: 0.349589, acc.: 84.38%] [G loss: 3.038567]\n",
      "255 [D loss: 0.410062, acc.: 84.38%] [G loss: 3.253724]\n",
      "256 [D loss: 0.486482, acc.: 76.56%] [G loss: 3.936319]\n",
      "257 [D loss: 0.379523, acc.: 84.38%] [G loss: 3.443602]\n",
      "258 [D loss: 0.547546, acc.: 73.44%] [G loss: 3.253511]\n",
      "259 [D loss: 0.496362, acc.: 73.44%] [G loss: 2.915762]\n",
      "260 [D loss: 0.429206, acc.: 73.44%] [G loss: 2.838001]\n",
      "261 [D loss: 0.420102, acc.: 79.69%] [G loss: 3.515318]\n",
      "262 [D loss: 0.297473, acc.: 90.62%] [G loss: 3.319172]\n",
      "263 [D loss: 0.495501, acc.: 68.75%] [G loss: 3.099111]\n",
      "264 [D loss: 0.388924, acc.: 81.25%] [G loss: 3.340429]\n",
      "265 [D loss: 0.519898, acc.: 73.44%] [G loss: 2.476715]\n",
      "266 [D loss: 0.380511, acc.: 84.38%] [G loss: 2.692907]\n",
      "267 [D loss: 0.451004, acc.: 78.12%] [G loss: 2.845210]\n",
      "268 [D loss: 0.258738, acc.: 90.62%] [G loss: 3.069581]\n",
      "269 [D loss: 0.448351, acc.: 79.69%] [G loss: 3.022685]\n",
      "270 [D loss: 0.357250, acc.: 85.94%] [G loss: 2.747010]\n",
      "271 [D loss: 0.474681, acc.: 79.69%] [G loss: 3.550790]\n",
      "272 [D loss: 0.456474, acc.: 78.12%] [G loss: 3.389620]\n",
      "273 [D loss: 0.406750, acc.: 82.81%] [G loss: 3.635185]\n",
      "274 [D loss: 0.513997, acc.: 73.44%] [G loss: 3.955716]\n",
      "275 [D loss: 0.473503, acc.: 73.44%] [G loss: 3.323870]\n",
      "276 [D loss: 0.397056, acc.: 82.81%] [G loss: 3.470597]\n",
      "277 [D loss: 0.545121, acc.: 71.88%] [G loss: 3.350984]\n",
      "278 [D loss: 0.379390, acc.: 81.25%] [G loss: 3.702948]\n",
      "279 [D loss: 0.419167, acc.: 79.69%] [G loss: 3.410972]\n",
      "280 [D loss: 0.508489, acc.: 76.56%] [G loss: 3.036745]\n",
      "281 [D loss: 0.504964, acc.: 73.44%] [G loss: 2.551359]\n",
      "282 [D loss: 0.351763, acc.: 82.81%] [G loss: 3.686366]\n",
      "283 [D loss: 0.293441, acc.: 90.62%] [G loss: 3.145486]\n",
      "284 [D loss: 0.476496, acc.: 79.69%] [G loss: 3.060478]\n",
      "285 [D loss: 0.495950, acc.: 79.69%] [G loss: 3.574545]\n",
      "286 [D loss: 0.553343, acc.: 75.00%] [G loss: 3.074522]\n",
      "287 [D loss: 0.472628, acc.: 78.12%] [G loss: 2.765369]\n",
      "288 [D loss: 0.394721, acc.: 81.25%] [G loss: 2.929285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289 [D loss: 0.477438, acc.: 73.44%] [G loss: 3.319340]\n",
      "290 [D loss: 0.505857, acc.: 73.44%] [G loss: 3.524441]\n",
      "291 [D loss: 0.497352, acc.: 68.75%] [G loss: 2.742554]\n",
      "292 [D loss: 0.525956, acc.: 67.19%] [G loss: 2.366599]\n",
      "293 [D loss: 0.426361, acc.: 78.12%] [G loss: 3.177254]\n",
      "294 [D loss: 0.458717, acc.: 78.12%] [G loss: 2.637603]\n",
      "295 [D loss: 0.429441, acc.: 79.69%] [G loss: 2.327596]\n",
      "296 [D loss: 0.455274, acc.: 76.56%] [G loss: 2.866589]\n",
      "297 [D loss: 0.351170, acc.: 87.50%] [G loss: 2.429166]\n",
      "298 [D loss: 0.421864, acc.: 81.25%] [G loss: 3.721194]\n",
      "299 [D loss: 0.334139, acc.: 84.38%] [G loss: 3.427050]\n",
      "300 [D loss: 0.419423, acc.: 79.69%] [G loss: 3.216164]\n",
      "301 [D loss: 0.460941, acc.: 76.56%] [G loss: 3.477185]\n",
      "302 [D loss: 0.419219, acc.: 76.56%] [G loss: 2.870505]\n",
      "303 [D loss: 0.411297, acc.: 76.56%] [G loss: 3.250077]\n",
      "304 [D loss: 0.477691, acc.: 73.44%] [G loss: 4.018425]\n",
      "305 [D loss: 0.457793, acc.: 78.12%] [G loss: 3.259924]\n",
      "306 [D loss: 0.354786, acc.: 82.81%] [G loss: 3.602116]\n",
      "307 [D loss: 0.457423, acc.: 76.56%] [G loss: 3.488196]\n",
      "308 [D loss: 0.348173, acc.: 82.81%] [G loss: 3.968480]\n",
      "309 [D loss: 0.328874, acc.: 87.50%] [G loss: 3.513869]\n",
      "310 [D loss: 0.384349, acc.: 81.25%] [G loss: 3.697703]\n",
      "311 [D loss: 0.376561, acc.: 81.25%] [G loss: 3.794606]\n",
      "312 [D loss: 0.463767, acc.: 73.44%] [G loss: 3.483834]\n",
      "313 [D loss: 0.573628, acc.: 70.31%] [G loss: 3.138883]\n",
      "314 [D loss: 0.367794, acc.: 82.81%] [G loss: 3.704468]\n",
      "315 [D loss: 0.505978, acc.: 76.56%] [G loss: 3.227850]\n",
      "316 [D loss: 0.316810, acc.: 85.94%] [G loss: 3.109391]\n",
      "317 [D loss: 0.473318, acc.: 71.88%] [G loss: 2.956122]\n",
      "318 [D loss: 0.387357, acc.: 81.25%] [G loss: 3.126513]\n",
      "319 [D loss: 0.536661, acc.: 68.75%] [G loss: 3.226353]\n",
      "320 [D loss: 0.563454, acc.: 71.88%] [G loss: 2.935106]\n",
      "321 [D loss: 0.343872, acc.: 84.38%] [G loss: 2.852947]\n",
      "322 [D loss: 0.423190, acc.: 78.12%] [G loss: 2.811815]\n",
      "323 [D loss: 0.405706, acc.: 79.69%] [G loss: 3.679202]\n",
      "324 [D loss: 0.371270, acc.: 81.25%] [G loss: 3.734369]\n",
      "325 [D loss: 0.437403, acc.: 73.44%] [G loss: 3.414991]\n",
      "326 [D loss: 0.500687, acc.: 68.75%] [G loss: 2.875584]\n",
      "327 [D loss: 0.328212, acc.: 82.81%] [G loss: 3.554625]\n",
      "328 [D loss: 0.396770, acc.: 79.69%] [G loss: 2.949461]\n",
      "329 [D loss: 0.560015, acc.: 68.75%] [G loss: 3.165517]\n",
      "330 [D loss: 0.302695, acc.: 87.50%] [G loss: 3.527480]\n",
      "331 [D loss: 0.390132, acc.: 81.25%] [G loss: 3.922681]\n",
      "332 [D loss: 0.384608, acc.: 84.38%] [G loss: 3.221451]\n",
      "333 [D loss: 0.552693, acc.: 71.88%] [G loss: 3.387146]\n",
      "334 [D loss: 0.484173, acc.: 71.88%] [G loss: 3.435127]\n",
      "335 [D loss: 0.406219, acc.: 79.69%] [G loss: 2.736775]\n",
      "336 [D loss: 0.462218, acc.: 71.88%] [G loss: 2.850915]\n",
      "337 [D loss: 0.313805, acc.: 89.06%] [G loss: 3.026929]\n",
      "338 [D loss: 0.385880, acc.: 82.81%] [G loss: 3.433688]\n",
      "339 [D loss: 0.500221, acc.: 71.88%] [G loss: 3.497339]\n",
      "340 [D loss: 0.468780, acc.: 73.44%] [G loss: 3.687924]\n",
      "341 [D loss: 0.525739, acc.: 67.19%] [G loss: 2.588963]\n",
      "342 [D loss: 0.523610, acc.: 73.44%] [G loss: 3.361338]\n",
      "343 [D loss: 0.464679, acc.: 70.31%] [G loss: 3.631629]\n",
      "344 [D loss: 0.580220, acc.: 65.62%] [G loss: 3.304126]\n",
      "345 [D loss: 0.466728, acc.: 75.00%] [G loss: 2.765488]\n",
      "346 [D loss: 0.431752, acc.: 76.56%] [G loss: 2.814970]\n",
      "347 [D loss: 0.632740, acc.: 60.94%] [G loss: 2.294534]\n",
      "348 [D loss: 0.370169, acc.: 84.38%] [G loss: 2.822409]\n",
      "349 [D loss: 0.516433, acc.: 64.06%] [G loss: 2.490932]\n",
      "350 [D loss: 0.446631, acc.: 75.00%] [G loss: 2.587858]\n",
      "24 ngá»« 32000 : [0]\n",
      "351 [D loss: 0.383769, acc.: 75.00%] [G loss: 2.560427]\n",
      "352 [D loss: 0.458918, acc.: 65.62%] [G loss: 3.516839]\n",
      "353 [D loss: 0.433389, acc.: 76.56%] [G loss: 3.500971]\n",
      "354 [D loss: 0.463502, acc.: 81.25%] [G loss: 2.835694]\n",
      "355 [D loss: 0.446453, acc.: 71.88%] [G loss: 3.089327]\n",
      "356 [D loss: 0.310542, acc.: 85.94%] [G loss: 2.576357]\n",
      "357 [D loss: 0.370513, acc.: 79.69%] [G loss: 2.660259]\n",
      "358 [D loss: 0.472584, acc.: 73.44%] [G loss: 2.173302]\n",
      "359 [D loss: 0.340511, acc.: 85.94%] [G loss: 2.738106]\n",
      "360 [D loss: 0.408524, acc.: 75.00%] [G loss: 1.788332]\n",
      "361 [D loss: 0.368224, acc.: 82.81%] [G loss: 2.830579]\n",
      "362 [D loss: 0.349363, acc.: 85.94%] [G loss: 2.627376]\n",
      "363 [D loss: 0.345522, acc.: 78.12%] [G loss: 3.134792]\n",
      "364 [D loss: 0.393406, acc.: 71.88%] [G loss: 2.810439]\n",
      "365 [D loss: 0.338632, acc.: 84.38%] [G loss: 3.015811]\n",
      "366 [D loss: 0.289999, acc.: 87.50%] [G loss: 3.900724]\n",
      "367 [D loss: 0.377701, acc.: 79.69%] [G loss: 3.654079]\n",
      "368 [D loss: 0.311834, acc.: 82.81%] [G loss: 3.600482]\n",
      "369 [D loss: 0.408741, acc.: 73.44%] [G loss: 3.598692]\n",
      "370 [D loss: 0.508041, acc.: 71.88%] [G loss: 3.239346]\n",
      "371 [D loss: 0.424087, acc.: 78.12%] [G loss: 2.953051]\n",
      "372 [D loss: 0.365805, acc.: 76.56%] [G loss: 2.947834]\n",
      "373 [D loss: 0.373838, acc.: 75.00%] [G loss: 2.537951]\n",
      "374 [D loss: 0.416038, acc.: 78.12%] [G loss: 2.411207]\n",
      "375 [D loss: 0.451572, acc.: 76.56%] [G loss: 2.671511]\n",
      "376 [D loss: 0.385191, acc.: 78.12%] [G loss: 2.761681]\n",
      "377 [D loss: 0.517543, acc.: 75.00%] [G loss: 3.426830]\n",
      "378 [D loss: 0.494402, acc.: 73.44%] [G loss: 3.118514]\n",
      "379 [D loss: 0.303886, acc.: 81.25%] [G loss: 2.841025]\n",
      "380 [D loss: 0.414454, acc.: 82.81%] [G loss: 2.956256]\n",
      "381 [D loss: 0.426634, acc.: 75.00%] [G loss: 2.424228]\n",
      "382 [D loss: 0.381459, acc.: 78.12%] [G loss: 3.269454]\n",
      "383 [D loss: 0.454172, acc.: 76.56%] [G loss: 3.571099]\n",
      "384 [D loss: 0.542210, acc.: 64.06%] [G loss: 3.308750]\n",
      "385 [D loss: 0.476180, acc.: 70.31%] [G loss: 3.096133]\n",
      "386 [D loss: 0.501755, acc.: 64.06%] [G loss: 2.586563]\n",
      "387 [D loss: 0.473346, acc.: 68.75%] [G loss: 2.912463]\n",
      "388 [D loss: 0.485549, acc.: 75.00%] [G loss: 2.916447]\n",
      "389 [D loss: 0.422830, acc.: 70.31%] [G loss: 2.509366]\n",
      "390 [D loss: 0.460052, acc.: 71.88%] [G loss: 2.844015]\n",
      "391 [D loss: 0.435436, acc.: 76.56%] [G loss: 3.911155]\n",
      "392 [D loss: 0.395460, acc.: 76.56%] [G loss: 2.877110]\n",
      "393 [D loss: 0.376116, acc.: 79.69%] [G loss: 3.112648]\n",
      "394 [D loss: 0.414945, acc.: 82.81%] [G loss: 3.053059]\n",
      "395 [D loss: 0.418481, acc.: 71.88%] [G loss: 3.192150]\n",
      "396 [D loss: 0.547175, acc.: 78.12%] [G loss: 2.568019]\n",
      "397 [D loss: 0.320281, acc.: 85.94%] [G loss: 3.014360]\n",
      "398 [D loss: 0.272872, acc.: 89.06%] [G loss: 3.136590]\n",
      "399 [D loss: 0.440917, acc.: 73.44%] [G loss: 3.252220]\n",
      "400 [D loss: 0.479326, acc.: 76.56%] [G loss: 2.984609]\n",
      "401 [D loss: 0.368870, acc.: 81.25%] [G loss: 3.026374]\n",
      "402 [D loss: 0.520826, acc.: 62.50%] [G loss: 2.686133]\n",
      "403 [D loss: 0.423554, acc.: 75.00%] [G loss: 3.036517]\n",
      "404 [D loss: 0.401165, acc.: 78.12%] [G loss: 2.600921]\n",
      "405 [D loss: 0.495392, acc.: 73.44%] [G loss: 2.898576]\n",
      "406 [D loss: 0.380150, acc.: 84.38%] [G loss: 2.211424]\n",
      "407 [D loss: 0.417621, acc.: 71.88%] [G loss: 2.909119]\n",
      "408 [D loss: 0.495043, acc.: 65.62%] [G loss: 2.127271]\n",
      "409 [D loss: 0.428240, acc.: 87.50%] [G loss: 2.685419]\n",
      "410 [D loss: 0.424948, acc.: 76.56%] [G loss: 3.074377]\n",
      "411 [D loss: 0.468900, acc.: 73.44%] [G loss: 2.525963]\n",
      "412 [D loss: 0.724429, acc.: 60.94%] [G loss: 2.469803]\n",
      "413 [D loss: 0.428174, acc.: 76.56%] [G loss: 3.118583]\n",
      "414 [D loss: 0.557079, acc.: 68.75%] [G loss: 3.344126]\n",
      "415 [D loss: 0.419893, acc.: 73.44%] [G loss: 2.914874]\n",
      "416 [D loss: 0.413438, acc.: 76.56%] [G loss: 2.638142]\n",
      "417 [D loss: 0.620211, acc.: 51.56%] [G loss: 2.528622]\n",
      "418 [D loss: 0.394288, acc.: 78.12%] [G loss: 2.581275]\n",
      "419 [D loss: 0.523255, acc.: 65.62%] [G loss: 2.669827]\n",
      "420 [D loss: 0.614460, acc.: 62.50%] [G loss: 3.291868]\n",
      "421 [D loss: 0.534269, acc.: 75.00%] [G loss: 2.594249]\n",
      "422 [D loss: 0.371930, acc.: 89.06%] [G loss: 2.160416]\n",
      "423 [D loss: 0.450433, acc.: 71.88%] [G loss: 2.369020]\n",
      "424 [D loss: 0.406698, acc.: 82.81%] [G loss: 2.442821]\n",
      "425 [D loss: 0.428436, acc.: 75.00%] [G loss: 3.016806]\n",
      "426 [D loss: 0.406072, acc.: 81.25%] [G loss: 3.171423]\n",
      "427 [D loss: 0.525863, acc.: 75.00%] [G loss: 2.917678]\n",
      "428 [D loss: 0.545197, acc.: 73.44%] [G loss: 2.825805]\n",
      "429 [D loss: 0.408920, acc.: 82.81%] [G loss: 2.486131]\n",
      "430 [D loss: 0.583302, acc.: 68.75%] [G loss: 2.734090]\n",
      "431 [D loss: 0.467340, acc.: 78.12%] [G loss: 2.989022]\n",
      "432 [D loss: 0.591370, acc.: 54.69%] [G loss: 2.210418]\n",
      "433 [D loss: 0.485556, acc.: 76.56%] [G loss: 2.102327]\n",
      "434 [D loss: 0.574732, acc.: 68.75%] [G loss: 2.854992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435 [D loss: 0.403153, acc.: 75.00%] [G loss: 3.113393]\n",
      "436 [D loss: 0.416278, acc.: 73.44%] [G loss: 3.356145]\n",
      "437 [D loss: 0.404466, acc.: 78.12%] [G loss: 2.655216]\n",
      "438 [D loss: 0.474793, acc.: 76.56%] [G loss: 2.893569]\n",
      "439 [D loss: 0.415365, acc.: 78.12%] [G loss: 2.654727]\n",
      "440 [D loss: 0.415918, acc.: 84.38%] [G loss: 2.994840]\n",
      "441 [D loss: 0.508466, acc.: 71.88%] [G loss: 2.783825]\n",
      "442 [D loss: 0.446413, acc.: 78.12%] [G loss: 3.400716]\n",
      "443 [D loss: 0.548216, acc.: 71.88%] [G loss: 2.735521]\n",
      "444 [D loss: 0.416114, acc.: 76.56%] [G loss: 3.383129]\n",
      "445 [D loss: 0.448748, acc.: 71.88%] [G loss: 2.704679]\n",
      "446 [D loss: 0.564007, acc.: 71.88%] [G loss: 2.959918]\n",
      "447 [D loss: 0.324198, acc.: 84.38%] [G loss: 2.092519]\n",
      "448 [D loss: 0.501459, acc.: 73.44%] [G loss: 2.633904]\n",
      "449 [D loss: 0.387009, acc.: 76.56%] [G loss: 2.420676]\n",
      "450 [D loss: 0.490483, acc.: 67.19%] [G loss: 3.023414]\n",
      "30 1989 36 0903144 : [0]\n",
      "ngá»« : [1]\n",
      "000 cÆ¡ 12 1 000 sl : [2]\n",
      "451 [D loss: 0.509787, acc.: 65.62%] [G loss: 2.261121]\n",
      "452 [D loss: 0.545543, acc.: 67.19%] [G loss: 2.251533]\n",
      "453 [D loss: 0.441805, acc.: 73.44%] [G loss: 2.581717]\n",
      "454 [D loss: 0.493139, acc.: 73.44%] [G loss: 2.484842]\n",
      "455 [D loss: 0.402808, acc.: 73.44%] [G loss: 1.860004]\n",
      "456 [D loss: 0.404284, acc.: 81.25%] [G loss: 3.050750]\n",
      "457 [D loss: 0.450105, acc.: 70.31%] [G loss: 2.432522]\n",
      "458 [D loss: 0.422445, acc.: 70.31%] [G loss: 2.969130]\n",
      "459 [D loss: 0.590756, acc.: 67.19%] [G loss: 2.224060]\n",
      "460 [D loss: 0.469804, acc.: 84.38%] [G loss: 1.901899]\n",
      "461 [D loss: 0.405511, acc.: 82.81%] [G loss: 2.317590]\n",
      "462 [D loss: 0.584633, acc.: 65.62%] [G loss: 2.015174]\n",
      "463 [D loss: 0.435555, acc.: 75.00%] [G loss: 2.772954]\n",
      "464 [D loss: 0.530015, acc.: 62.50%] [G loss: 2.071590]\n",
      "465 [D loss: 0.405522, acc.: 78.12%] [G loss: 2.478968]\n",
      "466 [D loss: 0.506111, acc.: 68.75%] [G loss: 1.930037]\n",
      "467 [D loss: 0.439519, acc.: 75.00%] [G loss: 2.519897]\n",
      "468 [D loss: 0.489837, acc.: 79.69%] [G loss: 2.398464]\n",
      "469 [D loss: 0.415910, acc.: 82.81%] [G loss: 2.497636]\n",
      "470 [D loss: 0.370951, acc.: 78.12%] [G loss: 2.278911]\n",
      "471 [D loss: 0.414615, acc.: 81.25%] [G loss: 3.135509]\n",
      "472 [D loss: 0.436119, acc.: 71.88%] [G loss: 2.247346]\n",
      "473 [D loss: 0.461466, acc.: 68.75%] [G loss: 1.753351]\n",
      "474 [D loss: 0.419030, acc.: 79.69%] [G loss: 2.261123]\n",
      "475 [D loss: 0.486755, acc.: 71.88%] [G loss: 2.465068]\n",
      "476 [D loss: 0.549429, acc.: 64.06%] [G loss: 2.314109]\n",
      "477 [D loss: 0.602543, acc.: 67.19%] [G loss: 2.532556]\n",
      "478 [D loss: 0.429001, acc.: 79.69%] [G loss: 3.339546]\n",
      "479 [D loss: 0.503463, acc.: 70.31%] [G loss: 2.117018]\n",
      "480 [D loss: 0.495427, acc.: 67.19%] [G loss: 1.996904]\n",
      "481 [D loss: 0.531325, acc.: 78.12%] [G loss: 1.868714]\n",
      "482 [D loss: 0.386233, acc.: 81.25%] [G loss: 2.529854]\n",
      "483 [D loss: 0.418805, acc.: 75.00%] [G loss: 2.724081]\n",
      "484 [D loss: 0.434988, acc.: 79.69%] [G loss: 1.748794]\n",
      "485 [D loss: 0.442459, acc.: 78.12%] [G loss: 1.587171]\n",
      "486 [D loss: 0.582996, acc.: 59.38%] [G loss: 1.837883]\n",
      "487 [D loss: 0.449669, acc.: 78.12%] [G loss: 3.040800]\n",
      "488 [D loss: 0.615017, acc.: 70.31%] [G loss: 2.044346]\n",
      "489 [D loss: 0.432751, acc.: 79.69%] [G loss: 2.161198]\n",
      "490 [D loss: 0.346819, acc.: 82.81%] [G loss: 2.315273]\n",
      "491 [D loss: 0.501724, acc.: 79.69%] [G loss: 2.944654]\n",
      "492 [D loss: 0.401487, acc.: 73.44%] [G loss: 2.556703]\n",
      "493 [D loss: 0.525502, acc.: 71.88%] [G loss: 2.386663]\n",
      "494 [D loss: 0.674916, acc.: 60.94%] [G loss: 1.715662]\n",
      "495 [D loss: 0.454551, acc.: 79.69%] [G loss: 2.316127]\n",
      "496 [D loss: 0.512245, acc.: 78.12%] [G loss: 2.341086]\n",
      "497 [D loss: 0.496503, acc.: 65.62%] [G loss: 1.786796]\n",
      "498 [D loss: 0.496976, acc.: 75.00%] [G loss: 2.885883]\n",
      "499 [D loss: 0.553000, acc.: 76.56%] [G loss: 1.997228]\n",
      "500 [D loss: 0.601264, acc.: 68.75%] [G loss: 2.266639]\n",
      "000 ngá»« : [0]\n",
      "000 : [1]\n",
      "000 2 náº¥m 15 60 : [2]\n",
      "tÃªn 000 : [3]\n",
      "ngá»« : [4]\n",
      "sl cÃ¡ tráº§n : [5]\n",
      "501 [D loss: 0.374897, acc.: 81.25%] [G loss: 2.494360]\n",
      "502 [D loss: 0.625327, acc.: 62.50%] [G loss: 2.156352]\n",
      "503 [D loss: 0.376640, acc.: 82.81%] [G loss: 2.397326]\n",
      "504 [D loss: 0.334927, acc.: 85.94%] [G loss: 1.840801]\n",
      "505 [D loss: 0.470645, acc.: 68.75%] [G loss: 2.521537]\n",
      "506 [D loss: 0.561522, acc.: 70.31%] [G loss: 2.084290]\n",
      "507 [D loss: 0.543771, acc.: 76.56%] [G loss: 2.848795]\n",
      "508 [D loss: 0.548773, acc.: 62.50%] [G loss: 2.375467]\n",
      "509 [D loss: 0.623601, acc.: 64.06%] [G loss: 2.227215]\n",
      "510 [D loss: 0.730183, acc.: 56.25%] [G loss: 1.669684]\n",
      "511 [D loss: 0.436166, acc.: 85.94%] [G loss: 1.679167]\n",
      "512 [D loss: 0.419135, acc.: 81.25%] [G loss: 1.837744]\n",
      "513 [D loss: 0.400156, acc.: 82.81%] [G loss: 2.093079]\n",
      "514 [D loss: 0.547964, acc.: 75.00%] [G loss: 1.991917]\n",
      "515 [D loss: 0.512671, acc.: 76.56%] [G loss: 2.246977]\n",
      "516 [D loss: 0.477593, acc.: 78.12%] [G loss: 1.647924]\n",
      "517 [D loss: 0.478725, acc.: 67.19%] [G loss: 2.555167]\n",
      "518 [D loss: 0.419186, acc.: 76.56%] [G loss: 1.817556]\n",
      "519 [D loss: 0.357806, acc.: 90.62%] [G loss: 2.419123]\n",
      "520 [D loss: 0.628603, acc.: 67.19%] [G loss: 2.069369]\n",
      "521 [D loss: 0.436955, acc.: 85.94%] [G loss: 1.673762]\n",
      "522 [D loss: 0.478672, acc.: 68.75%] [G loss: 2.101925]\n",
      "523 [D loss: 0.534502, acc.: 73.44%] [G loss: 1.775221]\n",
      "524 [D loss: 0.427464, acc.: 79.69%] [G loss: 2.003745]\n",
      "525 [D loss: 0.456826, acc.: 78.12%] [G loss: 1.915273]\n",
      "526 [D loss: 0.376009, acc.: 84.38%] [G loss: 2.284155]\n",
      "527 [D loss: 0.522575, acc.: 78.12%] [G loss: 2.022484]\n",
      "528 [D loss: 0.458763, acc.: 81.25%] [G loss: 2.344755]\n",
      "529 [D loss: 0.533881, acc.: 64.06%] [G loss: 2.398009]\n",
      "530 [D loss: 0.536689, acc.: 71.88%] [G loss: 2.511688]\n",
      "531 [D loss: 0.491661, acc.: 71.88%] [G loss: 2.029383]\n",
      "532 [D loss: 0.398531, acc.: 84.38%] [G loss: 2.099517]\n",
      "533 [D loss: 0.529887, acc.: 71.88%] [G loss: 2.141555]\n",
      "534 [D loss: 0.435309, acc.: 75.00%] [G loss: 2.161394]\n",
      "535 [D loss: 0.563402, acc.: 73.44%] [G loss: 2.242581]\n",
      "536 [D loss: 0.526184, acc.: 71.88%] [G loss: 1.893942]\n",
      "537 [D loss: 0.396756, acc.: 82.81%] [G loss: 2.441375]\n",
      "538 [D loss: 0.392133, acc.: 79.69%] [G loss: 2.088953]\n",
      "539 [D loss: 0.377331, acc.: 81.25%] [G loss: 2.061137]\n",
      "540 [D loss: 0.415105, acc.: 78.12%] [G loss: 2.237805]\n",
      "541 [D loss: 0.542415, acc.: 71.88%] [G loss: 1.568007]\n",
      "542 [D loss: 0.365646, acc.: 79.69%] [G loss: 2.460119]\n",
      "543 [D loss: 0.504823, acc.: 76.56%] [G loss: 2.192944]\n",
      "544 [D loss: 0.365429, acc.: 81.25%] [G loss: 2.364758]\n",
      "545 [D loss: 0.501348, acc.: 79.69%] [G loss: 1.781594]\n",
      "546 [D loss: 0.401471, acc.: 82.81%] [G loss: 1.640805]\n",
      "547 [D loss: 0.356817, acc.: 85.94%] [G loss: 1.853168]\n",
      "548 [D loss: 0.499098, acc.: 78.12%] [G loss: 1.843121]\n",
      "549 [D loss: 0.345587, acc.: 85.94%] [G loss: 2.439334]\n",
      "550 [D loss: 0.557098, acc.: 75.00%] [G loss: 2.195930]\n",
      "551 [D loss: 0.475590, acc.: 71.88%] [G loss: 2.234138]\n",
      "552 [D loss: 0.409059, acc.: 81.25%] [G loss: 1.955137]\n",
      "553 [D loss: 0.647045, acc.: 70.31%] [G loss: 2.413447]\n",
      "554 [D loss: 0.486937, acc.: 79.69%] [G loss: 1.649071]\n",
      "555 [D loss: 0.561257, acc.: 75.00%] [G loss: 2.084195]\n",
      "556 [D loss: 0.482789, acc.: 79.69%] [G loss: 2.114580]\n",
      "557 [D loss: 0.526254, acc.: 71.88%] [G loss: 1.561474]\n",
      "558 [D loss: 0.506583, acc.: 75.00%] [G loss: 1.800732]\n",
      "559 [D loss: 0.374460, acc.: 84.38%] [G loss: 1.632531]\n",
      "560 [D loss: 0.450555, acc.: 78.12%] [G loss: 1.691029]\n",
      "561 [D loss: 0.445938, acc.: 82.81%] [G loss: 1.711057]\n",
      "562 [D loss: 0.324667, acc.: 90.62%] [G loss: 1.828446]\n",
      "563 [D loss: 0.353140, acc.: 82.81%] [G loss: 2.476447]\n",
      "564 [D loss: 0.410570, acc.: 79.69%] [G loss: 2.634666]\n",
      "565 [D loss: 0.385353, acc.: 87.50%] [G loss: 2.438318]\n",
      "566 [D loss: 0.478449, acc.: 79.69%] [G loss: 2.490808]\n",
      "567 [D loss: 0.437513, acc.: 71.88%] [G loss: 2.595766]\n",
      "568 [D loss: 0.311087, acc.: 87.50%] [G loss: 2.042008]\n",
      "569 [D loss: 0.347737, acc.: 82.81%] [G loss: 2.742764]\n",
      "570 [D loss: 0.331386, acc.: 90.62%] [G loss: 2.560071]\n",
      "571 [D loss: 0.342061, acc.: 84.38%] [G loss: 2.445029]\n",
      "572 [D loss: 0.426732, acc.: 76.56%] [G loss: 2.623908]\n",
      "573 [D loss: 0.544124, acc.: 73.44%] [G loss: 2.812255]\n",
      "574 [D loss: 0.425572, acc.: 81.25%] [G loss: 2.305868]\n",
      "575 [D loss: 0.359886, acc.: 84.38%] [G loss: 2.058466]\n",
      "576 [D loss: 0.548312, acc.: 81.25%] [G loss: 2.931719]\n",
      "577 [D loss: 0.430171, acc.: 79.69%] [G loss: 2.616634]\n",
      "578 [D loss: 0.408196, acc.: 79.69%] [G loss: 2.500284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579 [D loss: 0.320151, acc.: 85.94%] [G loss: 2.744467]\n",
      "580 [D loss: 0.444627, acc.: 70.31%] [G loss: 2.655011]\n",
      "581 [D loss: 0.395619, acc.: 87.50%] [G loss: 2.315201]\n",
      "582 [D loss: 0.338669, acc.: 84.38%] [G loss: 1.984555]\n",
      "583 [D loss: 0.432344, acc.: 78.12%] [G loss: 2.695192]\n",
      "584 [D loss: 0.266761, acc.: 92.19%] [G loss: 2.603987]\n",
      "585 [D loss: 0.585702, acc.: 60.94%] [G loss: 1.956355]\n",
      "586 [D loss: 0.325747, acc.: 87.50%] [G loss: 2.482729]\n",
      "587 [D loss: 0.502369, acc.: 75.00%] [G loss: 2.469703]\n",
      "588 [D loss: 0.438772, acc.: 79.69%] [G loss: 2.220942]\n",
      "589 [D loss: 0.427118, acc.: 79.69%] [G loss: 2.017577]\n",
      "590 [D loss: 0.280312, acc.: 89.06%] [G loss: 2.931275]\n",
      "591 [D loss: 0.406294, acc.: 81.25%] [G loss: 2.419018]\n",
      "592 [D loss: 0.300327, acc.: 87.50%] [G loss: 2.843345]\n",
      "593 [D loss: 0.520088, acc.: 79.69%] [G loss: 2.423062]\n",
      "594 [D loss: 0.501844, acc.: 84.38%] [G loss: 2.441606]\n",
      "595 [D loss: 0.365501, acc.: 84.38%] [G loss: 2.981214]\n",
      "596 [D loss: 0.405397, acc.: 85.94%] [G loss: 3.067921]\n",
      "597 [D loss: 0.647903, acc.: 68.75%] [G loss: 1.179026]\n",
      "598 [D loss: 0.481675, acc.: 82.81%] [G loss: 2.289575]\n",
      "599 [D loss: 0.508712, acc.: 71.88%] [G loss: 2.315593]\n",
      "600 [D loss: 0.458597, acc.: 79.69%] [G loss: 2.528422]\n",
      "ngá»« cam ngá»« : [0]\n",
      "17 0 bÃ¡i thÃºc : [1]\n",
      "10 vat 150 : [2]\n",
      "000 sl quÃ½ : [3]\n",
      "1 sl : [4]\n",
      "2 total mantis 2 hÃ ng : [5]\n"
     ]
    }
   ],
   "source": [
    "# Traing\n",
    "epochs = 20001\n",
    "batch_size=32\n",
    "sample_interval=200\n",
    "\n",
    "# Load the dataset\n",
    "# Load the dataset\n",
    "# (X_train, y_train), (_, _) = mnist.load_data()\n",
    "X_train = x_train\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Configure input\n",
    "half_vocab = Tokenizer_vocab_size/2\n",
    "X_train = (X_train.astype(np.float32) - half_vocab) / half_vocab\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "# Adversarial ground truths\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "\n",
    "    # Select a random half batch of images\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    imgs, labels = X_train[idx], y_train[idx]\n",
    "\n",
    "    # Sample noise as generator input\n",
    "    noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "    # Generate a half batch of new images\n",
    "    gen_imgs = generator.predict([noise, labels])\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss_real = discriminator.train_on_batch([imgs, labels], valid)\n",
    "    d_loss_fake = discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    # Condition on labels\n",
    "    sampled_labels = np.random.randint(0, len(y_col), batch_size).reshape(-1, 1)\n",
    "\n",
    "    # Train the generator\n",
    "    g_loss = combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "    # Plot the progress\n",
    "    print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "    # If at save interval => save generated image samples\n",
    "    if epoch % sample_interval == 0:\n",
    "        sample_images(epoch, generator)\n",
    "        save_model(generator,discriminator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
