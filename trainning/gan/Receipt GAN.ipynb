{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get receipt dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "df_col = [\"sentence\",\"brand_name\",\"info\",\"index\",\"content\",\"total\",\"thank_you\"]\n",
    "y_col = [\"brand_name\",\"info\",\"index\",\"content\",\"total\",\"thank_you\"]\n",
    "train_df = pd.read_csv('../text_classification/31-07-vigroupped.csv',   encoding='utf-8')\n",
    "\n",
    "seed = 120\n",
    "np.random.seed(seed)\n",
    "train_df = shuffle(train_df)\n",
    "train_df.head()\n",
    "\n",
    "X_train = train_df[\"sentence\"].fillna(\"fillna\").values\n",
    "Y_train = train_df[['brand_name', 'info', 'index', 'content', 'total', 'thank_you']].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "texts = X_train\n",
    "\n",
    "tokenizer.fit_on_texts(texts) \n",
    "Tokenizer_vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "X_train_encoded_words = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "maxWordCount= 10\n",
    "maxDictionary_size=Tokenizer_vocab_size\n",
    "X_train_encoded_padded_words = sequence.pad_sequences(X_train_encoded_words, maxlen=maxWordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(644, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_encoded_padded_words.shape\n",
    "# Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index['ngá»«']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644\n"
     ]
    }
   ],
   "source": [
    "# Convert Y_train\n",
    "# range(0,len(y_col))[5]\n",
    "y_train = []\n",
    "for row in Y_train:\n",
    "    for index,col in enumerate(range(0,len(y_col))):\n",
    "        if row[col] == 1:\n",
    "            y_train.append(index)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "def padded_sequence_to_text(int_arr):\n",
    "    \n",
    "    padded_sequence = int_arr.reshape((maxWordCount))\n",
    "    padded_sequence = padded_sequence.tolist()\n",
    "#     print(padded_sequence)\n",
    "    started = False\n",
    "    word_seq = []\n",
    "    for word in padded_sequence:\n",
    "        if started:\n",
    "            word_seq.append(word)\n",
    "        else:\n",
    "            if word != 0:\n",
    "                started = True\n",
    "                word_seq.append(word)\n",
    "    \n",
    "    sentences = list(map(sequence_to_text, [word_seq]))\n",
    "    if len(sentences)>0:\n",
    "        my_texts = []\n",
    "        for word in sentences[0]:\n",
    "            if word:\n",
    "                my_texts.append(word)\n",
    "            \n",
    "        return ' '.join(my_texts)\n",
    "    return None\n",
    "\n",
    "def convert_y(y):\n",
    "    result = []\n",
    "    for index, col in enumerate(y_col):\n",
    "        if index == y:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "#     print(result)\n",
    "    return result\n",
    "\n",
    "def reshape_x_train(X_train_encoded_padded_words, r, c):\n",
    "    x_train = []\n",
    "    for row in X_train_encoded_padded_words:\n",
    "        aa = np.array(row)\n",
    "\n",
    "        aa = np.reshape(aa,(r ,c))\n",
    "#         print(aa)\n",
    "        x_train.append(aa)\n",
    "    return np.array(x_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self, r,c, token_vocal_size):\n",
    "        self.img_rows = r\n",
    "        self.img_cols = c\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        self.token_vocal_size = token_vocal_size\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(self.token_vocal_size, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(self.token_vocal_size*2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(self.token_vocal_size*4))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(self.token_vocal_size*2))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(self.token_vocal_size))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, X_train, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "#         (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        csvfile = 'cgan2d.csv'\n",
    "        c = 6\n",
    "#         self.img_cols = c\n",
    "        noise = np.random.normal(0, 1, (c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        gen_imgs = Tokenizer_vocab_size*gen_imgs\n",
    "        \n",
    "        \n",
    "        int_arr = np.array(gen_imgs, dtype='int')\n",
    "#         print(int_arr[0])\n",
    "        \n",
    "        \n",
    "#         print(len(int_arr[0,:,:,0]))\n",
    "#         fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "\n",
    "        for j in range(c):\n",
    "            sentence = padded_sequence_to_text(int_arr[cnt])\n",
    "#             result = convert_y(sampled_labels[cnt])\n",
    "            if len(sentence) <= 0:\n",
    "                continue\n",
    "            print(sentence)\n",
    "            cnt += 1\n",
    "            df = pd.read_csv(csvfile)# Loading a csv file with headers \n",
    "            data = {\n",
    "                'sentence':sentence,\n",
    "            }\n",
    "#             for index, col in enumerate(y_col):\n",
    "#                 data[col] = result[index]\n",
    "            df = df.append(data, ignore_index=True)\n",
    "            df.to_csv(csvfile, index = False,  encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 2392)              26312     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1196)              2862028   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 1196)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 1)                 1197      \n",
      "=================================================================\n",
      "Total params: 2,889,537\n",
      "Trainable params: 2,889,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 1196)              120796    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 1196)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 1196)              4784      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 2392)              2863224   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 2392)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 2392)              9568      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 4784)              11448112  \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 4784)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4784)              19136     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 10)                47850     \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 5, 2, 1)           0         \n",
      "=================================================================\n",
      "Total params: 14,513,470\n",
      "Trainable params: 14,496,726\n",
      "Non-trainable params: 16,744\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.736564, acc.: 7.81%] [G loss: 0.579863]\n",
      "398 joint 45 1989 4l lim 0 luc\n",
      "free Ä 6684 t 540000 66 phÃª vanila cÃ¡m sl\n",
      "náºµng 34 ngá»« dong sl cÃ¡ 35000 80 phÆ°á»ng\n",
      "prowct 1 2q tÆ°Æ¡i s1 thÃ¡i 1989 40 35\n",
      "cuá»n nhan 2015 lavie vÄ©nh 55 tÃªn náºµng 1 000\n",
      "bÃ³ gá»ng trÃ¢u total Æ¡n niá» muoi kin 6a\n",
      "1 [D loss: 0.646678, acc.: 50.00%] [G loss: 0.516297]\n",
      "2 [D loss: 0.605100, acc.: 54.69%] [G loss: 0.540616]\n",
      "3 [D loss: 0.553765, acc.: 57.81%] [G loss: 0.603300]\n",
      "4 [D loss: 0.595259, acc.: 56.25%] [G loss: 0.666787]\n",
      "5 [D loss: 0.567548, acc.: 70.31%] [G loss: 0.702229]\n",
      "6 [D loss: 0.540129, acc.: 68.75%] [G loss: 0.724338]\n",
      "7 [D loss: 0.513477, acc.: 73.44%] [G loss: 0.821398]\n",
      "8 [D loss: 0.546900, acc.: 76.56%] [G loss: 0.960608]\n",
      "9 [D loss: 0.517465, acc.: 71.88%] [G loss: 0.902964]\n",
      "10 [D loss: 0.495429, acc.: 81.25%] [G loss: 0.947359]\n",
      "11 [D loss: 0.506518, acc.: 78.12%] [G loss: 0.924685]\n",
      "12 [D loss: 0.534457, acc.: 79.69%] [G loss: 0.982970]\n",
      "13 [D loss: 0.512839, acc.: 79.69%] [G loss: 0.913076]\n",
      "14 [D loss: 0.543617, acc.: 71.88%] [G loss: 0.996298]\n",
      "15 [D loss: 0.498418, acc.: 75.00%] [G loss: 1.025177]\n",
      "16 [D loss: 0.410317, acc.: 84.38%] [G loss: 1.088714]\n",
      "17 [D loss: 0.520644, acc.: 76.56%] [G loss: 1.129914]\n",
      "18 [D loss: 0.530347, acc.: 68.75%] [G loss: 1.302223]\n",
      "19 [D loss: 0.447326, acc.: 81.25%] [G loss: 1.153407]\n",
      "20 [D loss: 0.676828, acc.: 71.88%] [G loss: 1.079313]\n",
      "21 [D loss: 0.570587, acc.: 79.69%] [G loss: 1.123679]\n",
      "22 [D loss: 0.552547, acc.: 75.00%] [G loss: 1.021953]\n",
      "23 [D loss: 0.439817, acc.: 81.25%] [G loss: 0.994148]\n",
      "24 [D loss: 0.439559, acc.: 79.69%] [G loss: 1.245891]\n",
      "25 [D loss: 0.537452, acc.: 76.56%] [G loss: 1.254614]\n",
      "26 [D loss: 0.483921, acc.: 71.88%] [G loss: 1.161328]\n",
      "27 [D loss: 0.492438, acc.: 76.56%] [G loss: 1.414760]\n",
      "28 [D loss: 0.460051, acc.: 75.00%] [G loss: 1.194610]\n",
      "29 [D loss: 0.466091, acc.: 76.56%] [G loss: 1.266062]\n",
      "30 [D loss: 0.528467, acc.: 65.62%] [G loss: 1.278796]\n",
      "31 [D loss: 0.440736, acc.: 75.00%] [G loss: 1.415703]\n",
      "32 [D loss: 0.417872, acc.: 76.56%] [G loss: 1.254433]\n",
      "33 [D loss: 0.466859, acc.: 82.81%] [G loss: 1.260872]\n",
      "34 [D loss: 0.504377, acc.: 76.56%] [G loss: 1.423530]\n",
      "35 [D loss: 0.406908, acc.: 78.12%] [G loss: 1.359009]\n",
      "36 [D loss: 0.386959, acc.: 81.25%] [G loss: 1.357689]\n",
      "37 [D loss: 0.643954, acc.: 70.31%] [G loss: 1.433445]\n",
      "38 [D loss: 0.430508, acc.: 75.00%] [G loss: 1.351711]\n",
      "39 [D loss: 0.506498, acc.: 70.31%] [G loss: 1.251112]\n",
      "40 [D loss: 0.448453, acc.: 79.69%] [G loss: 1.193919]\n",
      "41 [D loss: 0.491524, acc.: 76.56%] [G loss: 1.086207]\n",
      "42 [D loss: 0.509483, acc.: 75.00%] [G loss: 1.359518]\n",
      "43 [D loss: 0.438031, acc.: 75.00%] [G loss: 1.382720]\n",
      "44 [D loss: 0.462802, acc.: 78.12%] [G loss: 1.320829]\n",
      "45 [D loss: 0.537915, acc.: 71.88%] [G loss: 1.415044]\n",
      "46 [D loss: 0.348456, acc.: 82.81%] [G loss: 1.641876]\n",
      "47 [D loss: 0.421958, acc.: 75.00%] [G loss: 1.500177]\n",
      "48 [D loss: 0.493298, acc.: 68.75%] [G loss: 1.812090]\n",
      "49 [D loss: 0.454213, acc.: 71.88%] [G loss: 1.422918]\n",
      "50 [D loss: 0.408255, acc.: 65.62%] [G loss: 1.547572]\n",
      "51 [D loss: 0.402810, acc.: 78.12%] [G loss: 1.770543]\n",
      "52 [D loss: 0.414503, acc.: 78.12%] [G loss: 1.500697]\n",
      "53 [D loss: 0.356431, acc.: 82.81%] [G loss: 1.404343]\n",
      "54 [D loss: 0.474930, acc.: 70.31%] [G loss: 1.294177]\n",
      "55 [D loss: 0.381622, acc.: 79.69%] [G loss: 1.475869]\n",
      "56 [D loss: 0.425241, acc.: 75.00%] [G loss: 1.702557]\n",
      "57 [D loss: 0.450171, acc.: 71.88%] [G loss: 1.625832]\n",
      "58 [D loss: 0.372389, acc.: 78.12%] [G loss: 1.672900]\n",
      "59 [D loss: 0.377137, acc.: 79.69%] [G loss: 1.524677]\n",
      "60 [D loss: 0.449252, acc.: 76.56%] [G loss: 1.534925]\n",
      "61 [D loss: 0.356540, acc.: 87.50%] [G loss: 1.711617]\n",
      "62 [D loss: 0.383459, acc.: 87.50%] [G loss: 1.672453]\n",
      "63 [D loss: 0.470457, acc.: 75.00%] [G loss: 1.512964]\n",
      "64 [D loss: 0.398360, acc.: 75.00%] [G loss: 1.780233]\n",
      "65 [D loss: 0.460800, acc.: 71.88%] [G loss: 1.483394]\n",
      "66 [D loss: 0.320664, acc.: 82.81%] [G loss: 1.710360]\n",
      "67 [D loss: 0.418445, acc.: 78.12%] [G loss: 1.625373]\n",
      "68 [D loss: 0.398560, acc.: 78.12%] [G loss: 1.640401]\n",
      "69 [D loss: 0.290187, acc.: 85.94%] [G loss: 1.584078]\n",
      "70 [D loss: 0.363477, acc.: 78.12%] [G loss: 1.871899]\n",
      "71 [D loss: 0.381728, acc.: 81.25%] [G loss: 1.866275]\n",
      "72 [D loss: 0.389654, acc.: 76.56%] [G loss: 1.810184]\n",
      "73 [D loss: 0.388768, acc.: 82.81%] [G loss: 1.592396]\n",
      "74 [D loss: 0.504790, acc.: 70.31%] [G loss: 1.596083]\n",
      "75 [D loss: 0.416483, acc.: 81.25%] [G loss: 1.538590]\n",
      "76 [D loss: 0.588640, acc.: 53.12%] [G loss: 1.726948]\n",
      "77 [D loss: 0.424979, acc.: 73.44%] [G loss: 1.487469]\n",
      "78 [D loss: 0.386335, acc.: 79.69%] [G loss: 1.620589]\n",
      "79 [D loss: 0.436433, acc.: 75.00%] [G loss: 1.551715]\n",
      "80 [D loss: 0.441955, acc.: 70.31%] [G loss: 1.774032]\n",
      "81 [D loss: 0.371841, acc.: 78.12%] [G loss: 1.542053]\n",
      "82 [D loss: 0.443112, acc.: 73.44%] [G loss: 1.562288]\n",
      "83 [D loss: 0.404807, acc.: 76.56%] [G loss: 1.943337]\n",
      "84 [D loss: 0.465903, acc.: 73.44%] [G loss: 1.945094]\n",
      "85 [D loss: 0.519935, acc.: 68.75%] [G loss: 1.624828]\n",
      "86 [D loss: 0.408996, acc.: 76.56%] [G loss: 1.824627]\n",
      "87 [D loss: 0.409916, acc.: 85.94%] [G loss: 1.429637]\n",
      "88 [D loss: 0.438501, acc.: 75.00%] [G loss: 1.483228]\n",
      "89 [D loss: 0.451435, acc.: 75.00%] [G loss: 1.592692]\n",
      "90 [D loss: 0.361707, acc.: 78.12%] [G loss: 1.736095]\n",
      "91 [D loss: 0.389369, acc.: 82.81%] [G loss: 1.774153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 [D loss: 0.513512, acc.: 64.06%] [G loss: 1.557312]\n",
      "93 [D loss: 0.349081, acc.: 87.50%] [G loss: 1.924021]\n",
      "94 [D loss: 0.507735, acc.: 76.56%] [G loss: 1.468247]\n",
      "95 [D loss: 0.533893, acc.: 68.75%] [G loss: 1.510409]\n",
      "96 [D loss: 0.459330, acc.: 78.12%] [G loss: 1.578167]\n",
      "97 [D loss: 0.307929, acc.: 92.19%] [G loss: 1.561700]\n",
      "98 [D loss: 0.349282, acc.: 87.50%] [G loss: 1.421267]\n",
      "99 [D loss: 0.291824, acc.: 89.06%] [G loss: 1.836307]\n",
      "100 [D loss: 0.434606, acc.: 75.00%] [G loss: 1.581208]\n",
      "tÃ´m ngá»« ngá»« ngá»« vá» ngÃ n\n",
      "0\n",
      "5 ngá»« cuá»n ngá»« Äáº¡u ngá»«\n",
      "tin\n",
      "ngá»« ngá»« ngá»« ngá»« ngá»« ngá»«\n",
      "ngá»« 0 ngá»«\n",
      "101 [D loss: 0.392048, acc.: 82.81%] [G loss: 1.737487]\n",
      "102 [D loss: 0.429860, acc.: 78.12%] [G loss: 1.670451]\n",
      "103 [D loss: 0.336022, acc.: 81.25%] [G loss: 1.883909]\n",
      "104 [D loss: 0.584530, acc.: 78.12%] [G loss: 1.495946]\n",
      "105 [D loss: 0.380465, acc.: 82.81%] [G loss: 1.517966]\n",
      "106 [D loss: 0.332591, acc.: 82.81%] [G loss: 1.744921]\n",
      "107 [D loss: 0.378664, acc.: 82.81%] [G loss: 1.810818]\n",
      "108 [D loss: 0.360261, acc.: 89.06%] [G loss: 1.387476]\n",
      "109 [D loss: 0.341557, acc.: 89.06%] [G loss: 1.531181]\n",
      "110 [D loss: 0.281538, acc.: 90.62%] [G loss: 1.451780]\n",
      "111 [D loss: 0.293031, acc.: 92.19%] [G loss: 1.552701]\n",
      "112 [D loss: 0.385516, acc.: 81.25%] [G loss: 1.627140]\n",
      "113 [D loss: 0.345252, acc.: 90.62%] [G loss: 1.609547]\n",
      "114 [D loss: 0.382708, acc.: 82.81%] [G loss: 1.523668]\n",
      "115 [D loss: 0.430453, acc.: 81.25%] [G loss: 1.635927]\n",
      "116 [D loss: 0.411286, acc.: 79.69%] [G loss: 1.674419]\n",
      "117 [D loss: 0.336195, acc.: 85.94%] [G loss: 1.510197]\n",
      "118 [D loss: 0.635216, acc.: 90.62%] [G loss: 1.086070]\n",
      "119 [D loss: 0.371606, acc.: 89.06%] [G loss: 1.382668]\n",
      "120 [D loss: 0.332790, acc.: 85.94%] [G loss: 1.398546]\n",
      "121 [D loss: 0.352411, acc.: 84.38%] [G loss: 1.628381]\n",
      "122 [D loss: 0.444468, acc.: 82.81%] [G loss: 1.701858]\n",
      "123 [D loss: 0.490423, acc.: 68.75%] [G loss: 1.387241]\n",
      "124 [D loss: 0.403113, acc.: 81.25%] [G loss: 1.415014]\n",
      "125 [D loss: 0.423729, acc.: 70.31%] [G loss: 1.583961]\n",
      "126 [D loss: 0.500373, acc.: 71.88%] [G loss: 1.563169]\n",
      "127 [D loss: 0.446081, acc.: 79.69%] [G loss: 1.610973]\n",
      "128 [D loss: 0.363778, acc.: 81.25%] [G loss: 1.575033]\n",
      "129 [D loss: 0.622644, acc.: 78.12%] [G loss: 1.505052]\n",
      "130 [D loss: 0.486177, acc.: 76.56%] [G loss: 1.338701]\n",
      "131 [D loss: 0.488992, acc.: 89.06%] [G loss: 1.302892]\n",
      "132 [D loss: 0.397050, acc.: 79.69%] [G loss: 1.375634]\n",
      "133 [D loss: 0.579720, acc.: 78.12%] [G loss: 1.205775]\n",
      "134 [D loss: 0.389511, acc.: 76.56%] [G loss: 1.149176]\n",
      "135 [D loss: 0.431905, acc.: 75.00%] [G loss: 1.543109]\n",
      "136 [D loss: 0.381104, acc.: 82.81%] [G loss: 1.484965]\n",
      "137 [D loss: 0.362873, acc.: 81.25%] [G loss: 1.643276]\n",
      "138 [D loss: 0.398143, acc.: 79.69%] [G loss: 1.531983]\n",
      "139 [D loss: 0.386170, acc.: 71.88%] [G loss: 1.532532]\n",
      "140 [D loss: 0.351480, acc.: 78.12%] [G loss: 1.938779]\n",
      "141 [D loss: 0.315257, acc.: 81.25%] [G loss: 1.886617]\n",
      "142 [D loss: 0.421071, acc.: 70.31%] [G loss: 1.987164]\n",
      "143 [D loss: 0.315798, acc.: 84.38%] [G loss: 1.629742]\n",
      "144 [D loss: 0.412101, acc.: 75.00%] [G loss: 1.502468]\n",
      "145 [D loss: 0.386864, acc.: 75.00%] [G loss: 1.646358]\n",
      "146 [D loss: 0.527280, acc.: 76.56%] [G loss: 1.395913]\n",
      "147 [D loss: 0.386843, acc.: 81.25%] [G loss: 1.292289]\n",
      "148 [D loss: 0.424919, acc.: 73.44%] [G loss: 1.529279]\n",
      "149 [D loss: 0.367909, acc.: 75.00%] [G loss: 1.575957]\n",
      "150 [D loss: 0.455987, acc.: 68.75%] [G loss: 1.606811]\n",
      "ngá»«\n",
      "4629719 ngá»« and ngá»«\n",
      "sr 000 ngá»«\n",
      "2 ngá»« ngá»« ngá»«\n",
      "Äá»ng ngá»« 20 thÃ nh tráº¯ng\n",
      "ngá»« v close ngá»« ngá»«\n",
      "151 [D loss: 0.399453, acc.: 81.25%] [G loss: 1.543721]\n",
      "152 [D loss: 0.369325, acc.: 78.12%] [G loss: 1.785215]\n",
      "153 [D loss: 0.448247, acc.: 71.88%] [G loss: 1.693627]\n",
      "154 [D loss: 0.467239, acc.: 70.31%] [G loss: 1.691496]\n",
      "155 [D loss: 0.484563, acc.: 76.56%] [G loss: 1.638265]\n",
      "156 [D loss: 0.277342, acc.: 89.06%] [G loss: 1.708354]\n",
      "157 [D loss: 0.314489, acc.: 89.06%] [G loss: 1.748101]\n",
      "158 [D loss: 0.354835, acc.: 87.50%] [G loss: 1.392833]\n",
      "159 [D loss: 0.351124, acc.: 84.38%] [G loss: 1.566668]\n",
      "160 [D loss: 0.306519, acc.: 90.62%] [G loss: 1.686669]\n",
      "161 [D loss: 0.282174, acc.: 90.62%] [G loss: 1.797081]\n",
      "162 [D loss: 0.312755, acc.: 90.62%] [G loss: 1.992952]\n",
      "163 [D loss: 0.349482, acc.: 85.94%] [G loss: 1.809368]\n",
      "164 [D loss: 0.260542, acc.: 92.19%] [G loss: 1.802897]\n",
      "165 [D loss: 0.317062, acc.: 92.19%] [G loss: 1.725049]\n",
      "166 [D loss: 0.364373, acc.: 85.94%] [G loss: 1.766002]\n",
      "167 [D loss: 0.319496, acc.: 84.38%] [G loss: 1.854498]\n",
      "168 [D loss: 0.208805, acc.: 95.31%] [G loss: 2.059872]\n",
      "169 [D loss: 0.283547, acc.: 92.19%] [G loss: 2.257267]\n",
      "170 [D loss: 0.307104, acc.: 89.06%] [G loss: 1.951283]\n",
      "171 [D loss: 0.350347, acc.: 87.50%] [G loss: 1.939506]\n",
      "172 [D loss: 0.376448, acc.: 79.69%] [G loss: 2.084798]\n",
      "173 [D loss: 0.342646, acc.: 82.81%] [G loss: 2.120062]\n",
      "174 [D loss: 0.343089, acc.: 90.62%] [G loss: 1.928433]\n",
      "175 [D loss: 0.351085, acc.: 78.12%] [G loss: 1.830314]\n",
      "176 [D loss: 0.351320, acc.: 79.69%] [G loss: 1.907592]\n",
      "177 [D loss: 0.248923, acc.: 93.75%] [G loss: 2.016128]\n",
      "178 [D loss: 0.402982, acc.: 73.44%] [G loss: 1.769164]\n",
      "179 [D loss: 0.348580, acc.: 76.56%] [G loss: 1.924379]\n",
      "180 [D loss: 0.328455, acc.: 84.38%] [G loss: 2.288501]\n",
      "181 [D loss: 0.301669, acc.: 89.06%] [G loss: 2.170176]\n",
      "182 [D loss: 0.381960, acc.: 81.25%] [G loss: 2.206730]\n",
      "183 [D loss: 0.414661, acc.: 76.56%] [G loss: 1.868610]\n",
      "184 [D loss: 0.407694, acc.: 78.12%] [G loss: 1.962021]\n",
      "185 [D loss: 0.237813, acc.: 93.75%] [G loss: 2.117340]\n",
      "186 [D loss: 0.535309, acc.: 75.00%] [G loss: 2.171531]\n",
      "187 [D loss: 0.350372, acc.: 85.94%] [G loss: 1.827784]\n",
      "188 [D loss: 0.291662, acc.: 87.50%] [G loss: 1.825063]\n",
      "189 [D loss: 0.324274, acc.: 84.38%] [G loss: 1.943215]\n",
      "190 [D loss: 0.318741, acc.: 82.81%] [G loss: 1.935661]\n",
      "191 [D loss: 0.323790, acc.: 79.69%] [G loss: 1.966222]\n",
      "192 [D loss: 0.351993, acc.: 89.06%] [G loss: 1.773921]\n",
      "193 [D loss: 0.335086, acc.: 84.38%] [G loss: 1.823944]\n",
      "194 [D loss: 0.284893, acc.: 87.50%] [G loss: 2.183560]\n",
      "195 [D loss: 0.275676, acc.: 89.06%] [G loss: 2.033313]\n",
      "196 [D loss: 0.336423, acc.: 84.38%] [G loss: 1.753842]\n",
      "197 [D loss: 0.314721, acc.: 92.19%] [G loss: 1.770227]\n",
      "198 [D loss: 0.345811, acc.: 81.25%] [G loss: 1.915432]\n",
      "199 [D loss: 0.316927, acc.: 85.94%] [G loss: 2.119930]\n",
      "200 [D loss: 0.355316, acc.: 79.69%] [G loss: 1.969149]\n",
      "ngá»«\n",
      "1989 nguyá»n ngá»« ngá»«\n",
      "201 [D loss: 0.308491, acc.: 85.94%] [G loss: 1.996983]\n",
      "202 [D loss: 0.219536, acc.: 95.31%] [G loss: 2.017952]\n",
      "203 [D loss: 0.315958, acc.: 87.50%] [G loss: 1.907973]\n",
      "204 [D loss: 0.303182, acc.: 90.62%] [G loss: 1.988945]\n",
      "205 [D loss: 0.411141, acc.: 84.38%] [G loss: 2.009216]\n",
      "206 [D loss: 0.336111, acc.: 85.94%] [G loss: 1.922195]\n",
      "207 [D loss: 0.318710, acc.: 85.94%] [G loss: 2.205636]\n",
      "208 [D loss: 0.328374, acc.: 84.38%] [G loss: 2.263152]\n",
      "209 [D loss: 0.531366, acc.: 90.62%] [G loss: 1.778570]\n",
      "210 [D loss: 0.399438, acc.: 78.12%] [G loss: 1.659923]\n",
      "211 [D loss: 0.347716, acc.: 79.69%] [G loss: 1.528953]\n",
      "212 [D loss: 0.405776, acc.: 84.38%] [G loss: 1.565297]\n",
      "213 [D loss: 0.364043, acc.: 87.50%] [G loss: 1.605237]\n",
      "214 [D loss: 0.361235, acc.: 84.38%] [G loss: 1.753392]\n",
      "215 [D loss: 0.486145, acc.: 67.19%] [G loss: 1.766327]\n",
      "216 [D loss: 0.319483, acc.: 87.50%] [G loss: 1.791736]\n",
      "217 [D loss: 0.389708, acc.: 78.12%] [G loss: 1.692435]\n",
      "218 [D loss: 0.306460, acc.: 90.62%] [G loss: 1.865744]\n",
      "219 [D loss: 0.359050, acc.: 81.25%] [G loss: 2.041908]\n",
      "220 [D loss: 0.366135, acc.: 78.12%] [G loss: 2.072808]\n",
      "221 [D loss: 0.345805, acc.: 81.25%] [G loss: 1.883979]\n",
      "222 [D loss: 0.271680, acc.: 90.62%] [G loss: 1.919947]\n",
      "223 [D loss: 0.454234, acc.: 73.44%] [G loss: 2.061796]\n",
      "224 [D loss: 0.453825, acc.: 79.69%] [G loss: 1.798329]\n",
      "225 [D loss: 0.350993, acc.: 79.69%] [G loss: 1.943035]\n",
      "226 [D loss: 0.358412, acc.: 85.94%] [G loss: 1.864503]\n",
      "227 [D loss: 0.356185, acc.: 89.06%] [G loss: 1.606783]\n",
      "228 [D loss: 0.245220, acc.: 92.19%] [G loss: 1.844964]\n",
      "229 [D loss: 0.322935, acc.: 85.94%] [G loss: 1.833896]\n",
      "230 [D loss: 0.430217, acc.: 76.56%] [G loss: 2.100318]\n",
      "231 [D loss: 0.376691, acc.: 85.94%] [G loss: 1.838279]\n",
      "232 [D loss: 0.339536, acc.: 87.50%] [G loss: 1.980470]\n",
      "233 [D loss: 0.301514, acc.: 87.50%] [G loss: 2.129865]\n",
      "234 [D loss: 0.383276, acc.: 76.56%] [G loss: 2.414181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235 [D loss: 0.361190, acc.: 82.81%] [G loss: 2.139299]\n",
      "236 [D loss: 0.371368, acc.: 79.69%] [G loss: 1.890949]\n",
      "237 [D loss: 0.442867, acc.: 73.44%] [G loss: 1.834191]\n",
      "238 [D loss: 0.401794, acc.: 78.12%] [G loss: 1.841414]\n",
      "239 [D loss: 0.370381, acc.: 78.12%] [G loss: 1.845249]\n",
      "240 [D loss: 0.336451, acc.: 82.81%] [G loss: 1.859740]\n",
      "241 [D loss: 0.337714, acc.: 82.81%] [G loss: 2.071570]\n",
      "242 [D loss: 0.256608, acc.: 90.62%] [G loss: 2.242397]\n",
      "243 [D loss: 0.415657, acc.: 73.44%] [G loss: 2.131674]\n",
      "244 [D loss: 0.329112, acc.: 90.62%] [G loss: 1.749195]\n",
      "245 [D loss: 0.342945, acc.: 85.94%] [G loss: 1.544173]\n",
      "246 [D loss: 0.396079, acc.: 82.81%] [G loss: 1.763821]\n",
      "247 [D loss: 0.372587, acc.: 79.69%] [G loss: 1.999385]\n",
      "248 [D loss: 0.248116, acc.: 93.75%] [G loss: 1.978695]\n",
      "249 [D loss: 0.349782, acc.: 84.38%] [G loss: 2.076462]\n",
      "250 [D loss: 0.326590, acc.: 87.50%] [G loss: 2.288359]\n",
      "ngá»« 000 phiáº¿u\n",
      "251 [D loss: 0.235474, acc.: 93.75%] [G loss: 2.225498]\n",
      "252 [D loss: 0.312176, acc.: 82.81%] [G loss: 2.086239]\n",
      "253 [D loss: 0.258320, acc.: 89.06%] [G loss: 1.998348]\n",
      "254 [D loss: 0.303098, acc.: 90.62%] [G loss: 2.124202]\n",
      "255 [D loss: 0.284275, acc.: 87.50%] [G loss: 2.106106]\n",
      "256 [D loss: 0.373192, acc.: 87.50%] [G loss: 2.011469]\n",
      "257 [D loss: 0.253595, acc.: 90.62%] [G loss: 2.110732]\n",
      "258 [D loss: 0.247869, acc.: 85.94%] [G loss: 1.899259]\n",
      "259 [D loss: 0.284335, acc.: 95.31%] [G loss: 2.077798]\n",
      "260 [D loss: 0.267641, acc.: 93.75%] [G loss: 2.213590]\n",
      "261 [D loss: 0.304343, acc.: 85.94%] [G loss: 2.293407]\n",
      "262 [D loss: 0.226905, acc.: 95.31%] [G loss: 2.177972]\n",
      "263 [D loss: 0.208025, acc.: 95.31%] [G loss: 2.384943]\n",
      "264 [D loss: 0.191658, acc.: 98.44%] [G loss: 2.469480]\n",
      "265 [D loss: 0.293561, acc.: 90.62%] [G loss: 2.268788]\n",
      "266 [D loss: 0.272909, acc.: 92.19%] [G loss: 2.285057]\n",
      "267 [D loss: 0.183494, acc.: 95.31%] [G loss: 2.509455]\n",
      "268 [D loss: 0.241977, acc.: 92.19%] [G loss: 2.391813]\n",
      "269 [D loss: 0.271987, acc.: 93.75%] [G loss: 1.786718]\n",
      "270 [D loss: 0.175849, acc.: 96.88%] [G loss: 1.939075]\n",
      "271 [D loss: 0.247568, acc.: 92.19%] [G loss: 2.223176]\n",
      "272 [D loss: 0.162937, acc.: 95.31%] [G loss: 2.359097]\n",
      "273 [D loss: 0.201468, acc.: 93.75%] [G loss: 2.421322]\n",
      "274 [D loss: 0.280387, acc.: 89.06%] [G loss: 2.395146]\n",
      "275 [D loss: 0.237427, acc.: 93.75%] [G loss: 1.983619]\n",
      "276 [D loss: 0.261681, acc.: 92.19%] [G loss: 2.411977]\n",
      "277 [D loss: 0.209075, acc.: 95.31%] [G loss: 2.402270]\n",
      "278 [D loss: 0.196024, acc.: 93.75%] [G loss: 2.476346]\n",
      "279 [D loss: 0.235922, acc.: 92.19%] [G loss: 2.483926]\n",
      "280 [D loss: 0.366751, acc.: 84.38%] [G loss: 2.110453]\n",
      "281 [D loss: 0.261243, acc.: 89.06%] [G loss: 2.114258]\n",
      "282 [D loss: 0.217205, acc.: 92.19%] [G loss: 2.470446]\n",
      "283 [D loss: 0.444751, acc.: 79.69%] [G loss: 2.030511]\n",
      "284 [D loss: 0.291652, acc.: 84.38%] [G loss: 1.709460]\n",
      "285 [D loss: 0.292217, acc.: 85.94%] [G loss: 1.968359]\n",
      "286 [D loss: 0.308955, acc.: 87.50%] [G loss: 2.147167]\n",
      "287 [D loss: 0.453349, acc.: 65.62%] [G loss: 1.876739]\n",
      "288 [D loss: 0.354229, acc.: 84.38%] [G loss: 1.706939]\n",
      "289 [D loss: 0.303489, acc.: 90.62%] [G loss: 1.969617]\n",
      "290 [D loss: 0.485067, acc.: 79.69%] [G loss: 1.840186]\n",
      "291 [D loss: 0.326308, acc.: 85.94%] [G loss: 2.004568]\n",
      "292 [D loss: 0.471812, acc.: 70.31%] [G loss: 1.707052]\n",
      "293 [D loss: 0.370792, acc.: 78.12%] [G loss: 1.846782]\n",
      "294 [D loss: 0.283830, acc.: 85.94%] [G loss: 2.173821]\n",
      "295 [D loss: 0.251251, acc.: 84.38%] [G loss: 1.944006]\n",
      "296 [D loss: 0.450241, acc.: 76.56%] [G loss: 2.091951]\n",
      "297 [D loss: 0.250481, acc.: 92.19%] [G loss: 1.975111]\n",
      "298 [D loss: 0.388809, acc.: 82.81%] [G loss: 2.030016]\n",
      "299 [D loss: 0.341424, acc.: 84.38%] [G loss: 2.175496]\n",
      "300 [D loss: 0.271662, acc.: 95.31%] [G loss: 2.623875]\n",
      "2bÃ°ggq ngá»« ngá»«\n",
      "da tÃ¢m giÃ¡ 1x\n",
      "301 [D loss: 0.227267, acc.: 92.19%] [G loss: 2.073030]\n",
      "302 [D loss: 0.344232, acc.: 82.81%] [G loss: 2.394572]\n",
      "303 [D loss: 0.252020, acc.: 89.06%] [G loss: 2.324884]\n",
      "304 [D loss: 0.215749, acc.: 92.19%] [G loss: 2.453212]\n",
      "305 [D loss: 0.408125, acc.: 71.88%] [G loss: 2.411727]\n",
      "306 [D loss: 0.364807, acc.: 82.81%] [G loss: 2.094532]\n",
      "307 [D loss: 0.363409, acc.: 79.69%] [G loss: 1.934973]\n",
      "308 [D loss: 0.322708, acc.: 84.38%] [G loss: 1.956818]\n",
      "309 [D loss: 0.348543, acc.: 85.94%] [G loss: 2.308384]\n",
      "310 [D loss: 0.287517, acc.: 89.06%] [G loss: 2.117708]\n",
      "311 [D loss: 0.288553, acc.: 87.50%] [G loss: 2.262324]\n",
      "312 [D loss: 0.374490, acc.: 79.69%] [G loss: 2.205609]\n",
      "313 [D loss: 0.388947, acc.: 84.38%] [G loss: 2.017200]\n",
      "314 [D loss: 0.315632, acc.: 85.94%] [G loss: 2.219295]\n",
      "315 [D loss: 0.430955, acc.: 73.44%] [G loss: 2.066257]\n",
      "316 [D loss: 0.312260, acc.: 85.94%] [G loss: 2.040087]\n",
      "317 [D loss: 0.411486, acc.: 78.12%] [G loss: 2.001197]\n",
      "318 [D loss: 0.320780, acc.: 85.94%] [G loss: 2.011655]\n",
      "319 [D loss: 0.375295, acc.: 85.94%] [G loss: 1.697537]\n",
      "320 [D loss: 0.354827, acc.: 85.94%] [G loss: 1.827502]\n",
      "321 [D loss: 0.335249, acc.: 82.81%] [G loss: 1.892395]\n",
      "322 [D loss: 0.430402, acc.: 75.00%] [G loss: 2.025970]\n",
      "323 [D loss: 0.256045, acc.: 93.75%] [G loss: 2.028987]\n",
      "324 [D loss: 0.333437, acc.: 82.81%] [G loss: 2.008424]\n",
      "325 [D loss: 0.261127, acc.: 90.62%] [G loss: 2.221949]\n",
      "326 [D loss: 0.293196, acc.: 87.50%] [G loss: 2.143088]\n",
      "327 [D loss: 0.285711, acc.: 87.50%] [G loss: 2.073789]\n",
      "328 [D loss: 0.281892, acc.: 82.81%] [G loss: 2.221590]\n",
      "329 [D loss: 0.330868, acc.: 84.38%] [G loss: 1.950486]\n",
      "330 [D loss: 0.223755, acc.: 90.62%] [G loss: 1.817806]\n",
      "331 [D loss: 0.307057, acc.: 87.50%] [G loss: 2.184250]\n",
      "332 [D loss: 0.355401, acc.: 82.81%] [G loss: 2.099090]\n",
      "333 [D loss: 0.281944, acc.: 82.81%] [G loss: 2.125721]\n",
      "334 [D loss: 0.288795, acc.: 87.50%] [G loss: 2.297912]\n",
      "335 [D loss: 0.256956, acc.: 92.19%] [G loss: 2.250957]\n",
      "336 [D loss: 0.299040, acc.: 84.38%] [G loss: 2.157642]\n",
      "337 [D loss: 0.329893, acc.: 81.25%] [G loss: 2.078050]\n",
      "338 [D loss: 0.489524, acc.: 76.56%] [G loss: 2.046822]\n",
      "339 [D loss: 0.320754, acc.: 82.81%] [G loss: 2.304646]\n",
      "340 [D loss: 0.352383, acc.: 79.69%] [G loss: 2.262372]\n",
      "341 [D loss: 0.357797, acc.: 82.81%] [G loss: 2.079480]\n",
      "342 [D loss: 0.331086, acc.: 79.69%] [G loss: 1.846403]\n",
      "343 [D loss: 0.467288, acc.: 75.00%] [G loss: 2.217760]\n",
      "344 [D loss: 0.294864, acc.: 87.50%] [G loss: 2.087687]\n",
      "345 [D loss: 0.339106, acc.: 87.50%] [G loss: 2.436501]\n",
      "346 [D loss: 0.175296, acc.: 93.75%] [G loss: 2.217330]\n",
      "347 [D loss: 0.320191, acc.: 82.81%] [G loss: 2.232578]\n",
      "348 [D loss: 0.262887, acc.: 89.06%] [G loss: 2.511552]\n",
      "349 [D loss: 0.204460, acc.: 96.88%] [G loss: 2.242404]\n",
      "350 [D loss: 0.260605, acc.: 89.06%] [G loss: 2.551771]\n",
      "cuá»n\n",
      "2 300\n",
      "hÃ ng\n",
      "giÃ¡\n",
      "ngá»« 540000\n",
      "khÃ¡ch\n",
      "351 [D loss: 0.356727, acc.: 85.94%] [G loss: 2.391018]\n",
      "352 [D loss: 0.273064, acc.: 90.62%] [G loss: 2.513032]\n",
      "353 [D loss: 0.364312, acc.: 78.12%] [G loss: 2.380328]\n",
      "354 [D loss: 0.259522, acc.: 87.50%] [G loss: 2.339056]\n",
      "355 [D loss: 0.319390, acc.: 85.94%] [G loss: 2.063874]\n",
      "356 [D loss: 0.272284, acc.: 85.94%] [G loss: 2.305308]\n",
      "357 [D loss: 0.305969, acc.: 87.50%] [G loss: 2.245782]\n",
      "358 [D loss: 0.295211, acc.: 85.94%] [G loss: 2.235878]\n",
      "359 [D loss: 0.347297, acc.: 82.81%] [G loss: 2.151587]\n",
      "360 [D loss: 0.390783, acc.: 84.38%] [G loss: 2.201974]\n",
      "361 [D loss: 0.292524, acc.: 89.06%] [G loss: 2.409306]\n",
      "362 [D loss: 0.351638, acc.: 89.06%] [G loss: 2.016642]\n",
      "363 [D loss: 0.351368, acc.: 81.25%] [G loss: 2.095103]\n",
      "364 [D loss: 0.258098, acc.: 90.62%] [G loss: 2.211077]\n",
      "365 [D loss: 0.402264, acc.: 78.12%] [G loss: 2.009093]\n",
      "366 [D loss: 0.306380, acc.: 87.50%] [G loss: 1.888530]\n",
      "367 [D loss: 0.422561, acc.: 76.56%] [G loss: 2.102276]\n",
      "368 [D loss: 0.396317, acc.: 79.69%] [G loss: 2.080525]\n",
      "369 [D loss: 0.339495, acc.: 79.69%] [G loss: 2.207376]\n",
      "370 [D loss: 0.208342, acc.: 93.75%] [G loss: 2.330582]\n",
      "371 [D loss: 0.296929, acc.: 84.38%] [G loss: 2.287724]\n",
      "372 [D loss: 0.357403, acc.: 84.38%] [G loss: 1.951363]\n",
      "373 [D loss: 0.282296, acc.: 87.50%] [G loss: 1.939504]\n",
      "374 [D loss: 0.358196, acc.: 85.94%] [G loss: 1.969418]\n",
      "375 [D loss: 0.274225, acc.: 85.94%] [G loss: 1.831991]\n",
      "376 [D loss: 0.297936, acc.: 85.94%] [G loss: 2.090347]\n",
      "377 [D loss: 0.400373, acc.: 82.81%] [G loss: 2.072190]\n",
      "378 [D loss: 0.298765, acc.: 87.50%] [G loss: 2.132977]\n",
      "379 [D loss: 0.285968, acc.: 87.50%] [G loss: 2.258456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380 [D loss: 0.312346, acc.: 85.94%] [G loss: 2.266357]\n",
      "381 [D loss: 0.364495, acc.: 79.69%] [G loss: 1.953483]\n",
      "382 [D loss: 0.286255, acc.: 84.38%] [G loss: 2.192087]\n",
      "383 [D loss: 0.247929, acc.: 90.62%] [G loss: 2.134801]\n",
      "384 [D loss: 0.289771, acc.: 87.50%] [G loss: 2.013503]\n",
      "385 [D loss: 0.235784, acc.: 93.75%] [G loss: 2.355342]\n",
      "386 [D loss: 0.335690, acc.: 84.38%] [G loss: 2.297014]\n",
      "387 [D loss: 0.220987, acc.: 92.19%] [G loss: 2.507144]\n",
      "388 [D loss: 0.261318, acc.: 90.62%] [G loss: 2.674321]\n",
      "389 [D loss: 0.305058, acc.: 85.94%] [G loss: 2.475689]\n",
      "390 [D loss: 0.291231, acc.: 87.50%] [G loss: 2.332996]\n",
      "391 [D loss: 0.310568, acc.: 90.62%] [G loss: 2.246202]\n",
      "392 [D loss: 0.368567, acc.: 82.81%] [G loss: 2.174954]\n",
      "393 [D loss: 0.286234, acc.: 87.50%] [G loss: 2.448154]\n",
      "394 [D loss: 0.168018, acc.: 93.75%] [G loss: 2.435511]\n",
      "395 [D loss: 0.347697, acc.: 79.69%] [G loss: 2.530761]\n",
      "396 [D loss: 0.434484, acc.: 76.56%] [G loss: 2.252105]\n",
      "397 [D loss: 0.289210, acc.: 87.50%] [G loss: 2.110487]\n",
      "398 [D loss: 0.337482, acc.: 81.25%] [G loss: 1.873587]\n",
      "399 [D loss: 0.322109, acc.: 85.94%] [G loss: 2.045319]\n",
      "400 [D loss: 0.334541, acc.: 84.38%] [G loss: 2.064011]\n",
      "Äáº±ng gian\n"
     ]
    }
   ],
   "source": [
    "r = 5\n",
    "c = 2\n",
    "x_train = reshape_x_train(X_train_encoded_padded_words, r, c)\n",
    "gan = GAN(r,c,Tokenizer_vocab_size)\n",
    "gan.train(x_train,epochs=401, batch_size=32, sample_interval=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
